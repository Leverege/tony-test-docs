{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"ingestion/mqtt-ingestor/","text":"mqtt-ingestor MQTT Ingestor \u00b6 You are building an ingestion server. Weeeeeeeee! Getting Started \u00b6 Enter steps necessary to start using the repo. Install \u00b6 <INSTALLATION INSTRUCTIONS>","title":"MQTT"},{"location":"ingestion/mqtt-ingestor/#mqtt-ingestor-mqtt-ingestor","text":"You are building an ingestion server. Weeeeeeeee!","title":"mqtt-ingestor  MQTT  Ingestor"},{"location":"ingestion/mqtt-ingestor/#getting-started","text":"Enter steps necessary to start using the repo.","title":"Getting Started"},{"location":"ingestion/mqtt-ingestor/#install","text":"<INSTALLATION INSTRUCTIONS>","title":"Install"},{"location":"platform-overview/api-ui/","text":"The Leverege UI interacts intimately with the Leverege API. The Leverege UI is the recommended way to interact with the API to do project setup. For instructions on how to setup your project on the Leverege Platform, refer to the getting started guide. The following are guides for common actions taken with the Leverege UI. Run API UI locally \u00b6 npm run decrypt Change the IMAGINE_HOST_URL in your .env accordingly (local vs hosted API Server URL) npm start Reason Scripts \u00b6 To create a script, navigate to the scripts tab of the UI, create a script and add functionality to the script. Reasoner documentation will assist when writing functionality. Save the script, and deploy it. Next determine the trigger for the script. This trigger can be a timer, message route or a message on the reason topic. For creating a timer, read the scheduling timers section . Create this timer with the Trigger Script functionality, and copy the script id into the input. Input in JSON, the options that are needed, these will be passed into the script through the params key the context . Details for setting up a message route can be found in the Reason documentation . When set up this way, the context will contain deviceData, system, device and blueprint information. The third was is to send a message from inside the platform on the reason topic. This is the least supported way, and not recommended. Creating Users \u00b6 To create a user, navigate to the users tab, and click + new user . The required fields are username and password . Passwords need to be at least 6 characters. Once a user has been created, they can be permissioned on systems or devices. To permission a user on a system navigate to the system, then in the access tab, add the user and define their permissions. To permission a user on a device, use the api library, call api.device( deviceId ).users().add( userId, permissions, metadata ) or use Interface . To create a project user, the an api call is required. api.project( projectId ).users.create( user, permissions, metadata ) . Changing Device Attributes \u00b6 Under the Blueprints tab, all the current Blueprints are visible, click on the Blueprint name to open an attribute editor. Here attributes can be deleted and created, and can be minimally edited. when adding an attribute, a type and units can be designated. Deep attributes can be created by including / separating the keys. If a attribute is created with an incorrect type or unit, it has to be deleted and recreated. Creating a Simulation \u00b6 Simulations are created under the Scenarios tab. The Scenarios are all tied to a blueprint. The UI allows for values to be changed on an hour time line. Click on the timeline to create a specific value at that time. Clicking on the line between two value points opens an editor for the interpolation, which also allows the update frequency to be changed. When creating a device, it can be specified as simulated, or a device can have its simulator turned on in its info tab. When clicking into the device details, when a device is simulated, it has a play button in the upper right, clicking on this opens a selector for scenarios. Once the simulation is run, it will begin to change the value as designated in the scenario. Scheduling Timers \u00b6 Timers are created under the Timers tab of the UI. The Scheduler service manages all of the timers. Timers can have several different end functionalities. these functionalities are Trigger Script Query Url Publish Topic and specified through a dropdown. These functionalities have additional parameters that can be specified in the UI. The other main Attribute of Timers are when they are run. They can be specified to run once, at an interval, or a specific time everyday. This is changed in the Timer section of a timer.","title":"API UI"},{"location":"platform-overview/api-ui/#run-api-ui-locally","text":"npm run decrypt Change the IMAGINE_HOST_URL in your .env accordingly (local vs hosted API Server URL) npm start","title":"Run API UI locally"},{"location":"platform-overview/api-ui/#reason-scripts","text":"To create a script, navigate to the scripts tab of the UI, create a script and add functionality to the script. Reasoner documentation will assist when writing functionality. Save the script, and deploy it. Next determine the trigger for the script. This trigger can be a timer, message route or a message on the reason topic. For creating a timer, read the scheduling timers section . Create this timer with the Trigger Script functionality, and copy the script id into the input. Input in JSON, the options that are needed, these will be passed into the script through the params key the context . Details for setting up a message route can be found in the Reason documentation . When set up this way, the context will contain deviceData, system, device and blueprint information. The third was is to send a message from inside the platform on the reason topic. This is the least supported way, and not recommended.","title":"Reason Scripts"},{"location":"platform-overview/api-ui/#creating-users","text":"To create a user, navigate to the users tab, and click + new user . The required fields are username and password . Passwords need to be at least 6 characters. Once a user has been created, they can be permissioned on systems or devices. To permission a user on a system navigate to the system, then in the access tab, add the user and define their permissions. To permission a user on a device, use the api library, call api.device( deviceId ).users().add( userId, permissions, metadata ) or use Interface . To create a project user, the an api call is required. api.project( projectId ).users.create( user, permissions, metadata ) .","title":"Creating Users"},{"location":"platform-overview/api-ui/#changing-device-attributes","text":"Under the Blueprints tab, all the current Blueprints are visible, click on the Blueprint name to open an attribute editor. Here attributes can be deleted and created, and can be minimally edited. when adding an attribute, a type and units can be designated. Deep attributes can be created by including / separating the keys. If a attribute is created with an incorrect type or unit, it has to be deleted and recreated.","title":"Changing Device Attributes"},{"location":"platform-overview/api-ui/#creating-a-simulation","text":"Simulations are created under the Scenarios tab. The Scenarios are all tied to a blueprint. The UI allows for values to be changed on an hour time line. Click on the timeline to create a specific value at that time. Clicking on the line between two value points opens an editor for the interpolation, which also allows the update frequency to be changed. When creating a device, it can be specified as simulated, or a device can have its simulator turned on in its info tab. When clicking into the device details, when a device is simulated, it has a play button in the upper right, clicking on this opens a selector for scenarios. Once the simulation is run, it will begin to change the value as designated in the scenario.","title":"Creating a Simulation"},{"location":"platform-overview/api-ui/#scheduling-timers","text":"Timers are created under the Timers tab of the UI. The Scheduler service manages all of the timers. Timers can have several different end functionalities. these functionalities are Trigger Script Query Url Publish Topic and specified through a dropdown. These functionalities have additional parameters that can be specified in the UI. The other main Attribute of Timers are when they are run. They can be specified to run once, at an interval, or a specific time everyday. This is changed in the Timer section of a timer.","title":"Scheduling Timers"},{"location":"server-libraries/cache/","text":"Cache \u00b6 The Cache library creates an cache to quickly set are retrieve values and speed up the operation of services, Install \u00b6 npm install --save @leverege/cache const Cache = require( '@leverege/cache' ); Usage \u00b6 In general, use the cacheConfig to drive the type of cache. NOTE: Version 2.0 changed the create() method to be async. Please wait for it to complete before using the result. const cacheConfig = process.env.CACHE_CONFIG || { type : 'redis', connection : { host : '127.0.0.1', port : 6379 } }; const cache = await Cache.create( { cacheConfig, namespace : 'myApp'} ) ... await cache.set( 'myKey', myValue ) // This will set a value to a global myKey const v = await cache.get( 'myKey' ) // This will access a global myKey await cache.setNs( 'myKey', myValue ) // This will set a value to a namespaced myKey (myApp:myKey) const v = await cache.getNs( 'myKey' ) // This will access a namespaced myKey (myApp:myKey) Other usage \u00b6 To make a redis cache: const cache = Cache.create( { type : 'redis', namespace : 'myApp', connection : { host : '127.0.0.1', port : 6379 } } )","title":"Cache"},{"location":"server-libraries/cache/#cache","text":"The Cache library creates an cache to quickly set are retrieve values and speed up the operation of services,","title":"Cache"},{"location":"server-libraries/cache/#install","text":"npm install --save @leverege/cache const Cache = require( '@leverege/cache' );","title":"Install"},{"location":"server-libraries/cache/#usage","text":"In general, use the cacheConfig to drive the type of cache. NOTE: Version 2.0 changed the create() method to be async. Please wait for it to complete before using the result. const cacheConfig = process.env.CACHE_CONFIG || { type : 'redis', connection : { host : '127.0.0.1', port : 6379 } }; const cache = await Cache.create( { cacheConfig, namespace : 'myApp'} ) ... await cache.set( 'myKey', myValue ) // This will set a value to a global myKey const v = await cache.get( 'myKey' ) // This will access a global myKey await cache.setNs( 'myKey', myValue ) // This will set a value to a namespaced myKey (myApp:myKey) const v = await cache.getNs( 'myKey' ) // This will access a namespaced myKey (myApp:myKey)","title":"Usage"},{"location":"server-libraries/cache/#other-usage","text":"To make a redis cache: const cache = Cache.create( { type : 'redis', namespace : 'myApp', connection : { host : '127.0.0.1', port : 6379 } } )","title":"Other usage"},{"location":"server-libraries/cluster-manager/","text":"Cluster Manager \u00b6 ClusterManager will start and restart child processes. It will also allow control of the those processes via a service express port. The log levels of the children can be set, the status of the cluster can be queried and the children can be restarted. Initial Setup \u00b6 npm install @leverege/cluster-manager --save const ClusterManager = require( '@leverege/cluster-manager' ) Usage \u00b6 The ClusterManager installs graceful async exit handlers using @leverege/exit so it s recommended that you install and use that library. There are two ways of using this class. The passed in function is the recommended use. Exit.installDefaultHandlers() const options = { /*...*/ } ClusterManager.start( options, ( ) => { MyServer.start( cfg ) } ) If you need to start special processing in your master process, you can use the third argument: const options = { /*...*/ } ClusterManager.start( options, ( ) => { MyServer.start( cfg ) }, ( ) => { MyMasterProcess.start( cfg ) } ) By default, the options.count value use the env variable CM_MAX_CHILD_PROCESSES or MAX_CHILD_PROCESSES or 32 clamped to the number of cpus to determine the number of child processes to start. Often, for Kubernetes deployments, you will want to set MAX_CHILD_PROCESSES to 1. If your process MUST have only 1 (or N) children, set count explicitely. The old method still works, but is more verbose: if ( ClusterManager.isMaster ) { ClusterManager.start( { } ) } else { MyServer.start( Config ) ClusterManager.connectToParent( { updateStatusMs : 10000 } ) } Express Options \u00b6 If you set the webServerPort to a number, like 5111, you can do the following: http://hostname:5111/ This will show the current status in JSON form http://hostname:5111/logLevel/:level This will set the log level of the parent and child processes. Valid values of :level are 'trace', 'debug', 'info', 'warn', 'error', or 'fatal' http://hostname:5111/children/exit This will stop all child processes. If autoRestart is true, they will be restarted Ideally, this would be behind a firewall and not accessible to the world. By default, webServerPort will default to process.env.CM_WEBSERVER_PORT . Child Processes \u00b6 When started, a child process will be given a CM_CHILD_ID process.env argument that will remain the same through restarts of the child process. The log functions use this to continue writing to the same log file. On a restart, CM_RESTART will be set to the number of restarts that have occured on that child id. ClusterManager.isRestart will be true if the current fork is the first instance run for that id.","title":"Cluster Manager"},{"location":"server-libraries/cluster-manager/#cluster-manager","text":"ClusterManager will start and restart child processes. It will also allow control of the those processes via a service express port. The log levels of the children can be set, the status of the cluster can be queried and the children can be restarted.","title":"Cluster Manager"},{"location":"server-libraries/cluster-manager/#initial-setup","text":"npm install @leverege/cluster-manager --save const ClusterManager = require( '@leverege/cluster-manager' )","title":"Initial Setup"},{"location":"server-libraries/cluster-manager/#usage","text":"The ClusterManager installs graceful async exit handlers using @leverege/exit so it s recommended that you install and use that library. There are two ways of using this class. The passed in function is the recommended use. Exit.installDefaultHandlers() const options = { /*...*/ } ClusterManager.start( options, ( ) => { MyServer.start( cfg ) } ) If you need to start special processing in your master process, you can use the third argument: const options = { /*...*/ } ClusterManager.start( options, ( ) => { MyServer.start( cfg ) }, ( ) => { MyMasterProcess.start( cfg ) } ) By default, the options.count value use the env variable CM_MAX_CHILD_PROCESSES or MAX_CHILD_PROCESSES or 32 clamped to the number of cpus to determine the number of child processes to start. Often, for Kubernetes deployments, you will want to set MAX_CHILD_PROCESSES to 1. If your process MUST have only 1 (or N) children, set count explicitely. The old method still works, but is more verbose: if ( ClusterManager.isMaster ) { ClusterManager.start( { } ) } else { MyServer.start( Config ) ClusterManager.connectToParent( { updateStatusMs : 10000 } ) }","title":"Usage"},{"location":"server-libraries/cluster-manager/#express-options","text":"If you set the webServerPort to a number, like 5111, you can do the following: http://hostname:5111/ This will show the current status in JSON form http://hostname:5111/logLevel/:level This will set the log level of the parent and child processes. Valid values of :level are 'trace', 'debug', 'info', 'warn', 'error', or 'fatal' http://hostname:5111/children/exit This will stop all child processes. If autoRestart is true, they will be restarted Ideally, this would be behind a firewall and not accessible to the world. By default, webServerPort will default to process.env.CM_WEBSERVER_PORT .","title":"Express Options"},{"location":"server-libraries/cluster-manager/#child-processes","text":"When started, a child process will be given a CM_CHILD_ID process.env argument that will remain the same through restarts of the child process. The log functions use this to continue writing to the same log file. On a restart, CM_RESTART will be set to the number of restarts that have occured on that child id. ClusterManager.isRestart will be true if the current fork is the first instance run for that id.","title":"Child Processes"},{"location":"server-libraries/error/","text":"The error library contains a set of commonly used errors, and allows for easy construction of new Errors. It will allow for instanceof checks and you don't need to use the new syntax to create an Error. Usage \u00b6 const Err = require ( '@leverege/error' ) // to throw a base error, use one of these constructor types. They // will work on any Err type. throw Err ( ) throw Err ( 'My error message' ) throw Err ( 'My error message' , 1234 ) throw Err ( 'My error message' , 1234 , { otherValue : 789 } ) throw Err ( 'My error message' , { otherValue : 789 } ) throw Err ( { message : 'My error message' , status : 1234 , otherValue : 789 } ) const e = Err () e instanceof Err // true e instanceof Error // true Default Err types include // Common Http Errors Err . badRequest = Err . extend ( 'BadRequest' , 400 , 'Bad arguments' ) Err . unauthorized = Err . extend ( 'Unauthorized' , 401 , 'Unauthorized' ) Err . forbidden = Err . extend ( 'Forbidden' , 403 , 'Forbidden' ) Err . notFound = Err . extend ( 'NotFound' , 404 , 'Not found' ) Err . requestTimeout = Err . extend ( 'RequestTimeout' , 408 , 'Request timeout' ) Err . conflict = Err . extend ( 'Conflict' , 409 , 'Conflict' ) Err . gone = Err . extend ( 'Gone' , 410 , 'Gone' ) Err . internalError = Err . extend ( 'InternalError' , 500 , 'Internal Error' ) Err . notImplemented = Err . extend ( 'NotImplemented' , 501 , 'Not implemented' ) Err . serviceUnavailable = Err . extend ( 'ServiceUnavailable' , 503 , 'Service unavailable' ) // Other errors Err . illegalArgument = Err . extend ( 'IllegalArgument' , 400 , 'Illegal argument' ) // use BadRequest code Err . unknownMessage = Err . extend ( 'UnknownMessage' , 400 , 'Unknown Message' ) So, if your express request has a bad post parameter, you could do this: app . post ( '/foo' ( req , res , next ) => { if ( isNameInvalid ( req . param . name ) ) { return next ( Err . badRequest ( 'The name is invalid' ) ) } doStuff ( next ) }) To create a new error type, use the extend() method. Err . badStuff = Err . extend ( 'BadStuff' , 666 , 'Bad stuff occurred' ) throw Err . badStuff () /* will produce { BadStuff: Bad stuff occurred at Function.SubErr [as badStuff] (/Users/steve/Dev/Leverege/error/src/Err.js:50:14) at repl:1:5 at ContextifyScript.Script.runInThisContext (vm.js:50:33) at REPLServer.defaultEval (repl.js:240:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:441:10) at emitOne (events.js:121:20) at REPLServer.emit (events.js:211:7) at REPLServer.Interface._onLine (readline.js:282:10) status: 666 } */ const bs = Err . badStuff ( 'name field wasnt set' ) bs instanceof Err . badStuff // true bs instanceof Err // true bs instanceof Error // true console . log ( bs ) /* { BadStuff: name field wasnt set at Function.SubErr [as badStuff] (/Users/steve/Dev/Leverege/error/src/Err.js:50:14) at repl:1:10 at ContextifyScript.Script.runInThisContext (vm.js:50:33) at REPLServer.defaultEval (repl.js:240:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:441:10) at emitOne (events.js:121:20) at REPLServer.emit (events.js:211:7) at REPLServer.Interface._onLine (readline.js:282:10) status: 666, message: 'name field wasnt set' } */ console . log ( bs . toString () ) // BadStuff: name field wasnt set {\"status\":666,\"message\":\"name field wasnt set\"}","title":"Error"},{"location":"server-libraries/error/#usage","text":"const Err = require ( '@leverege/error' ) // to throw a base error, use one of these constructor types. They // will work on any Err type. throw Err ( ) throw Err ( 'My error message' ) throw Err ( 'My error message' , 1234 ) throw Err ( 'My error message' , 1234 , { otherValue : 789 } ) throw Err ( 'My error message' , { otherValue : 789 } ) throw Err ( { message : 'My error message' , status : 1234 , otherValue : 789 } ) const e = Err () e instanceof Err // true e instanceof Error // true Default Err types include // Common Http Errors Err . badRequest = Err . extend ( 'BadRequest' , 400 , 'Bad arguments' ) Err . unauthorized = Err . extend ( 'Unauthorized' , 401 , 'Unauthorized' ) Err . forbidden = Err . extend ( 'Forbidden' , 403 , 'Forbidden' ) Err . notFound = Err . extend ( 'NotFound' , 404 , 'Not found' ) Err . requestTimeout = Err . extend ( 'RequestTimeout' , 408 , 'Request timeout' ) Err . conflict = Err . extend ( 'Conflict' , 409 , 'Conflict' ) Err . gone = Err . extend ( 'Gone' , 410 , 'Gone' ) Err . internalError = Err . extend ( 'InternalError' , 500 , 'Internal Error' ) Err . notImplemented = Err . extend ( 'NotImplemented' , 501 , 'Not implemented' ) Err . serviceUnavailable = Err . extend ( 'ServiceUnavailable' , 503 , 'Service unavailable' ) // Other errors Err . illegalArgument = Err . extend ( 'IllegalArgument' , 400 , 'Illegal argument' ) // use BadRequest code Err . unknownMessage = Err . extend ( 'UnknownMessage' , 400 , 'Unknown Message' ) So, if your express request has a bad post parameter, you could do this: app . post ( '/foo' ( req , res , next ) => { if ( isNameInvalid ( req . param . name ) ) { return next ( Err . badRequest ( 'The name is invalid' ) ) } doStuff ( next ) }) To create a new error type, use the extend() method. Err . badStuff = Err . extend ( 'BadStuff' , 666 , 'Bad stuff occurred' ) throw Err . badStuff () /* will produce { BadStuff: Bad stuff occurred at Function.SubErr [as badStuff] (/Users/steve/Dev/Leverege/error/src/Err.js:50:14) at repl:1:5 at ContextifyScript.Script.runInThisContext (vm.js:50:33) at REPLServer.defaultEval (repl.js:240:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:441:10) at emitOne (events.js:121:20) at REPLServer.emit (events.js:211:7) at REPLServer.Interface._onLine (readline.js:282:10) status: 666 } */ const bs = Err . badStuff ( 'name field wasnt set' ) bs instanceof Err . badStuff // true bs instanceof Err // true bs instanceof Error // true console . log ( bs ) /* { BadStuff: name field wasnt set at Function.SubErr [as badStuff] (/Users/steve/Dev/Leverege/error/src/Err.js:50:14) at repl:1:10 at ContextifyScript.Script.runInThisContext (vm.js:50:33) at REPLServer.defaultEval (repl.js:240:29) at bound (domain.js:301:14) at REPLServer.runBound [as eval] (domain.js:314:12) at REPLServer.onLine (repl.js:441:10) at emitOne (events.js:121:20) at REPLServer.emit (events.js:211:7) at REPLServer.Interface._onLine (readline.js:282:10) status: 666, message: 'name field wasnt set' } */ console . log ( bs . toString () ) // BadStuff: name field wasnt set {\"status\":666,\"message\":\"name field wasnt set\"}","title":"Usage"},{"location":"server-libraries/exit/","text":"The purpose of the exit library is to allow for the graceful asynchronous shutdown of resources when signals and exceptions are caught. This cannot be used with process.exit() . Use Exit.exit() instead. Installation \u00b6 npm install --save @leverege/exit Usage \u00b6 Include the library in your module: const Exit = require( '@leverege/exit' ) To configure Exit, you can use init() or the individual set methods. By default, Exit will force and exit in 20 seconds if the shutdown hooks don't complete. When the shutdown hooks complete, Exit will wait 100 ms before calling exit to allow for logs to be written. For testing purposes, the exit function can be set. It will default to process.exit() if one is not supplied. Exit.init( { timeout : 20000, exitDelay : 100, exit : null }) Install the default handlers, which will cover SIGINT , SIGHUP , SIGBREAK , SIGTERM , beforeExit , uncaughtException and unhandledRejection . Exit.installDefaultHandlers() Installing Shutdown hooks \u00b6 To install a shutdown hook, use the add() method: const httpServer = app.listen( 1234 ) const expressShutdown = ( err, { eventType, eventCode } ) => { return new Promise( ( resolve, reject ) => { app.close( resolve ) } ) } Exit.add( expressShutdown ) If you want to remove the shutdown hook: Exit.remove( expressShutdown ) Custom Event Handling \u00b6 You do not have to use Exit.installDefaultHandlers() . If you wish to handle different or addition signals and events, you can elect to register for event individually. The installDefaultHandlers() implementation looks like this: handle( 'beforeExit', 0 ) handle( 'SIGHUP', 128 + 1 ) handle( 'SIGINT', 128 + 2 ) handle( 'SIGTERM', 128 + 15 ) handle( 'SIGBREAK', 128 + 21 ) handleErr( 'uncaughtException', 1 ) handleErr( 'unhandledRejection', 1 ) Exit Filtering \u00b6 You can install a filter against an event type that can cancelled the shutdown and exit procedure. Returning true from a filter will cancel the exit. The following will not exit if the err object has a field called dontExit which is true. Exit.addFilter( 'uncaughtException', ( err ) => err && err.dontExit === true )","title":"Exit"},{"location":"server-libraries/exit/#installation","text":"npm install --save @leverege/exit","title":"Installation"},{"location":"server-libraries/exit/#usage","text":"Include the library in your module: const Exit = require( '@leverege/exit' ) To configure Exit, you can use init() or the individual set methods. By default, Exit will force and exit in 20 seconds if the shutdown hooks don't complete. When the shutdown hooks complete, Exit will wait 100 ms before calling exit to allow for logs to be written. For testing purposes, the exit function can be set. It will default to process.exit() if one is not supplied. Exit.init( { timeout : 20000, exitDelay : 100, exit : null }) Install the default handlers, which will cover SIGINT , SIGHUP , SIGBREAK , SIGTERM , beforeExit , uncaughtException and unhandledRejection . Exit.installDefaultHandlers()","title":"Usage"},{"location":"server-libraries/exit/#installing-shutdown-hooks","text":"To install a shutdown hook, use the add() method: const httpServer = app.listen( 1234 ) const expressShutdown = ( err, { eventType, eventCode } ) => { return new Promise( ( resolve, reject ) => { app.close( resolve ) } ) } Exit.add( expressShutdown ) If you want to remove the shutdown hook: Exit.remove( expressShutdown )","title":"Installing Shutdown hooks"},{"location":"server-libraries/exit/#custom-event-handling","text":"You do not have to use Exit.installDefaultHandlers() . If you wish to handle different or addition signals and events, you can elect to register for event individually. The installDefaultHandlers() implementation looks like this: handle( 'beforeExit', 0 ) handle( 'SIGHUP', 128 + 1 ) handle( 'SIGINT', 128 + 2 ) handle( 'SIGTERM', 128 + 15 ) handle( 'SIGBREAK', 128 + 21 ) handleErr( 'uncaughtException', 1 ) handleErr( 'unhandledRejection', 1 )","title":"Custom Event Handling"},{"location":"server-libraries/exit/#exit-filtering","text":"You can install a filter against an event type that can cancelled the shutdown and exit procedure. Returning true from a filter will cancel the exit. The following will not exit if the err object has a field called dontExit which is true. Exit.addFilter( 'uncaughtException', ( err ) => err && err.dontExit === true )","title":"Exit Filtering"},{"location":"server-libraries/limit/","text":"Limit \u00b6 The limit library is used with messaging services. The limit library limits the number of messages that a user can get in a time period, and if that limit is reached, does not send the user more messages for a specified time period. Limit uses namespaces, so that messages can be limited independent of each other. Install \u00b6 npm install - save @ leverege / limit Usage \u00b6 An example of a redis based approach: const Limit = require ( '@leverege/limit' ); const limitConfig = process . env . LIMIT_CONFIG || { type : 'redis' , connection : { host : '127.0.0.1' , port : 6379 }, }; const limit = await Limit . create ( { namespace : 'myApp' , limitConfig } ) NOTE: Version 2.0 changed the create() method to be async. Please wait for it to complete before using the result. Limiting \u00b6 // Create a connection to the limiter mechanism. Give it a namespace const limiter = Limit . create ( { namespace : 'myApp' }, limitConfig ) // Create a limit object for a specific key (which can be namespaced), // with specific properties. const limit = limiter . limit ( 'myKey' , { count : 10 , // only allow 10 period : 10 * 1000 , // over a 10 second period duration : 60 * 60 * 1000 , // or limit for one hour }) // Same can be done with a namespaced key. const limit = limiter . limit ( 'myKey' , { count : 5 , // only allow 5 period : 30 * 1000 , // over a 30 second period duration : 5 * 60 * 1000 , // or limit for 5 minutes }) const limited = await limit . isLimited ( ) // increment and check if we are limited if ( limited ) { // don't do the action return } // do the action Limiting with Remaining Limit Time \u00b6 In some cases it can be useful to know the remaining time until the limit expires. getRemainingLimit can be used in place of isLimited in these situations. const limiter = Limit . create ( { namespace : 'myApp' }, limitConfig ) const limit = limiter . limit ( 'myKey' , { count : 10 , // only allow 10 period : 10 * 1000 , // over a 10 second period duration : 60 * 60 * 1000 , // or limit for one hour }) const remainingLimit = await limit . getRemainingLimit ( ) // increment and get time remaining until limit expiry if ( remainingLimit > 0 ) { // don't do the action return `Try again in ${ Math . ceiling ( remainingLimit / 1000 ) } seconds` } // do the action Checking Limit without Incrementing Count \u00b6 const isLimited = await limit . checkLimit ( ) // true if and only if limited const remainingLimit = await limit . checkLimit ( { asTime : true } ) // MS time until limit expires (min 0)","title":"Limit"},{"location":"server-libraries/limit/#limit","text":"The limit library is used with messaging services. The limit library limits the number of messages that a user can get in a time period, and if that limit is reached, does not send the user more messages for a specified time period. Limit uses namespaces, so that messages can be limited independent of each other.","title":"Limit"},{"location":"server-libraries/limit/#install","text":"npm install - save @ leverege / limit","title":"Install"},{"location":"server-libraries/limit/#usage","text":"An example of a redis based approach: const Limit = require ( '@leverege/limit' ); const limitConfig = process . env . LIMIT_CONFIG || { type : 'redis' , connection : { host : '127.0.0.1' , port : 6379 }, }; const limit = await Limit . create ( { namespace : 'myApp' , limitConfig } ) NOTE: Version 2.0 changed the create() method to be async. Please wait for it to complete before using the result.","title":"Usage"},{"location":"server-libraries/limit/#limiting","text":"// Create a connection to the limiter mechanism. Give it a namespace const limiter = Limit . create ( { namespace : 'myApp' }, limitConfig ) // Create a limit object for a specific key (which can be namespaced), // with specific properties. const limit = limiter . limit ( 'myKey' , { count : 10 , // only allow 10 period : 10 * 1000 , // over a 10 second period duration : 60 * 60 * 1000 , // or limit for one hour }) // Same can be done with a namespaced key. const limit = limiter . limit ( 'myKey' , { count : 5 , // only allow 5 period : 30 * 1000 , // over a 30 second period duration : 5 * 60 * 1000 , // or limit for 5 minutes }) const limited = await limit . isLimited ( ) // increment and check if we are limited if ( limited ) { // don't do the action return } // do the action","title":"Limiting"},{"location":"server-libraries/limit/#limiting-with-remaining-limit-time","text":"In some cases it can be useful to know the remaining time until the limit expires. getRemainingLimit can be used in place of isLimited in these situations. const limiter = Limit . create ( { namespace : 'myApp' }, limitConfig ) const limit = limiter . limit ( 'myKey' , { count : 10 , // only allow 10 period : 10 * 1000 , // over a 10 second period duration : 60 * 60 * 1000 , // or limit for one hour }) const remainingLimit = await limit . getRemainingLimit ( ) // increment and get time remaining until limit expiry if ( remainingLimit > 0 ) { // don't do the action return `Try again in ${ Math . ceiling ( remainingLimit / 1000 ) } seconds` } // do the action","title":"Limiting with Remaining Limit Time"},{"location":"server-libraries/limit/#checking-limit-without-incrementing-count","text":"const isLimited = await limit . checkLimit ( ) // true if and only if limited const remainingLimit = await limit . checkLimit ( { asTime : true } ) // MS time until limit expires (min 0)","title":"Checking Limit without Incrementing Count"},{"location":"server-libraries/lock/","text":"Lock \u00b6 Install \u00b6 npm install -save @leverege/lock const Lock = require( '@leverege/lock' ); Usage \u00b6 In general, use the lockConfig to drive the type of shared lock. This allows the type to be driven by the environment variables setup by the deployment mechanism. NOTE: Version 2.0 changed the create() method to be async. Please wait for it to complete before using the result. const lockConfig = process.env.LOCK_CONFIG || { type : 'redis', connection : { host : '127.0.0.1', port : 6379 }, lockOptions : { ... } }; const lock = await Lock.create( { lockConfig, namespace : 'myApp'} ) You can destroy the Lock resources by calling: lock.destroy() Locking \u00b6 ... const l = await lock.lock( 'myKey', 1000 ) // This will lock globally on myKey // do work await l.unlock() l = await lock.lockNs( 'myKey', myValue ) // This will lock using namespaced myKey (myApp:myKey) // do work await l.unlock() Extending a lock \u00b6 To extend a lock, call extend( timeMs ) and use the returned lock let l = await lock.lock( 'myKey', { ttl : 1000 } ) // do work l = await l.extend( 1000 ) // do more work l.unlock() InMemoryLock \u00b6 The InMemoryLock will lock on the instance of the lock, and will work only within the process itself. const lock = Lock.create({ type : 'inMemory', namespace : 'myApp', retry : 0, // the number of attempts to make before failing. Negative numbers means infinite delay : 100, // time in milliseconds between lock attempts }) // or, using lockConfig constlock = Lock.create({ namespace : 'test', lockConfig : { type : 'inMemory', retry : 0, // the number of attempts to make before failing. Negative numbers means infinite delay : 100, // time in milliseconds between lock attempts } }) Redis \u00b6 The Redis lock can be used to lock between processes. const lock = Lock.create( { type : 'redis', namespace : 'myApp', connection : { host : '127.0.0.1', port : 6379 }, // From RedLock lockOptions : { // the expected clock drift; for more details // see http://redis.io/topics/distlock driftFactor: 0.01, // time in ms // the max number of times Redlock will attempt // to lock a resource before erroring retryCount: 0, // the time in ms between attempts retryDelay: 200, // time in ms // the max time in ms randomly added to retries // to improve performance under high contention // see https://www.awsarchitectureblog.com/2015/03/backoff.html retryJitter: 100 // time in ms } } ) // Or using lockConfig const lock = Lock.create( { namespace : 'myApp', lockConfig : { type : 'redis', connection : { host : '127.0.0.1', port : 6379 }, // From RedLock lockOptions : { // the expected clock drift; for more details // see http://redis.io/topics/distlock driftFactor: 0.01, // time in ms // the max number of times Redlock will attempt // to lock a resource before erroring retryCount: 0, // the time in ms between attempts retryDelay: 200, // time in ms // the max time in ms randomly added to retries // to improve performance under high contention // see https://www.awsarchitectureblog.com/2015/03/backoff.html retryJitter: 100 // time in ms } } } )","title":"Lock"},{"location":"server-libraries/lock/#lock","text":"","title":"Lock"},{"location":"server-libraries/lock/#install","text":"npm install -save @leverege/lock const Lock = require( '@leverege/lock' );","title":"Install"},{"location":"server-libraries/lock/#usage","text":"In general, use the lockConfig to drive the type of shared lock. This allows the type to be driven by the environment variables setup by the deployment mechanism. NOTE: Version 2.0 changed the create() method to be async. Please wait for it to complete before using the result. const lockConfig = process.env.LOCK_CONFIG || { type : 'redis', connection : { host : '127.0.0.1', port : 6379 }, lockOptions : { ... } }; const lock = await Lock.create( { lockConfig, namespace : 'myApp'} ) You can destroy the Lock resources by calling: lock.destroy()","title":"Usage"},{"location":"server-libraries/lock/#locking","text":"... const l = await lock.lock( 'myKey', 1000 ) // This will lock globally on myKey // do work await l.unlock() l = await lock.lockNs( 'myKey', myValue ) // This will lock using namespaced myKey (myApp:myKey) // do work await l.unlock()","title":"Locking"},{"location":"server-libraries/lock/#extending-a-lock","text":"To extend a lock, call extend( timeMs ) and use the returned lock let l = await lock.lock( 'myKey', { ttl : 1000 } ) // do work l = await l.extend( 1000 ) // do more work l.unlock()","title":"Extending a lock"},{"location":"server-libraries/lock/#inmemorylock","text":"The InMemoryLock will lock on the instance of the lock, and will work only within the process itself. const lock = Lock.create({ type : 'inMemory', namespace : 'myApp', retry : 0, // the number of attempts to make before failing. Negative numbers means infinite delay : 100, // time in milliseconds between lock attempts }) // or, using lockConfig constlock = Lock.create({ namespace : 'test', lockConfig : { type : 'inMemory', retry : 0, // the number of attempts to make before failing. Negative numbers means infinite delay : 100, // time in milliseconds between lock attempts } })","title":"InMemoryLock"},{"location":"server-libraries/lock/#redis","text":"The Redis lock can be used to lock between processes. const lock = Lock.create( { type : 'redis', namespace : 'myApp', connection : { host : '127.0.0.1', port : 6379 }, // From RedLock lockOptions : { // the expected clock drift; for more details // see http://redis.io/topics/distlock driftFactor: 0.01, // time in ms // the max number of times Redlock will attempt // to lock a resource before erroring retryCount: 0, // the time in ms between attempts retryDelay: 200, // time in ms // the max time in ms randomly added to retries // to improve performance under high contention // see https://www.awsarchitectureblog.com/2015/03/backoff.html retryJitter: 100 // time in ms } } ) // Or using lockConfig const lock = Lock.create( { namespace : 'myApp', lockConfig : { type : 'redis', connection : { host : '127.0.0.1', port : 6379 }, // From RedLock lockOptions : { // the expected clock drift; for more details // see http://redis.io/topics/distlock driftFactor: 0.01, // time in ms // the max number of times Redlock will attempt // to lock a resource before erroring retryCount: 0, // the time in ms between attempts retryDelay: 200, // time in ms // the max time in ms randomly added to retries // to improve performance under high contention // see https://www.awsarchitectureblog.com/2015/03/backoff.html retryJitter: 100 // time in ms } } } )","title":"Redis"},{"location":"server-libraries/log/","text":"The Log library abstracts away much of the logging functionality, making it so that with one call, log statements get logged in multiple places. Usage \u00b6 npm install @leverege/log --save By default, the log defaults to the EmptyLog, which no-ops all logging methods. To install a console logger, in the main class call: const log = require ( '@leverege/log' ); log . setAdapter ( log . ConsoleLog () ) To install a bunyan logger, call: const bunyanLog = bunyan . createLogger ( { ... } ); log . setAdapter ( log . BunyanLog ( bunyanLog ) ); Automated Log Creation and Installation \u00b6 If you want to install a log entirely from a configuration variable (JSON, file, or plain object) call: log . createAdapter ( < config - object , JSON string , or file > , < params - object > , < install - boolean > ) This function will create an Adapter based on your configuration, can parameterize your configuration if you are using a string or file, and will automatically call setAdapter if install is not false. a configuration may look like: const logFile = '/var/config/transponder/logConfig.json' or const logString = '{ \"type\" : \"bunyan\", \"name\" : \"${logName}\", \"streams\" : [ { \"type\" : \"rotating-file\", \"period\" : \"1d\", \"count\" : 3, \"level\" : \"warn\", \"path\" : \"./logs3/${logName}\" }, { \"type\" : \"stream\", \"logName\" : \"${resource}\" } ] }' or const logObject = { type : 'console' , mod : 'transponder' } In order to make the second JSON string configuration work, a parameters object would be supplied like so: const logParams = { logName : ClusterManager . logName ( 'transponder' ), resource : `transponder- ${ process . env . TRANSPONDER_RUN_MODE || 'all' } ` , logServiceAccount : process . env . LOG_SERVICE_ACCOUNT } So the installation would look like the following: log . createAdapter ( logString , params , true ) Logging \u00b6 To log, call: const log = require ( '@leverege/log' ); log . trace ( 'trace log' ); log . debug ( 'debug log' ); log . info ( 'info log' ); log . warn ( 'warn log' ); log . fatal ( 'fatal log' ); The log methods can take an object as the first argument. See bunyans log for more information. Other methods include: child, setLevel, getLevel, isTrace, isDebug, isInfo, isWarn, isError, isError, isFatal","title":"Log"},{"location":"server-libraries/log/#usage","text":"npm install @leverege/log --save By default, the log defaults to the EmptyLog, which no-ops all logging methods. To install a console logger, in the main class call: const log = require ( '@leverege/log' ); log . setAdapter ( log . ConsoleLog () ) To install a bunyan logger, call: const bunyanLog = bunyan . createLogger ( { ... } ); log . setAdapter ( log . BunyanLog ( bunyanLog ) );","title":"Usage"},{"location":"server-libraries/log/#automated-log-creation-and-installation","text":"If you want to install a log entirely from a configuration variable (JSON, file, or plain object) call: log . createAdapter ( < config - object , JSON string , or file > , < params - object > , < install - boolean > ) This function will create an Adapter based on your configuration, can parameterize your configuration if you are using a string or file, and will automatically call setAdapter if install is not false. a configuration may look like: const logFile = '/var/config/transponder/logConfig.json' or const logString = '{ \"type\" : \"bunyan\", \"name\" : \"${logName}\", \"streams\" : [ { \"type\" : \"rotating-file\", \"period\" : \"1d\", \"count\" : 3, \"level\" : \"warn\", \"path\" : \"./logs3/${logName}\" }, { \"type\" : \"stream\", \"logName\" : \"${resource}\" } ] }' or const logObject = { type : 'console' , mod : 'transponder' } In order to make the second JSON string configuration work, a parameters object would be supplied like so: const logParams = { logName : ClusterManager . logName ( 'transponder' ), resource : `transponder- ${ process . env . TRANSPONDER_RUN_MODE || 'all' } ` , logServiceAccount : process . env . LOG_SERVICE_ACCOUNT } So the installation would look like the following: log . createAdapter ( logString , params , true )","title":"Automated Log Creation and Installation"},{"location":"server-libraries/log/#logging","text":"To log, call: const log = require ( '@leverege/log' ); log . trace ( 'trace log' ); log . debug ( 'debug log' ); log . info ( 'info log' ); log . warn ( 'warn log' ); log . fatal ( 'fatal log' ); The log methods can take an object as the first argument. See bunyans log for more information. Other methods include: child, setLevel, getLevel, isTrace, isDebug, isInfo, isWarn, isError, isError, isFatal","title":"Logging"},{"location":"server-libraries/message-queue/","text":"This library creates interfaces for sending and receiving messages on a queuing system. Quick Usage \u00b6 BREAKING CHANGE: As of version 4.0.0, this lib has been updated to an ESM Module. The createReader() and createWriter() methods are now asynchronous. They will also automatically invoke start() unless the last argument is false. import MsgQueue from 'message-queue' const { MessageProcessor } = MsgQueue; const { transportConfig, topic, channel } = options; // Create a writer const writer = await MsgQueue.createWriter( { transportConfig } ); await writer.start(); // Create the process message function. It should return a promise const processMessage = function( message, messageOptions ) { return new Promise( ( resolve, reject ) => { // process message }) } // Give the process function to a MessageProcessor that handles autoAck and routing const processor = new MessageProcessor( { writer, processor : processMessage } ) const reader = await MsgQueue.createReader( { transportConfig, topic, channel, processor } ); await reader.start() Optionally, you can derive from MessageProcessor: const { MessageProcessor } = require( 'message-queue' ) class MyProcessor extends MessageProcessor { process( message, messageOptions ) { return new Promise( ( resolve, reject ) => { /* process */ }) } } and create one of those to give to the reader: const processor = new MyProcessor( { ...myOptions, writer } ); const reader = MsgQueue.createReader( { transportConfig, topic, channel, processor } ); await reader.start() Note about Config \u00b6 The createReader and createWriter methods allow the transport options to be passed in to the methods as the option transportConfig . The value of this can be a JSON string, the path to a file that will be read via require , or an object with the options. This allows for easier configuration via environment variables: MsgQueue.createReader( { transportConfig : process.env.TRANSPORT_CONFIG || { type : 'nsq', host : process.env.NSQ_HOST_ADDR || '127.0.0.1', port : process.env.NSQ_HOST_PORT || 4150 }, topic : 'myTopic', channel : 'defaultChannel', processor : p } ) The above will create a reader listening to the myTopic topic on the defaultChannel , and will process messages with processor p . If process.env.TRANSPORT_CONFIG is not defined, a default NSQ reader will be used as the transport. Writer Creation \u00b6 Create a writer by using the MsqQueue.createWriter( options ) method. The options will define the type of writer that is created and its options. NSQ \u00b6 To create an NSQ writer, use these options let options = { type : 'nsq', // Kind of writer name : <string>, // used for logging. Defaults to 'NsqWriter' host : <string>, // the host name of the nsq server. eg: '127.0.0.1', port : <string|number>, // the port of the server to talk on. eg: 4150 } Pub/Sub (Google) \u00b6 To create an Google Pub/Sub writer, use these options let options = { type : 'pubsub', // Kind of writer name : <string>, // used for logging. Defaults to 'PubSubWriter' projectId : <string>, // The google projectId, keyFilename : <string>, // The location of the service account JSON file. publisher : { batching : { // The number of bytes to cache before sending the message. Default Math.pow(1024, 2) * 5, maxBytes: <number>, // The number of messages to cache before they are sent. Defaults to 1000 maxMessages: <number>, // The max milliseconds to cache before sending. Default to 1000 maxMilliseconds: <number> } } } Note, by default the Pub/Sub publisher will delay a second before sending, so setting batching.maxMilliseconds to something small(0) will cause it to send messages faster. Writer Usage \u00b6 const writer = MsqQueue.createWriter( options ) writer.start( ) .then( onReady ) .catch ( onErr ) Once you have the writer, there are two different publish methods publish or route . Both can take an option callback( err ) method, and if it is not supplied, a promise will be returned instead. publish( topicName, message, callback ) is the lowest level method and is used by route() . It will put the message onto the specified topic for consumption. The message should be a JSON object. The route( routes, message, callback ) method will send the message on the specified routing path. The routes object can be an array, object, or string that determines how to send the message. In the case of an array, the message will be sent to the next topic handler and it is up to that handler to continue the route supplied. In the case of an object (see Route for format), the message will be published to many topics at once. Each of those handlers must follow any specified continuation route. If the routes is a string, it behaves like publish(). Along with routing, an options object can be sent with the message to allow it processed in dynamic ways (like specify a script, or thresholds) Reader Creation \u00b6 Create a reader by using the MsqQueue.createReader( options ) method. The options will define the type of reader that is created and its options. NSQ \u00b6 To create an NSQ reader, use these options: let options = { type : 'nsq', // Kind of writer name : <string>, // used for logging. Defaults to 'NsqReader( topic / channel )' host : <string>, // the host name of the nsq server. eg: '127.0.0.1', port : <string|number>, // the port of the server to talk on. eg: 4150 topic : <string>, // The name of the topic to listen to. Required. channel : <string> // The channel name/group to use, defaults <topic>-default-processor processor : <MessageProcessor, function, or object with onMessage> // Required } Pub/Sub (Google) \u00b6 To create an Google Pub/Sub reader, use these options: let options = { type : 'pubsub', // Kind of writer name : <string>, // used for logging. Defaults to 'PubSubWriter' projectId : <string>, // The googel projectId. Required keyFilename : <string>, // The location of the service account json file. Required topic : <string>, // The name of the topic to listen to. Required subscription : <string> // The name of the subscription to use, defaults to <topic>-default-processor processor : <MessageProcessor, function, or object with onMessage> // Required } As a note, Pub/Sub rejects attempts to send to any topic that does not exist. By default, The PubSubReader will create the topic and the subscription attached to it. Subscription names are global to the project, so using the topic name in the subscription name is suggested. Reader Usage \u00b6 When creating a reader, the options object requires a processor field. This can be either a function( Message ) or an object (preferably an instance of MessageProcessor ) that has an onMessage( Message ) method. Both of these should process the message and return a Promise. const reader = MsqQueue.createReader( options ) reader.start( ) .then( onReady ) .catch ( onErr ) When reader receives a message from its source, it is wrapped in an implementation of Message and given to the processor to process. Message \u00b6 The Message object attempts to adapt messages from different queue types into a standard interface. The Message object will also handle breaking out of any routing information that has been passed along. To get the actual message to process, use Message.getMessage() . In some cases, Message.json() will equal Message.getMessage() , but if the message contains routing information, Message.getMessage() will actually return the message to route. Messages must be accepted ( Message.ack() ) or rejected ( Message.noAck() ). Be default, the MessageProcessor can be setup to call ack() before processing ( options.autoAck : 'onReceipt' ), after processing ( options.autoAck : 'afterProcess' ), or never ( options.autoAck : 'none' ). If the never option is chosen, the process call must make sure to either call ack() or noAck() before completing or the message may be redelivered. If autoAck is 'afterProcess' , the options.ackOnError determines if ack() or noAck() will be called on the message. Routes \u00b6 As mentioned, a Message can be published along a route by calling the publish() method on the Writer. A route consists of a collection of topics to send to and how to continue sending data along a chain. It is up to the Reader/Processor to continue a route if indicated and not cancelled by processor itself. The message sent to the next topic can be modified or replaced depending on the processor. Route definition has a long form and some shortcuts. To publish a message to one topic, the route would look like: const route = { topic : 'myTopic', options : null, route : null, terminate : false } writer.publish( route, msg ) The options, routes and terminate fields default to null, null, and false and can be left out. const route = { topic : 'myTopic' } writer.publish( route, msg ) The route, since all it indicates is a topic, can be written as a string for a shortcut: const route = 'myTopic' writer.publish( route, msg ) This is effectively the same call as publish( 'myTopic', msg ) . So a single route target can either be a topic name (ie, the string) or an object containing a topic, options, routes, and a terminate flag. Sending in Series \u00b6 To send a message to another topic after it is finished being processed by a first, the route object of the topic can be populated with the follow on route. const route = { topic : 'myTopic' route : { topic : 'myNextTopic' } } writer.publish( route, msg ) This will send msg to myTopic, and then the processor of 'myTopic' will send the results to the 'myNextTopic' topic. If after 'myNextTopic' is completed, you wanted to go to another location, that topic object could also have a route object. This can continue deeper and deepr. const route = { topic : 'myTopic' route : { topic : 'myNextTopic', route : { topic : 'myNextNextTopic', route : { topic : 'myNextNextNextTopic' } } } } writer.publish( route, msg ) Because that can be a little difficult to follow, the array object can be used as a shortcut: const route = [ { topic : 'myTopic' }, { topic : 'myNextTopic' }, { topic : 'myNextNextTopic' }, { topic : 'myNextNextNextTopic' } ] writer.publish( route, msg ) or, since the above example has no other values except topic names: const route = [ 'myTopic', 'myNextTopic', 'myNextNextTopic', 'myNextNextNextTopic' ] writer.publish( route, msg ) Sending in Parallel \u00b6 Sometimes you want to route messages to mulitple places at once. To do this, you can use the concurrent route object. const route = { type : 'concurrent', routes : [ { topic : 'A' }, { topic : 'B' } ] } writer.publish( route, msg ) In this case, msg will be sent to topic A and topic B at the same time. A shortened version of this can use the object notation without the 'type' equaling 'concurrent': const route = { A : true, B : true } writer.publish( route, msg ) Each of those topics (A, B) could be directed to send to more routes after they process, or have options. For example: const route = { type : 'concurrent', routes : [ { topic : 'A', route : [ 'A0', 'A1' ] }, { topic : 'B', route : [ 'B0' ] } ] } writer.publish( route, msg ) The above would result in two concurrent paths for the msg: A -> A0 -> A1 and B -> B0. In the above example the route arrays are represented as shortcuts, but could have use the expanded form. The concurrent object has a shortened version as well. As a note, if you wish to send to the same topic but with different options, this form CANNOT be used and the above 'concurrent' form must be used. The above can be shortend to: const route = { A : [ 'A0', 'A1' ], B : [ 'B0' ] } writer.publish( route, msg ) If B0 required options, it could be expanded. Note, in the shortened concurrent for, the topic that would normal be specified by the 'topic' field in the object is overridden by the concurrent shortcut's key. Because of this, if you need to send mulitple times to the same topic but with different options (ie Reason Scripts), the concurrent shortcut cannot be used. const route = { A : [ 'A0', 'A1' ], B : { type : 'B0', options : { v : 12 } } } writer.publish( route, msg ) The Terminate Flag \u00b6 In the case of concurrency, we run into a potential problem. Consider this: const route = [ { type : 'concurrent', routes : [ { topic : 'A', route : [ 'A0', 'A1' ] }, { topic : 'B', route : [ 'B0' ] } { topic : 'C', route : { type : 'concurrent', routes : [ 'C0', 'C1' ] } } ] }, 'D' ] writer.publish( route, msg ) As is, this would result in several paths for the message: A -> A0 -> A1 -> D B -> B0 -> D C -> C0 -> D C -> C1 -> D What if we didn't want C0 and C1 to continue to D? This is where the terminate flag comes in. Setting terminate to true in the 'C' block would cancel the forwarding to D. const route = [ { type : 'concurrent', routes : [ { topic : 'A', route : [ 'A0', 'A1' ] }, { topic : 'B', route : [ 'B0' ] } { topic : 'C', route : { type : 'concurrent', routes : [ 'C0', 'C1' ] }, terminate : true } ] }, 'D' ] writer.publish( route, msg ) The above would result in the following paths for the message: A -> A0 -> A1 -> D B -> B0 -> D C -> C0 C -> C1","title":"Message Queue"},{"location":"server-libraries/message-queue/#quick-usage","text":"BREAKING CHANGE: As of version 4.0.0, this lib has been updated to an ESM Module. The createReader() and createWriter() methods are now asynchronous. They will also automatically invoke start() unless the last argument is false. import MsgQueue from 'message-queue' const { MessageProcessor } = MsgQueue; const { transportConfig, topic, channel } = options; // Create a writer const writer = await MsgQueue.createWriter( { transportConfig } ); await writer.start(); // Create the process message function. It should return a promise const processMessage = function( message, messageOptions ) { return new Promise( ( resolve, reject ) => { // process message }) } // Give the process function to a MessageProcessor that handles autoAck and routing const processor = new MessageProcessor( { writer, processor : processMessage } ) const reader = await MsgQueue.createReader( { transportConfig, topic, channel, processor } ); await reader.start() Optionally, you can derive from MessageProcessor: const { MessageProcessor } = require( 'message-queue' ) class MyProcessor extends MessageProcessor { process( message, messageOptions ) { return new Promise( ( resolve, reject ) => { /* process */ }) } } and create one of those to give to the reader: const processor = new MyProcessor( { ...myOptions, writer } ); const reader = MsgQueue.createReader( { transportConfig, topic, channel, processor } ); await reader.start()","title":"Quick Usage"},{"location":"server-libraries/message-queue/#note-about-config","text":"The createReader and createWriter methods allow the transport options to be passed in to the methods as the option transportConfig . The value of this can be a JSON string, the path to a file that will be read via require , or an object with the options. This allows for easier configuration via environment variables: MsgQueue.createReader( { transportConfig : process.env.TRANSPORT_CONFIG || { type : 'nsq', host : process.env.NSQ_HOST_ADDR || '127.0.0.1', port : process.env.NSQ_HOST_PORT || 4150 }, topic : 'myTopic', channel : 'defaultChannel', processor : p } ) The above will create a reader listening to the myTopic topic on the defaultChannel , and will process messages with processor p . If process.env.TRANSPORT_CONFIG is not defined, a default NSQ reader will be used as the transport.","title":"Note about Config"},{"location":"server-libraries/message-queue/#writer-creation","text":"Create a writer by using the MsqQueue.createWriter( options ) method. The options will define the type of writer that is created and its options.","title":"Writer Creation"},{"location":"server-libraries/message-queue/#nsq","text":"To create an NSQ writer, use these options let options = { type : 'nsq', // Kind of writer name : <string>, // used for logging. Defaults to 'NsqWriter' host : <string>, // the host name of the nsq server. eg: '127.0.0.1', port : <string|number>, // the port of the server to talk on. eg: 4150 }","title":"NSQ"},{"location":"server-libraries/message-queue/#pubsub-google","text":"To create an Google Pub/Sub writer, use these options let options = { type : 'pubsub', // Kind of writer name : <string>, // used for logging. Defaults to 'PubSubWriter' projectId : <string>, // The google projectId, keyFilename : <string>, // The location of the service account JSON file. publisher : { batching : { // The number of bytes to cache before sending the message. Default Math.pow(1024, 2) * 5, maxBytes: <number>, // The number of messages to cache before they are sent. Defaults to 1000 maxMessages: <number>, // The max milliseconds to cache before sending. Default to 1000 maxMilliseconds: <number> } } } Note, by default the Pub/Sub publisher will delay a second before sending, so setting batching.maxMilliseconds to something small(0) will cause it to send messages faster.","title":"Pub/Sub (Google)"},{"location":"server-libraries/message-queue/#writer-usage","text":"const writer = MsqQueue.createWriter( options ) writer.start( ) .then( onReady ) .catch ( onErr ) Once you have the writer, there are two different publish methods publish or route . Both can take an option callback( err ) method, and if it is not supplied, a promise will be returned instead. publish( topicName, message, callback ) is the lowest level method and is used by route() . It will put the message onto the specified topic for consumption. The message should be a JSON object. The route( routes, message, callback ) method will send the message on the specified routing path. The routes object can be an array, object, or string that determines how to send the message. In the case of an array, the message will be sent to the next topic handler and it is up to that handler to continue the route supplied. In the case of an object (see Route for format), the message will be published to many topics at once. Each of those handlers must follow any specified continuation route. If the routes is a string, it behaves like publish(). Along with routing, an options object can be sent with the message to allow it processed in dynamic ways (like specify a script, or thresholds)","title":"Writer Usage"},{"location":"server-libraries/message-queue/#reader-creation","text":"Create a reader by using the MsqQueue.createReader( options ) method. The options will define the type of reader that is created and its options.","title":"Reader Creation"},{"location":"server-libraries/message-queue/#nsq_1","text":"To create an NSQ reader, use these options: let options = { type : 'nsq', // Kind of writer name : <string>, // used for logging. Defaults to 'NsqReader( topic / channel )' host : <string>, // the host name of the nsq server. eg: '127.0.0.1', port : <string|number>, // the port of the server to talk on. eg: 4150 topic : <string>, // The name of the topic to listen to. Required. channel : <string> // The channel name/group to use, defaults <topic>-default-processor processor : <MessageProcessor, function, or object with onMessage> // Required }","title":"NSQ"},{"location":"server-libraries/message-queue/#pubsub-google_1","text":"To create an Google Pub/Sub reader, use these options: let options = { type : 'pubsub', // Kind of writer name : <string>, // used for logging. Defaults to 'PubSubWriter' projectId : <string>, // The googel projectId. Required keyFilename : <string>, // The location of the service account json file. Required topic : <string>, // The name of the topic to listen to. Required subscription : <string> // The name of the subscription to use, defaults to <topic>-default-processor processor : <MessageProcessor, function, or object with onMessage> // Required } As a note, Pub/Sub rejects attempts to send to any topic that does not exist. By default, The PubSubReader will create the topic and the subscription attached to it. Subscription names are global to the project, so using the topic name in the subscription name is suggested.","title":"Pub/Sub (Google)"},{"location":"server-libraries/message-queue/#reader-usage","text":"When creating a reader, the options object requires a processor field. This can be either a function( Message ) or an object (preferably an instance of MessageProcessor ) that has an onMessage( Message ) method. Both of these should process the message and return a Promise. const reader = MsqQueue.createReader( options ) reader.start( ) .then( onReady ) .catch ( onErr ) When reader receives a message from its source, it is wrapped in an implementation of Message and given to the processor to process.","title":"Reader Usage"},{"location":"server-libraries/message-queue/#message","text":"The Message object attempts to adapt messages from different queue types into a standard interface. The Message object will also handle breaking out of any routing information that has been passed along. To get the actual message to process, use Message.getMessage() . In some cases, Message.json() will equal Message.getMessage() , but if the message contains routing information, Message.getMessage() will actually return the message to route. Messages must be accepted ( Message.ack() ) or rejected ( Message.noAck() ). Be default, the MessageProcessor can be setup to call ack() before processing ( options.autoAck : 'onReceipt' ), after processing ( options.autoAck : 'afterProcess' ), or never ( options.autoAck : 'none' ). If the never option is chosen, the process call must make sure to either call ack() or noAck() before completing or the message may be redelivered. If autoAck is 'afterProcess' , the options.ackOnError determines if ack() or noAck() will be called on the message.","title":"Message"},{"location":"server-libraries/message-queue/#routes","text":"As mentioned, a Message can be published along a route by calling the publish() method on the Writer. A route consists of a collection of topics to send to and how to continue sending data along a chain. It is up to the Reader/Processor to continue a route if indicated and not cancelled by processor itself. The message sent to the next topic can be modified or replaced depending on the processor. Route definition has a long form and some shortcuts. To publish a message to one topic, the route would look like: const route = { topic : 'myTopic', options : null, route : null, terminate : false } writer.publish( route, msg ) The options, routes and terminate fields default to null, null, and false and can be left out. const route = { topic : 'myTopic' } writer.publish( route, msg ) The route, since all it indicates is a topic, can be written as a string for a shortcut: const route = 'myTopic' writer.publish( route, msg ) This is effectively the same call as publish( 'myTopic', msg ) . So a single route target can either be a topic name (ie, the string) or an object containing a topic, options, routes, and a terminate flag.","title":"Routes"},{"location":"server-libraries/message-queue/#sending-in-series","text":"To send a message to another topic after it is finished being processed by a first, the route object of the topic can be populated with the follow on route. const route = { topic : 'myTopic' route : { topic : 'myNextTopic' } } writer.publish( route, msg ) This will send msg to myTopic, and then the processor of 'myTopic' will send the results to the 'myNextTopic' topic. If after 'myNextTopic' is completed, you wanted to go to another location, that topic object could also have a route object. This can continue deeper and deepr. const route = { topic : 'myTopic' route : { topic : 'myNextTopic', route : { topic : 'myNextNextTopic', route : { topic : 'myNextNextNextTopic' } } } } writer.publish( route, msg ) Because that can be a little difficult to follow, the array object can be used as a shortcut: const route = [ { topic : 'myTopic' }, { topic : 'myNextTopic' }, { topic : 'myNextNextTopic' }, { topic : 'myNextNextNextTopic' } ] writer.publish( route, msg ) or, since the above example has no other values except topic names: const route = [ 'myTopic', 'myNextTopic', 'myNextNextTopic', 'myNextNextNextTopic' ] writer.publish( route, msg )","title":"Sending in Series"},{"location":"server-libraries/message-queue/#sending-in-parallel","text":"Sometimes you want to route messages to mulitple places at once. To do this, you can use the concurrent route object. const route = { type : 'concurrent', routes : [ { topic : 'A' }, { topic : 'B' } ] } writer.publish( route, msg ) In this case, msg will be sent to topic A and topic B at the same time. A shortened version of this can use the object notation without the 'type' equaling 'concurrent': const route = { A : true, B : true } writer.publish( route, msg ) Each of those topics (A, B) could be directed to send to more routes after they process, or have options. For example: const route = { type : 'concurrent', routes : [ { topic : 'A', route : [ 'A0', 'A1' ] }, { topic : 'B', route : [ 'B0' ] } ] } writer.publish( route, msg ) The above would result in two concurrent paths for the msg: A -> A0 -> A1 and B -> B0. In the above example the route arrays are represented as shortcuts, but could have use the expanded form. The concurrent object has a shortened version as well. As a note, if you wish to send to the same topic but with different options, this form CANNOT be used and the above 'concurrent' form must be used. The above can be shortend to: const route = { A : [ 'A0', 'A1' ], B : [ 'B0' ] } writer.publish( route, msg ) If B0 required options, it could be expanded. Note, in the shortened concurrent for, the topic that would normal be specified by the 'topic' field in the object is overridden by the concurrent shortcut's key. Because of this, if you need to send mulitple times to the same topic but with different options (ie Reason Scripts), the concurrent shortcut cannot be used. const route = { A : [ 'A0', 'A1' ], B : { type : 'B0', options : { v : 12 } } } writer.publish( route, msg )","title":"Sending in Parallel"},{"location":"server-libraries/message-queue/#the-terminate-flag","text":"In the case of concurrency, we run into a potential problem. Consider this: const route = [ { type : 'concurrent', routes : [ { topic : 'A', route : [ 'A0', 'A1' ] }, { topic : 'B', route : [ 'B0' ] } { topic : 'C', route : { type : 'concurrent', routes : [ 'C0', 'C1' ] } } ] }, 'D' ] writer.publish( route, msg ) As is, this would result in several paths for the message: A -> A0 -> A1 -> D B -> B0 -> D C -> C0 -> D C -> C1 -> D What if we didn't want C0 and C1 to continue to D? This is where the terminate flag comes in. Setting terminate to true in the 'C' block would cancel the forwarding to D. const route = [ { type : 'concurrent', routes : [ { topic : 'A', route : [ 'A0', 'A1' ] }, { topic : 'B', route : [ 'B0' ] } { topic : 'C', route : { type : 'concurrent', routes : [ 'C0', 'C1' ] }, terminate : true } ] }, 'D' ] writer.publish( route, msg ) The above would result in the following paths for the message: A -> A0 -> A1 -> D B -> B0 -> D C -> C0 C -> C1","title":"The Terminate Flag"},{"location":"server-libraries/messages/","text":"Messages \u00b6 This library supplies message validation and some object description classes. Setup \u00b6 const Messages = require( '@leverege/messages' ) Quick Usage \u00b6 When a JSON object is received from a source like MessageQueue or a POST message, it can be given to the Messages factory to construct a new object. If the format is invalid, an exception will be thrown. If the type cannot be deteremined, null will be returned. const Messages = require( '@leverege/messages' ); const { SetTimerMsg } = Messages; // Create a message from a javascript object. // This will return null if the message type cannot be determined. // This will throw an exception if the message type is correct but // the data in it is not const msg = Messages.create( dataObj ) From the // This will throw an exception if myTimerData is inappropriate. The type // field will automatically be set const timerMsg = new SetTimerMsg( myTimerData ) Messages \u00b6 Leverege microservices uses Messages to communicate with each other. In addition to this, creating messages and sending them to various microservices can trigger various functionality. For example, sending an emailMsg to Emailer will cause emailer to send out emails to the addresses specified in the message. Device \u00b6 Device messages are used for dealing with inbound and outbound data. In addition they are used for creating events. The details for each device message can be found below. deviceDataEventMsg \u00b6 Field Type Description type 'deviceDataEventMsg' id string A uuid generated for this message time string receivedTime string storedTime string historical boolean whether or not the message is known to be historical only deviceId string A string representing the internal name of the device. systemId string The name of the system the device belongs to blueprintId string The id of the blueprint the device belongs to projectId string The id of the project the device belongs to networkId string The networkId of the source data array data.path string A string of keys, separated by a \u2018/\u2019. For examples, \u2018speed\u2019, or data.value any the value of the path event object This contains an event report. The event indicates that something source object this contains the device, user, or reason script source info of the userId string the userId (if any) that initiated the writing of information deviceAckMsg \u00b6 { type : 'deviceAckMsg', time : string || integer || date object || null, // the time of the message, as reported by the device receivedTime : string || integer || date object || null, // the time of the message, as reported by the ingestion server msgId : string, // The message id, created at outbound initiated step deviceId : string, // A string representing the internal id of the device. systemId : string, // The id of the system the device belongs to networkId : string, // The networkId of the source blueprintId : string, // the id of the blueprint that the device belongs to projectId : string, // the id of the project that the device belongs to status : string, // final updated string status of the message response : object, // This contains the response data from the device source : object // this contains the device, (or less likely) the user, or reason script source info of the inboundAckMsg this derived from } deviceEventAckMsg \u00b6 { type : 'deviceEventAckMsg', id : string, // A uuid generated for this message time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server storedTime : string || integer, // the time of the message, as reported by the writer write to rt dbs) deviceId : string, // A string representing the internal name of the device. systemId : string, // The name of the system the device belongs to blueprintId : string, // The id of the blueprint the device belongs to projectId : string, // The id of the project the device belongs to data : array || object, // This contains a data report. The data report as a path, which is the name of the data field, and a value. The path should only contain a-z, A-Z, 0-9, _. Forward slashes (\u2018/\u2019) are treated as directories data.path : string, // A string of keys, separated by a \u2018/\u2019. For examples, \u2018speed\u2019, or \u2018battery/temperature1\u201d data.value : any, // the value of the path eventType : string, // This contains an event report. The event indicates that something significant has happened. source : object, // this contains the device, user, or reason script source info of the object that created this msg userId : string // the userId (if any) that initiated the writing of information } inboundAckMsg \u00b6 { type : 'inboundAckMsg', time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server storedTime : string || integer, // the time of the message, as reported by the writer msgId : string, // The message id, created at outbound initiated step networkId : string, // The id of network contributing the acknowledgement deviceId : string, // A string representing the external name of the device. This could be an IotHub device id, a MAC address of a sensor, etc systemId : string, // the id of the system the device belongs to aliasKey : string, // The name of the key that forms the mapping between the external device\u2019s id and the internal device. This is stored in the internal device\u2019s aliases list, and tends to be unique per network. status : string, // an updated string status of the message response : object // This contains the response data from the device } outboundInitMsg \u00b6 { type : 'outboundInitMsg', time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server msgId : string, // The message id, created at outbound initiated step deviceId : string, // A string representing the internal name of the device. systemId : string, // The name of the system the device belongs to blueprintId : string, // the id of the blueprint that the device belongs to projectId : string, // the id of the project that the device belongs to message : object, // This contains the message to go out to the device source : object // information about the device, (or less likely) the user, or reason script that generated this message } outboundSentMsg \u00b6 { type : 'outboundSentMsg', time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server msgId : string, // The message id, created at outbound initiated step deviceId : string, // A string representing the internal name of the device. systemId : string, // The name of the system the device belongs to blueprintId : string, // the id of the blueprint that the device belongs to projectId : string, // the id of the project that the device belongs to status : string, // an updated string status of the device sendInfo : object, // any information or notes about the data sent source : object // information about the device, (or less likely) the user, or reason script that generated this message } Notifications \u00b6 Notification are messages that will result in a message that will be sent to a user in the form of an email or sms. emailMsg \u00b6 An emailMsg is sent to the emailer microservice. The result will be an email being sent with the desired content to a recipient or list of recipients. { type : 'emailMsg', subject : string, // the subject line of the email to be sent to : string || array, // to the email address/es to send the email to cc : string || array, // cc the email address/es to cc on the email bcc : string || array, // bcc the email address/es to bcc on the email from : string, // from the email address the email is to be sent from html : string, // html email content as html text : string, // text email content as plain text template : string, // email template to use as the message context : object, // context contains all the information that will be substituted into the template attachments : array // attachments attachments to be included with the email blindTo : boolean // whether 'to' field should send individual messages (batch send) instead of one msg to each recipient (Note: if blindTo is true, then cc and bcc are not allowed) } smsMsg \u00b6 A smsMsg is sent to the messenger microservice. The result will be a sms being sent with the desired content to a recipient or list of recipients. { recipients : array || string, // recipients the phone number/s to send the message to message : string, // message the text of the message that will be sent } Scheduler \u00b6 The Scheduler microservice manages timers on the Leverege platform. Instead of using setTimeout or setInterval , timers should created for scheduler to manage. Scheduler save timers to a database and reads them from the Database. It is more scalable then using timeouts and intervals. In addition to this, all Leverege microservices are stateless, and there is no guarantee that a timeout will be run. setTimerMsg \u00b6 A setTimerMsg sent to the scheduler microservice will create a timer and start it. After it has run, it will trigger the functionality that it has specified as its action . For more information on action s and timer s refer to scheduler.md. { type : 'setTimerMsg`, id : string, // identifier that will be used for the timer, unique projectId : string || null, systemId : string || null, deviceId : string || null, scriptId : string || null, timer : Timer, action : Action, patchAction : boolean || null } A timer can be reset by sending another setTimerMsg with the same id . Setting patchAction to true will patch the action of an existing setTimerMsg , but will not reset the timer. cancelTimerMsg \u00b6 A cancelTimerMsg sent to the scheduler microservice will cancel a timer that has been set/created. { type : 'cancelTimerMsg', id : string, // identifier of the timer to cancel } cancelRelatedTimerMsg \u00b6 A cancelRelatedTimerMsg set to the scheduler microservice will cancel all timers related to a device, system, project or script. { type : 'cancelRelatedTimerMsg', projectId : string || null, systemId : string || null, deviceId : string || null, scriptId : string || null } Reason \u00b6 To send a message to a Reason Script for processing, a message of type routeMsg should be sent to the topic reason must be sent. For example: const runScript = { type : 'routeMsg', message : { type : 'reasonRunScript', // Other data of you choice. This will be deliever as `message` to the script } options : { reasoner : { id : '<MyScriptId>', // The id of the reason script targetFunction : 'run', // The name of the function in the script to run. Defaults to 'run' // Other params as desired. These are passed to the script // as context.parameters param1 : <any> } } } messageWriter.send( 'reason', runScript ) Custom \u00b6 In addition to this, using the @leverege/message-queue library, a message reader can be created. This reader will listen on a topic for message, and upon receiving them, can execute custom functionality. For more details refer to the @leverege/message-queue documentation. Structures \u00b6 Timers \u00b6 A timer is used to run functionality on the Leverege platform. Timers consist of two parts, an action that is the functionality to be run once the timer has triggered, and a timer that details when the timer will trigger. Instead of using setTimeout or setInterval , timers should created for scheduler to manage. Scheduler save timers to a database and reads them from the Database. It is more scalable then using timeouts and intervals. In addition to this, all Leverege microservices are stateless, and there is no guarantee that a timeout will be run. There are two different types of timers, managed and unmanaged. Managed timers are created by sending a request to api-server, and api-server maintains a database of all the managed timers. Managed timers can be started and stopped. When a managed timer is started, a setTimerMsg is send to the scheduler microservice from api-server. An unmanaged timer is created by directly sending a setTimerMsg to scheduler. Scheduler only has a record of active timers, and all completed timers are forgotten. If a timer needs to be reused, it is recommended to use an managed timer. Timer \u00b6 Timers dictate when the functionality determined by the action will be run. Timers are a JSON object with a key of type and another key to dictate when the timer will run, this key is based off of the type of timer. There are several different types of timers, once , repeating and scheduled . once \u00b6 Once timers execute their functionality one time. They have either a delay key or a when . delay is a number in milliseconds, which is the amount of time the timer will wait after being started to execute. when is a timestamp in milliseconds, that is the specific time that the functionality will be executed. Field Type Description type 'once' The type of the timer. Must be 'once' delay [number] The time in milliseconds to wait before triggering the timer when [number] The timestamp (milliseconds since epoch) at which the timer should be triggered repeating \u00b6 Repeating timers execute at a set interval. They have a delay key. Field Type Description type 'repeating' The type of the timer. Must be 'repeating' delay number The time in milliseconds between timer triggers runImmediately [boolean] if true will run the functionality when the timer gets started scheduled \u00b6 Scheduled timers run on a set schedule. They have a cron key that takes a cron expression. In addition to this there is a timezone key that can be used to define the timezone of the cron string. Field Type Description type 'scheduled' The type of the timer. Must be 'scheduled' cron Cron Expression The cron expression used by the timer timezone string The timezone solar \u00b6 Scheduled timers run on a set schedule. They have a cron key that takes a cron expression. In addition to this there is a timezone key that can be used to define the timezone of the cron string. Field Type Description type 'solar' The type of the timer. Must be 'solar' cron Cron Expression The optional cron expression used by the timer. This should be the standard cron form ( '* * * * *' ), but hours and minutes will be ignored. latitude number The latitude in degrees of the sun viewer longitude number The longitude in degrees of the sun viewer fromSunrise boolean If true, the timer will run based on sunrise. If false, sunset is used. (Default: true) offset int The offset in milliseconds from either sunrise or sunset based on fromSunrise. A value of (-60*60*1000) would mean an hour before sunrise/sunset Action \u00b6 There are two types of actions that can be triggered by timers. One is to publish a message to a topic inside of the Leverege system. The other action that can be taken is to send a request to a url. publishTopic \u00b6 The publishTopic action injects a message into the Leverege system, starting at the specified topic . A message needs to be included, to be sent to the defined topic. { type : 'publishTopic', topic : 'reason' message : { message : { type : \"reasonRunScript\" }, options : { reasoner : { x : y, id : <id of the reason script to run> } }, type : \"routeMsg\" } } For more detail on how to create the JSON for a message, refer to messages.md. queryUrl \u00b6 The queryUrl action makes a http request. { type : 'queryUrl', url : string, method : string, header : object, body : any }","title":"Messages"},{"location":"server-libraries/messages/#messages","text":"This library supplies message validation and some object description classes.","title":"Messages"},{"location":"server-libraries/messages/#setup","text":"const Messages = require( '@leverege/messages' )","title":"Setup"},{"location":"server-libraries/messages/#quick-usage","text":"When a JSON object is received from a source like MessageQueue or a POST message, it can be given to the Messages factory to construct a new object. If the format is invalid, an exception will be thrown. If the type cannot be deteremined, null will be returned. const Messages = require( '@leverege/messages' ); const { SetTimerMsg } = Messages; // Create a message from a javascript object. // This will return null if the message type cannot be determined. // This will throw an exception if the message type is correct but // the data in it is not const msg = Messages.create( dataObj ) From the // This will throw an exception if myTimerData is inappropriate. The type // field will automatically be set const timerMsg = new SetTimerMsg( myTimerData )","title":"Quick Usage"},{"location":"server-libraries/messages/#messages_1","text":"Leverege microservices uses Messages to communicate with each other. In addition to this, creating messages and sending them to various microservices can trigger various functionality. For example, sending an emailMsg to Emailer will cause emailer to send out emails to the addresses specified in the message.","title":"Messages"},{"location":"server-libraries/messages/#device","text":"Device messages are used for dealing with inbound and outbound data. In addition they are used for creating events. The details for each device message can be found below.","title":"Device"},{"location":"server-libraries/messages/#devicedataeventmsg","text":"Field Type Description type 'deviceDataEventMsg' id string A uuid generated for this message time string receivedTime string storedTime string historical boolean whether or not the message is known to be historical only deviceId string A string representing the internal name of the device. systemId string The name of the system the device belongs to blueprintId string The id of the blueprint the device belongs to projectId string The id of the project the device belongs to networkId string The networkId of the source data array data.path string A string of keys, separated by a \u2018/\u2019. For examples, \u2018speed\u2019, or data.value any the value of the path event object This contains an event report. The event indicates that something source object this contains the device, user, or reason script source info of the userId string the userId (if any) that initiated the writing of information","title":"deviceDataEventMsg"},{"location":"server-libraries/messages/#deviceackmsg","text":"{ type : 'deviceAckMsg', time : string || integer || date object || null, // the time of the message, as reported by the device receivedTime : string || integer || date object || null, // the time of the message, as reported by the ingestion server msgId : string, // The message id, created at outbound initiated step deviceId : string, // A string representing the internal id of the device. systemId : string, // The id of the system the device belongs to networkId : string, // The networkId of the source blueprintId : string, // the id of the blueprint that the device belongs to projectId : string, // the id of the project that the device belongs to status : string, // final updated string status of the message response : object, // This contains the response data from the device source : object // this contains the device, (or less likely) the user, or reason script source info of the inboundAckMsg this derived from }","title":"deviceAckMsg"},{"location":"server-libraries/messages/#deviceeventackmsg","text":"{ type : 'deviceEventAckMsg', id : string, // A uuid generated for this message time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server storedTime : string || integer, // the time of the message, as reported by the writer write to rt dbs) deviceId : string, // A string representing the internal name of the device. systemId : string, // The name of the system the device belongs to blueprintId : string, // The id of the blueprint the device belongs to projectId : string, // The id of the project the device belongs to data : array || object, // This contains a data report. The data report as a path, which is the name of the data field, and a value. The path should only contain a-z, A-Z, 0-9, _. Forward slashes (\u2018/\u2019) are treated as directories data.path : string, // A string of keys, separated by a \u2018/\u2019. For examples, \u2018speed\u2019, or \u2018battery/temperature1\u201d data.value : any, // the value of the path eventType : string, // This contains an event report. The event indicates that something significant has happened. source : object, // this contains the device, user, or reason script source info of the object that created this msg userId : string // the userId (if any) that initiated the writing of information }","title":"deviceEventAckMsg"},{"location":"server-libraries/messages/#inboundackmsg","text":"{ type : 'inboundAckMsg', time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server storedTime : string || integer, // the time of the message, as reported by the writer msgId : string, // The message id, created at outbound initiated step networkId : string, // The id of network contributing the acknowledgement deviceId : string, // A string representing the external name of the device. This could be an IotHub device id, a MAC address of a sensor, etc systemId : string, // the id of the system the device belongs to aliasKey : string, // The name of the key that forms the mapping between the external device\u2019s id and the internal device. This is stored in the internal device\u2019s aliases list, and tends to be unique per network. status : string, // an updated string status of the message response : object // This contains the response data from the device }","title":"inboundAckMsg"},{"location":"server-libraries/messages/#outboundinitmsg","text":"{ type : 'outboundInitMsg', time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server msgId : string, // The message id, created at outbound initiated step deviceId : string, // A string representing the internal name of the device. systemId : string, // The name of the system the device belongs to blueprintId : string, // the id of the blueprint that the device belongs to projectId : string, // the id of the project that the device belongs to message : object, // This contains the message to go out to the device source : object // information about the device, (or less likely) the user, or reason script that generated this message }","title":"outboundInitMsg"},{"location":"server-libraries/messages/#outboundsentmsg","text":"{ type : 'outboundSentMsg', time : string || integer, // the time of the message, as reported by the device receivedTime : string || integer, // the time of the message, as reported by the ingestion server msgId : string, // The message id, created at outbound initiated step deviceId : string, // A string representing the internal name of the device. systemId : string, // The name of the system the device belongs to blueprintId : string, // the id of the blueprint that the device belongs to projectId : string, // the id of the project that the device belongs to status : string, // an updated string status of the device sendInfo : object, // any information or notes about the data sent source : object // information about the device, (or less likely) the user, or reason script that generated this message }","title":"outboundSentMsg"},{"location":"server-libraries/messages/#notifications","text":"Notification are messages that will result in a message that will be sent to a user in the form of an email or sms.","title":"Notifications"},{"location":"server-libraries/messages/#emailmsg","text":"An emailMsg is sent to the emailer microservice. The result will be an email being sent with the desired content to a recipient or list of recipients. { type : 'emailMsg', subject : string, // the subject line of the email to be sent to : string || array, // to the email address/es to send the email to cc : string || array, // cc the email address/es to cc on the email bcc : string || array, // bcc the email address/es to bcc on the email from : string, // from the email address the email is to be sent from html : string, // html email content as html text : string, // text email content as plain text template : string, // email template to use as the message context : object, // context contains all the information that will be substituted into the template attachments : array // attachments attachments to be included with the email blindTo : boolean // whether 'to' field should send individual messages (batch send) instead of one msg to each recipient (Note: if blindTo is true, then cc and bcc are not allowed) }","title":"emailMsg"},{"location":"server-libraries/messages/#smsmsg","text":"A smsMsg is sent to the messenger microservice. The result will be a sms being sent with the desired content to a recipient or list of recipients. { recipients : array || string, // recipients the phone number/s to send the message to message : string, // message the text of the message that will be sent }","title":"smsMsg"},{"location":"server-libraries/messages/#scheduler","text":"The Scheduler microservice manages timers on the Leverege platform. Instead of using setTimeout or setInterval , timers should created for scheduler to manage. Scheduler save timers to a database and reads them from the Database. It is more scalable then using timeouts and intervals. In addition to this, all Leverege microservices are stateless, and there is no guarantee that a timeout will be run.","title":"Scheduler"},{"location":"server-libraries/messages/#settimermsg","text":"A setTimerMsg sent to the scheduler microservice will create a timer and start it. After it has run, it will trigger the functionality that it has specified as its action . For more information on action s and timer s refer to scheduler.md. { type : 'setTimerMsg`, id : string, // identifier that will be used for the timer, unique projectId : string || null, systemId : string || null, deviceId : string || null, scriptId : string || null, timer : Timer, action : Action, patchAction : boolean || null } A timer can be reset by sending another setTimerMsg with the same id . Setting patchAction to true will patch the action of an existing setTimerMsg , but will not reset the timer.","title":"setTimerMsg"},{"location":"server-libraries/messages/#canceltimermsg","text":"A cancelTimerMsg sent to the scheduler microservice will cancel a timer that has been set/created. { type : 'cancelTimerMsg', id : string, // identifier of the timer to cancel }","title":"cancelTimerMsg"},{"location":"server-libraries/messages/#cancelrelatedtimermsg","text":"A cancelRelatedTimerMsg set to the scheduler microservice will cancel all timers related to a device, system, project or script. { type : 'cancelRelatedTimerMsg', projectId : string || null, systemId : string || null, deviceId : string || null, scriptId : string || null }","title":"cancelRelatedTimerMsg"},{"location":"server-libraries/messages/#reason","text":"To send a message to a Reason Script for processing, a message of type routeMsg should be sent to the topic reason must be sent. For example: const runScript = { type : 'routeMsg', message : { type : 'reasonRunScript', // Other data of you choice. This will be deliever as `message` to the script } options : { reasoner : { id : '<MyScriptId>', // The id of the reason script targetFunction : 'run', // The name of the function in the script to run. Defaults to 'run' // Other params as desired. These are passed to the script // as context.parameters param1 : <any> } } } messageWriter.send( 'reason', runScript )","title":"Reason"},{"location":"server-libraries/messages/#custom","text":"In addition to this, using the @leverege/message-queue library, a message reader can be created. This reader will listen on a topic for message, and upon receiving them, can execute custom functionality. For more details refer to the @leverege/message-queue documentation.","title":"Custom"},{"location":"server-libraries/messages/#structures","text":"","title":"Structures"},{"location":"server-libraries/messages/#timers","text":"A timer is used to run functionality on the Leverege platform. Timers consist of two parts, an action that is the functionality to be run once the timer has triggered, and a timer that details when the timer will trigger. Instead of using setTimeout or setInterval , timers should created for scheduler to manage. Scheduler save timers to a database and reads them from the Database. It is more scalable then using timeouts and intervals. In addition to this, all Leverege microservices are stateless, and there is no guarantee that a timeout will be run. There are two different types of timers, managed and unmanaged. Managed timers are created by sending a request to api-server, and api-server maintains a database of all the managed timers. Managed timers can be started and stopped. When a managed timer is started, a setTimerMsg is send to the scheduler microservice from api-server. An unmanaged timer is created by directly sending a setTimerMsg to scheduler. Scheduler only has a record of active timers, and all completed timers are forgotten. If a timer needs to be reused, it is recommended to use an managed timer.","title":"Timers"},{"location":"server-libraries/messages/#timer","text":"Timers dictate when the functionality determined by the action will be run. Timers are a JSON object with a key of type and another key to dictate when the timer will run, this key is based off of the type of timer. There are several different types of timers, once , repeating and scheduled .","title":"Timer"},{"location":"server-libraries/messages/#once","text":"Once timers execute their functionality one time. They have either a delay key or a when . delay is a number in milliseconds, which is the amount of time the timer will wait after being started to execute. when is a timestamp in milliseconds, that is the specific time that the functionality will be executed. Field Type Description type 'once' The type of the timer. Must be 'once' delay [number] The time in milliseconds to wait before triggering the timer when [number] The timestamp (milliseconds since epoch) at which the timer should be triggered","title":"once"},{"location":"server-libraries/messages/#repeating","text":"Repeating timers execute at a set interval. They have a delay key. Field Type Description type 'repeating' The type of the timer. Must be 'repeating' delay number The time in milliseconds between timer triggers runImmediately [boolean] if true will run the functionality when the timer gets started","title":"repeating"},{"location":"server-libraries/messages/#scheduled","text":"Scheduled timers run on a set schedule. They have a cron key that takes a cron expression. In addition to this there is a timezone key that can be used to define the timezone of the cron string. Field Type Description type 'scheduled' The type of the timer. Must be 'scheduled' cron Cron Expression The cron expression used by the timer timezone string The timezone","title":"scheduled"},{"location":"server-libraries/messages/#solar","text":"Scheduled timers run on a set schedule. They have a cron key that takes a cron expression. In addition to this there is a timezone key that can be used to define the timezone of the cron string. Field Type Description type 'solar' The type of the timer. Must be 'solar' cron Cron Expression The optional cron expression used by the timer. This should be the standard cron form ( '* * * * *' ), but hours and minutes will be ignored. latitude number The latitude in degrees of the sun viewer longitude number The longitude in degrees of the sun viewer fromSunrise boolean If true, the timer will run based on sunrise. If false, sunset is used. (Default: true) offset int The offset in milliseconds from either sunrise or sunset based on fromSunrise. A value of (-60*60*1000) would mean an hour before sunrise/sunset","title":"solar"},{"location":"server-libraries/messages/#action","text":"There are two types of actions that can be triggered by timers. One is to publish a message to a topic inside of the Leverege system. The other action that can be taken is to send a request to a url.","title":"Action"},{"location":"server-libraries/messages/#publishtopic","text":"The publishTopic action injects a message into the Leverege system, starting at the specified topic . A message needs to be included, to be sent to the defined topic. { type : 'publishTopic', topic : 'reason' message : { message : { type : \"reasonRunScript\" }, options : { reasoner : { x : y, id : <id of the reason script to run> } }, type : \"routeMsg\" } } For more detail on how to create the JSON for a message, refer to messages.md.","title":"publishTopic"},{"location":"server-libraries/messages/#queryurl","text":"The queryUrl action makes a http request. { type : 'queryUrl', url : string, method : string, header : object, body : any }","title":"queryUrl"},{"location":"services/api-server/","text":"API Server \u00b6 API Server is the core of the Leverege Platform. It's the gateway through which clients interact with all of its micro-services, and it contains all of the REST routes that the API UI as well as project UIs use to manage IoT solutions. Documentation for all of the REST routes available can be found in the Leverege API Documentation . The routes allow access to every aspect of the Leverege System. Routes can be used to create Blueprints, Devices, Users, Projects and Systems. It can also be used to edit and view them. There are routes to interact with most of the micro-services. Getting Started \u00b6 Before running npm-start , ensure these services are running on your machine: - NSQ (or other message-queue options like PubSub or SNS/SQS) - Elasticsearch (version 7.x) - Redis - Postgres Migrating from 3.x to 4.x \u00b6 Follow instructions to setup Authz Local Development Configuration \u00b6 npm run decrypt - Open the .env file output and replace the {MY_VALUE} templated values so that API Server can run in your local environment. - Ensure postgres, nsq, redis, and elastic search are all running on your machine. - Start AuthZ Server at least, and probably Transponder and Message Processor as well, depending on your needs. npm start Test Configuration \u00b6 Ensure elasticsearch and postgres services are running locally npm run test Deployment Configuration \u00b6 Helm chart contains all the default values to be used. See this repo's helm directory for more info. Required \u00b6 Name Description MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for models MODEL_FIREBASE_DATABASE_URL url to firebase database for models TEMPLATE_DIR path to the directory templates are stored LOCAL_AUTH_SECRET local secret for connecting to the Auth server or service AUTH0_DOMAIN domain name for Auth0 authentication AUTH0_CLIENT_ID client id for Auth0 AUTH0_CLIENT_SECRET client secret for Auth0 AUTH0_CLIENT_SECRET_B64 Base 64 encoded key for Auth0 DATA_FIREBASE_CONFIG firebase database config DATA_FIREBASE_SERVICE_ACCOUNT firebase database service account DATA_BIGQUERY_SERVICE_ACCOUNT BigQuery database service account DATA_BIGQUERY_PROJECT BigQuery database project SQL_USER sql user SQL_PASSWORD sql password SQL_HOST sql host SQL_PORT sql port SQL_CA sql ssl ca SQL_CLIENT_CERT sql ssl cert SQL_CLIENT_KEY sql ssl key PG_USER postgreSQL user PG_PASSWORD postgreSQL password PG_DATABASE postgreSQL database SIMULATOR_URL url to where simulator is hosted MODEL_SQL_USER user of model sql db MODEL_SQL_PASSWORD password of model sql db NPMRC_VALUE contents will be made into .npmrc file Optional \u00b6 Name Description Default APP_NAME name of app for firebase config 'api-server' SQL_CA sql ssl ca undefined SQL_CLIENT_CERT sql ssl cert undefined SQL_CLIENT_KEY sql ssl key undefined TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, if TRANSPORT_CONFIG is not specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, if TRANSPORT_CONFIG is not specified 4150 SERVER_PORT server port that api will listen on 8181 LOCAL_AUTH_ISSUER auth token issuer 'https://imagine.leverege.com' LOCAL_AUTH_AUDIENCE auth token audience '' LOCAL_AUTH_EXPIRATION expiration time of auth token '1d' INITIAL_USERS default user accounts for imagine null CACHE_CONFIG configuration for the cache db { type, connection : { host, port } } LOCK_CONFIG configuration for the lock db { type, connection : { host, port } } BQ_HISTORY_TAGS tags that are applied to the BigQuery database '[ 'long' ]' SQL_POOL_CONNECTION_LIMIT limit of the number of sql connections 10 MYSQL_HISTORY_TAGS tags that are applied to the MySql database '[ 'short' ]' PG_POOL_CONNECTION_LIMIT limit of the number of postgreSQL connections 10 PG_PORT postgreSQL port 5432 PG_HOST postgreSQL host 'localhost' PG_HISTORY_TAGS tags that are applied to the PostgreSQL database '[ 'tdb' ]' MODEL_SQL_HOST model sql db host 'localhost' MODEL_SQL_PORT model sql db port 3306 API_SERVICE_TOPIC topic that is assigned to the apiWorker message processor 'api-service-topic' GCF_FIREBASE_SERVICE_ACCOUNT service account for google cloud functions DATA_FIREBASE_SERVICE_ACCOUNT SCRIPT_BUILD_DIR directory where scripts will be deployed to '/tmp' REASONER_LIB_VERSION deployed reasoner version '1.0.7' SCHEDULER_URL url where scheduler is hosted 'http://localhost:9937'","title":"API Server"},{"location":"services/api-server/#api-server","text":"API Server is the core of the Leverege Platform. It's the gateway through which clients interact with all of its micro-services, and it contains all of the REST routes that the API UI as well as project UIs use to manage IoT solutions. Documentation for all of the REST routes available can be found in the Leverege API Documentation . The routes allow access to every aspect of the Leverege System. Routes can be used to create Blueprints, Devices, Users, Projects and Systems. It can also be used to edit and view them. There are routes to interact with most of the micro-services.","title":"API Server"},{"location":"services/api-server/#getting-started","text":"Before running npm-start , ensure these services are running on your machine: - NSQ (or other message-queue options like PubSub or SNS/SQS) - Elasticsearch (version 7.x) - Redis - Postgres","title":"Getting Started"},{"location":"services/api-server/#migrating-from-3x-to-4x","text":"Follow instructions to setup Authz","title":"Migrating from 3.x to 4.x"},{"location":"services/api-server/#local-development-configuration","text":"npm run decrypt - Open the .env file output and replace the {MY_VALUE} templated values so that API Server can run in your local environment. - Ensure postgres, nsq, redis, and elastic search are all running on your machine. - Start AuthZ Server at least, and probably Transponder and Message Processor as well, depending on your needs. npm start","title":"Local Development Configuration"},{"location":"services/api-server/#test-configuration","text":"Ensure elasticsearch and postgres services are running locally npm run test","title":"Test Configuration"},{"location":"services/api-server/#deployment-configuration","text":"Helm chart contains all the default values to be used. See this repo's helm directory for more info.","title":"Deployment Configuration"},{"location":"services/api-server/#required","text":"Name Description MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for models MODEL_FIREBASE_DATABASE_URL url to firebase database for models TEMPLATE_DIR path to the directory templates are stored LOCAL_AUTH_SECRET local secret for connecting to the Auth server or service AUTH0_DOMAIN domain name for Auth0 authentication AUTH0_CLIENT_ID client id for Auth0 AUTH0_CLIENT_SECRET client secret for Auth0 AUTH0_CLIENT_SECRET_B64 Base 64 encoded key for Auth0 DATA_FIREBASE_CONFIG firebase database config DATA_FIREBASE_SERVICE_ACCOUNT firebase database service account DATA_BIGQUERY_SERVICE_ACCOUNT BigQuery database service account DATA_BIGQUERY_PROJECT BigQuery database project SQL_USER sql user SQL_PASSWORD sql password SQL_HOST sql host SQL_PORT sql port SQL_CA sql ssl ca SQL_CLIENT_CERT sql ssl cert SQL_CLIENT_KEY sql ssl key PG_USER postgreSQL user PG_PASSWORD postgreSQL password PG_DATABASE postgreSQL database SIMULATOR_URL url to where simulator is hosted MODEL_SQL_USER user of model sql db MODEL_SQL_PASSWORD password of model sql db NPMRC_VALUE contents will be made into .npmrc file","title":"Required"},{"location":"services/api-server/#optional","text":"Name Description Default APP_NAME name of app for firebase config 'api-server' SQL_CA sql ssl ca undefined SQL_CLIENT_CERT sql ssl cert undefined SQL_CLIENT_KEY sql ssl key undefined TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, if TRANSPORT_CONFIG is not specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, if TRANSPORT_CONFIG is not specified 4150 SERVER_PORT server port that api will listen on 8181 LOCAL_AUTH_ISSUER auth token issuer 'https://imagine.leverege.com' LOCAL_AUTH_AUDIENCE auth token audience '' LOCAL_AUTH_EXPIRATION expiration time of auth token '1d' INITIAL_USERS default user accounts for imagine null CACHE_CONFIG configuration for the cache db { type, connection : { host, port } } LOCK_CONFIG configuration for the lock db { type, connection : { host, port } } BQ_HISTORY_TAGS tags that are applied to the BigQuery database '[ 'long' ]' SQL_POOL_CONNECTION_LIMIT limit of the number of sql connections 10 MYSQL_HISTORY_TAGS tags that are applied to the MySql database '[ 'short' ]' PG_POOL_CONNECTION_LIMIT limit of the number of postgreSQL connections 10 PG_PORT postgreSQL port 5432 PG_HOST postgreSQL host 'localhost' PG_HISTORY_TAGS tags that are applied to the PostgreSQL database '[ 'tdb' ]' MODEL_SQL_HOST model sql db host 'localhost' MODEL_SQL_PORT model sql db port 3306 API_SERVICE_TOPIC topic that is assigned to the apiWorker message processor 'api-service-topic' GCF_FIREBASE_SERVICE_ACCOUNT service account for google cloud functions DATA_FIREBASE_SERVICE_ACCOUNT SCRIPT_BUILD_DIR directory where scripts will be deployed to '/tmp' REASONER_LIB_VERSION deployed reasoner version '1.0.7' SCHEDULER_URL url where scheduler is hosted 'http://localhost:9937'","title":"Optional"},{"location":"services/authz-server/","text":"Authz Server \u00b6 This server acts as an authorization server. The JWT Token is used to identify the user making the request. This is currently driven by the Config.js file, as shown below: jwtOptions : { issuers : { <JWT Token iss field> : { secret : <shared secret with issuer>, domain : <simple domain> }, imagine : { secret : 'sheepie-sheepie-in-the-sky', domain : 'imagine' } } } The JWT token must be signed using the secret supplied in the config file. Between the end user and the resources sits a Resource Service (RS). The RS is responsible for determining who and what can be set by their users/services. The RS is uses the JWT signed with the shared secret to communicate these changes to the Authz Service. The RS, normally using the authz-api, will check that a user is allowed to perform some permission on its resources, and the perform it if the user is allowed. app.get( '/project/:projectId/script/:scriptId', can( Permissions.scriptRead ), getAndReturnScript ) To give a user permission permission, the RS should also supply a way to allow users to be assigned roles within the Authz, and potentially create custom roles, permissions and modules. Role \u00b6 Method Path Description Body/Query POST /v1/role Create a new role CreateRole GET /v1/role/ List multiple roles with options QueryRole GET /v1/role/:roleId Get role with the given Id - PATCH /v1/role/:roleId Update an existing role UpdateRole DELETE /v1/role Delete multiple roles DeleteRoles DELETE /v1/role/:roleId Delete the role with the given Id - POST /v1/role/:roleId/permissions Add/Set Permissions on a Role AddPermissions DELETE /v1/role/:roleId/permissions Remove Permissions from a Role RemovePermissions DELETE /v1/role/:roleId/permissions/:permissionId Remove an Permission from a Role - CreateRole \u00b6 Field type Required Description name string Yes The name of the role. Roles created for projectId string Yes The project Id displayName string No The human readable name. Default to name if null category string No CategoriZation field to be used for business logic. Blueprint roles will set this field to the blueprint id { name : 'string', // name of the role projectId : 'string', // project Id displayName : 'string? | null', // optional - displayname category : 'string?', // optional - category that be used to identify role types description : 'string?' // optional - description of role } QueryRole \u00b6 Field type Required Description { projectId : oneOrMany(), // A string or array of strings, used to filter by projectId id : oneOrMany( true ), // A string/number or array of strings/numbers, used to filter by roleId name : oneOrMany(), // A string or array of strings, category : oneOrMany(), // A string or array of strings. displayName : oneOrMany(), includePermissions : 'string?', order : 'array?', // and array of arrays/strings : eg [ [ 'category' : 'asc' ], [ 'name' : 'desc' ] ] limit : 'number?', // max number of results per page offset : 'number?' // the index of the first result } UpdateRole \u00b6 DeleteRoles \u00b6 The following is out dated \u00b6 Roles \u00b6 Roles represent a group of permissions that can be performed. Roles can be created for a particular project id. Create \u00b6 To create a role POST /v1/project/:projectId/role Body \u00b6 Field Type Description name string The name of the role category [string] The optional category of the role, usefuly for presenting only certain roles to an end user UI description [string] A user defined description of the role Output \u00b6 The result will be a Role object Field Type Description id int The id of the role domain string The domain of the role projectId string The id of the role name string The name of the role category string The optional category of the role, usefuly for presenting only certain roles to an end user UI description string A user defined description of the role createdAt string A date string representing the creation time updatedAt string A date string representing the last update time Errors \u00b6 Status Description 403 The user is not allowed to perform the create 400 The body params are invalid List \u00b6 To list the Roles in a project GET /v1/project/:projectId/role Query Params \u00b6 The query parameters that can be used to filter the results are: Query Param Value Description name [string] The role returned has this name category [string] The roles returned have this category limit [int] The number of results per page offset [int] The starting index of results includePermissions [true or false] If true, the permissions in each role are returned with the Role Output \u00b6 The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of roles Errors \u00b6 Status Description 403 The user is not allowed to perform the list 400 The query params are invalid List Roles from several different Projects \u00b6 To get Roles in several projects at once GET /v1/role Query Params \u00b6 The query parameters that can be used to filter the results are: Query Param Value Description projectIds string a comma separated list of projectIds name [string] The role returned has this name category [string] The roles returned have this category limit [int] The number of results per page offset [int] The starting index of results includePermissions [true or false] If true, the permissions in each role are returned with the Role Output \u00b6 The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of roles Errors \u00b6 Status Description 403 The user is not allowed to perform the get 404 The role doesnt exist (in the project) Get one Role \u00b6 To get a Role in a project GET /v1/project/:projectId/role/:roleId Output \u00b6 The result will be a Role object Errors \u00b6 Status Description 403 The user is not allowed to perform the get 404 The role doesnt exist (in the project) Update a Role \u00b6 To update a role POST /v1/project/:projectId/role/:roleId Body \u00b6 Field Type Description name [string] The name of the role category [string] The optional category of the role, usefuly for presenting only certain roles to an end user UI description [string] A user defined description of the role Output \u00b6 The result will be a Role object Errors \u00b6 Status Description 403 The user is not allowed to perform the update 400 The body params are invalid Delete Role \u00b6 To delete a Role in a project DELETE /v1/project/:projectId/role/:roleId Output \u00b6 The result will be a { status : 200, roleId : <id>, deleted : true } Errors \u00b6 Status Description 403 The user is not allowed to perform the delete 404 The role doesnt exist (in the project) Permissions \u00b6 Permissions represent something that can be done to a resource. Some common ones might include the standard CRUD permissions - create, read, update, delete - but they are not restricted to that. Custom permissions can be created as needed. An Permission has both a module name and a name. Create \u00b6 To create an permission POST /v1/project/:projectId/permission Body \u00b6 Field Type Description module string The module name of the permission name string The name of the permission category [string] The optional category of the permission, usefuly for presenting only certain permissions to an end user UI displayName [string] A user readable name for the permission description [string] A user defined description of the permission Output \u00b6 The result will be a Role object Field Type Description id int The id of the permission domain string The domain of the permission projectId string The id of the permission module string The name of the module name string The name of the permission category string The optional category of the permission, usefuly for presenting only certain permissions to an end user UI displayName string A user readable name for the permission description string A user defined description of the permission createdAt string A date string representing the creation time updatedAt string A date string representing the last update time Errors \u00b6 Status Description 403 The user is not allowed to perform the create 400 The body params are invalid List \u00b6 To list the Permissions in a project GET /v1/project/:projectId/permission Query Params \u00b6 The query parameters that can be used to filter the results are: Query Param Value Description module [string] The permission returned has this module name [string] The permission returned has this name category [string] The permissions returned have this category limit [int] The number of results per page offset [int] The starting index of results Output \u00b6 The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of permissionns Errors \u00b6 Status Description 403 The user is not allowed to perform the list 400 The query params are invalid Get one Permission \u00b6 To get a Permission in a project GET /v1/project/:projectId/permission/:permissionId Output \u00b6 The result will be a Permission object Errors \u00b6 Status Description 403 The user is not allowed to perform the get 404 The permission doesnt exist (in the project) Update an Permission \u00b6 To update an permission POST /v1/project/:projectId/permission/:permissionId Body \u00b6 Field Type Description module [string] The module of the permission name [string] The name of the permission category [string] The optional category of the permission, usefuly for presenting only certain permissions to an end user UI displayName [string] The human readable name of the permission description [string] A user defined description of the permission Output \u00b6 The result will be a Permission object Errors \u00b6 Status Description 403 The user is not allowed to perform the update 400 The body params are invalid Delete Permission \u00b6 To delete an Permission from a project DELETE /v1/project/:projectId/permission/:permissionId Output \u00b6 The result will be a { status : 200, permissionId : <id>, deleted : true } Errors \u00b6 Status Description 403 The user is not allowed to perform the delete 404 The permission doesnt exist (in the project) Modules \u00b6 Create \u00b6 To create an module POST /v1/project/:projectId/module Body \u00b6 Field Type Description name string The name of the module displayName [string] A user readable name for the module description [string] A user defined description of the module Output \u00b6 The result will be a Role object Field Type Description domain string The domain of the module projectId string The id of the module name string The name of the module displayName string A user readable name for the module description string A user defined description of the module createdAt string A date string representing the creation time updatedAt string A date string representing the last update time Errors \u00b6 Status Description 403 The user is not allowed to perform the create 400 The body params are invalid List \u00b6 To list the Modules in a project GET /v1/project/:projectId/module Query Params \u00b6 The query parameters that can be used to filter the results are: Query Param Value Description limit [int] The number of results per page offset [int] The starting index of results Output \u00b6 The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of Modules Errors \u00b6 Status Description 403 The user is not allowed to perform the list 400 The query params are invalid Get one Module \u00b6 To get a Module in a project GET /v1/project/:projectId/module/:moduleName Output \u00b6 The result will be a Module object Errors \u00b6 Status Description 403 The user is not allowed to perform the get 404 The permission doesnt exist (in the project) Update an Module \u00b6 To update an Module POST /v1/project/:projectId/module/:moduleName Body \u00b6 Field Type Description displayName [string] The human readable name of the permission description [string] A user defined description of the permission Output \u00b6 The result will be a Module object Errors \u00b6 Status Description 403 The user is not allowed to perform the update 400 The body params are invalid Delete Module \u00b6 To delete an Module from a project DELETE /v1/project/:projectId/module/:moduleName Output \u00b6 The result will be a { status : 200, name : 'moduleName', deleted : true } Errors \u00b6 Status Description 403 The user is not allowed to perform the delete 404 The permission doesnt exist (in the project) UserRole \u00b6 This defines the relationship between the User and Roles (which define what permissions can be performed). Create \u00b6 To create an UserRole POST /v1/userRole Body \u00b6 Field Type Description projectId string The id of the project userId string The id of the user role string, number, or object The id of the role or an object containing the role in the form of resourceId string The id of the resource resourceType [string] The type of the resource, used for user lookup Output \u00b6 The result will be a UserRole object Field Type Description projectId string The id of the project userId string The id of the user roleId number The role id resourceId string The resource Id. resourceType string The resource Type createdAt string A date string representing the creation time Errors \u00b6 Status Description 403 The user is not allowed to perform the create 400 The body params are invalid Get \u00b6 To get UserRoles GET /v1/userRole Query Params \u00b6 Query Param Type Description projectId string The id of the project userId string The id of the user roleId string or number The id of the role resourceId string The id of the resource of interest resourceType string The type of the resource of interest order string limit number The number per page offset number The starting index into the list to return format string Use 'userIds' to return only unique ids. Use 'includeRoles' to include the role object Output \u00b6 The result will be a UserRole object Field Type Description count number The number of results offset number The starting index of the results limit number The number of results per request items array The array of ids or user roles Errors \u00b6 Status Description 403 The user is not allowed to perform the create 400 The body params are invalid Delete \u00b6 To delete UserRoles DELETE /v1/userRole Body \u00b6 Body Type Description projectId string The id of the project userId string The id of the user roleId string or number The id of the role role Role The role represented by resourceId string The id of the resource of interest resourceType string The type of the resource of interest confirm string If the delete does not supply a userId, roleId and resourceId, this must be set to 'bulkDelete'. Output \u00b6 The result will be a UserRole object Field Type Description status number 200 the fields given to the delete Errors \u00b6 Status Description 403 The user is not allowed to perform the create 400 The body params are invalid Can \u00b6 POST v1/can Simple example. Sending this body will return true if user can perform permission on resource-id: { user : 'fred', permission : { projectId, module : 'Script', name : 'create' }, resourceId : 'resource-id' } Array example. Sending this body will return true if user can perform permission 1 on resource-id OR permission 2 on resource-id or parent-id: { user : 'fred', permissions : [ { permission : { projectId, module : 'Script', name : 'create' }, resource : 'resource-id' }, { permission : { projectId, module : 'Script', name : 'delete' }, resource : ['resource-id', 'parent-id'] } ] } Condition example. This will return true if user can perform permission 1 on resource-id AND permission 2 on resource-id or parent-d { user : 'fred', condition : { type : 'and', conditions : [ { permission : { projectId, module : 'Script', name : 'create' }, resource : 'resource-id' }, { permission : { projectId, module : 'Script', name : 'delete' }, resource : ['resource-id', 'parent-id'] } ] } }","title":"Authz Server"},{"location":"services/authz-server/#authz-server","text":"This server acts as an authorization server. The JWT Token is used to identify the user making the request. This is currently driven by the Config.js file, as shown below: jwtOptions : { issuers : { <JWT Token iss field> : { secret : <shared secret with issuer>, domain : <simple domain> }, imagine : { secret : 'sheepie-sheepie-in-the-sky', domain : 'imagine' } } } The JWT token must be signed using the secret supplied in the config file. Between the end user and the resources sits a Resource Service (RS). The RS is responsible for determining who and what can be set by their users/services. The RS is uses the JWT signed with the shared secret to communicate these changes to the Authz Service. The RS, normally using the authz-api, will check that a user is allowed to perform some permission on its resources, and the perform it if the user is allowed. app.get( '/project/:projectId/script/:scriptId', can( Permissions.scriptRead ), getAndReturnScript ) To give a user permission permission, the RS should also supply a way to allow users to be assigned roles within the Authz, and potentially create custom roles, permissions and modules.","title":"Authz Server"},{"location":"services/authz-server/#role","text":"Method Path Description Body/Query POST /v1/role Create a new role CreateRole GET /v1/role/ List multiple roles with options QueryRole GET /v1/role/:roleId Get role with the given Id - PATCH /v1/role/:roleId Update an existing role UpdateRole DELETE /v1/role Delete multiple roles DeleteRoles DELETE /v1/role/:roleId Delete the role with the given Id - POST /v1/role/:roleId/permissions Add/Set Permissions on a Role AddPermissions DELETE /v1/role/:roleId/permissions Remove Permissions from a Role RemovePermissions DELETE /v1/role/:roleId/permissions/:permissionId Remove an Permission from a Role -","title":"Role"},{"location":"services/authz-server/#createrole","text":"Field type Required Description name string Yes The name of the role. Roles created for projectId string Yes The project Id displayName string No The human readable name. Default to name if null category string No CategoriZation field to be used for business logic. Blueprint roles will set this field to the blueprint id { name : 'string', // name of the role projectId : 'string', // project Id displayName : 'string? | null', // optional - displayname category : 'string?', // optional - category that be used to identify role types description : 'string?' // optional - description of role }","title":"CreateRole"},{"location":"services/authz-server/#queryrole","text":"Field type Required Description { projectId : oneOrMany(), // A string or array of strings, used to filter by projectId id : oneOrMany( true ), // A string/number or array of strings/numbers, used to filter by roleId name : oneOrMany(), // A string or array of strings, category : oneOrMany(), // A string or array of strings. displayName : oneOrMany(), includePermissions : 'string?', order : 'array?', // and array of arrays/strings : eg [ [ 'category' : 'asc' ], [ 'name' : 'desc' ] ] limit : 'number?', // max number of results per page offset : 'number?' // the index of the first result }","title":"QueryRole"},{"location":"services/authz-server/#updaterole","text":"","title":"UpdateRole"},{"location":"services/authz-server/#deleteroles","text":"","title":"DeleteRoles"},{"location":"services/authz-server/#the-following-is-out-dated","text":"","title":"The following is out dated"},{"location":"services/authz-server/#roles","text":"Roles represent a group of permissions that can be performed. Roles can be created for a particular project id.","title":"Roles"},{"location":"services/authz-server/#create","text":"To create a role POST /v1/project/:projectId/role","title":"Create"},{"location":"services/authz-server/#body","text":"Field Type Description name string The name of the role category [string] The optional category of the role, usefuly for presenting only certain roles to an end user UI description [string] A user defined description of the role","title":"Body"},{"location":"services/authz-server/#output","text":"The result will be a Role object Field Type Description id int The id of the role domain string The domain of the role projectId string The id of the role name string The name of the role category string The optional category of the role, usefuly for presenting only certain roles to an end user UI description string A user defined description of the role createdAt string A date string representing the creation time updatedAt string A date string representing the last update time","title":"Output"},{"location":"services/authz-server/#errors","text":"Status Description 403 The user is not allowed to perform the create 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#list","text":"To list the Roles in a project GET /v1/project/:projectId/role","title":"List"},{"location":"services/authz-server/#query-params","text":"The query parameters that can be used to filter the results are: Query Param Value Description name [string] The role returned has this name category [string] The roles returned have this category limit [int] The number of results per page offset [int] The starting index of results includePermissions [true or false] If true, the permissions in each role are returned with the Role","title":"Query Params"},{"location":"services/authz-server/#output_1","text":"The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of roles","title":"Output"},{"location":"services/authz-server/#errors_1","text":"Status Description 403 The user is not allowed to perform the list 400 The query params are invalid","title":"Errors"},{"location":"services/authz-server/#list-roles-from-several-different-projects","text":"To get Roles in several projects at once GET /v1/role","title":"List Roles from several different Projects"},{"location":"services/authz-server/#query-params_1","text":"The query parameters that can be used to filter the results are: Query Param Value Description projectIds string a comma separated list of projectIds name [string] The role returned has this name category [string] The roles returned have this category limit [int] The number of results per page offset [int] The starting index of results includePermissions [true or false] If true, the permissions in each role are returned with the Role","title":"Query Params"},{"location":"services/authz-server/#output_2","text":"The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of roles","title":"Output"},{"location":"services/authz-server/#errors_2","text":"Status Description 403 The user is not allowed to perform the get 404 The role doesnt exist (in the project)","title":"Errors"},{"location":"services/authz-server/#get-one-role","text":"To get a Role in a project GET /v1/project/:projectId/role/:roleId","title":"Get one Role"},{"location":"services/authz-server/#output_3","text":"The result will be a Role object","title":"Output"},{"location":"services/authz-server/#errors_3","text":"Status Description 403 The user is not allowed to perform the get 404 The role doesnt exist (in the project)","title":"Errors"},{"location":"services/authz-server/#update-a-role","text":"To update a role POST /v1/project/:projectId/role/:roleId","title":"Update a Role"},{"location":"services/authz-server/#body_1","text":"Field Type Description name [string] The name of the role category [string] The optional category of the role, usefuly for presenting only certain roles to an end user UI description [string] A user defined description of the role","title":"Body"},{"location":"services/authz-server/#output_4","text":"The result will be a Role object","title":"Output"},{"location":"services/authz-server/#errors_4","text":"Status Description 403 The user is not allowed to perform the update 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#delete-role","text":"To delete a Role in a project DELETE /v1/project/:projectId/role/:roleId","title":"Delete Role"},{"location":"services/authz-server/#output_5","text":"The result will be a { status : 200, roleId : <id>, deleted : true }","title":"Output"},{"location":"services/authz-server/#errors_5","text":"Status Description 403 The user is not allowed to perform the delete 404 The role doesnt exist (in the project)","title":"Errors"},{"location":"services/authz-server/#permissions","text":"Permissions represent something that can be done to a resource. Some common ones might include the standard CRUD permissions - create, read, update, delete - but they are not restricted to that. Custom permissions can be created as needed. An Permission has both a module name and a name.","title":"Permissions"},{"location":"services/authz-server/#create_1","text":"To create an permission POST /v1/project/:projectId/permission","title":"Create"},{"location":"services/authz-server/#body_2","text":"Field Type Description module string The module name of the permission name string The name of the permission category [string] The optional category of the permission, usefuly for presenting only certain permissions to an end user UI displayName [string] A user readable name for the permission description [string] A user defined description of the permission","title":"Body"},{"location":"services/authz-server/#output_6","text":"The result will be a Role object Field Type Description id int The id of the permission domain string The domain of the permission projectId string The id of the permission module string The name of the module name string The name of the permission category string The optional category of the permission, usefuly for presenting only certain permissions to an end user UI displayName string A user readable name for the permission description string A user defined description of the permission createdAt string A date string representing the creation time updatedAt string A date string representing the last update time","title":"Output"},{"location":"services/authz-server/#errors_6","text":"Status Description 403 The user is not allowed to perform the create 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#list_1","text":"To list the Permissions in a project GET /v1/project/:projectId/permission","title":"List"},{"location":"services/authz-server/#query-params_2","text":"The query parameters that can be used to filter the results are: Query Param Value Description module [string] The permission returned has this module name [string] The permission returned has this name category [string] The permissions returned have this category limit [int] The number of results per page offset [int] The starting index of results","title":"Query Params"},{"location":"services/authz-server/#output_7","text":"The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of permissionns","title":"Output"},{"location":"services/authz-server/#errors_7","text":"Status Description 403 The user is not allowed to perform the list 400 The query params are invalid","title":"Errors"},{"location":"services/authz-server/#get-one-permission","text":"To get a Permission in a project GET /v1/project/:projectId/permission/:permissionId","title":"Get one Permission"},{"location":"services/authz-server/#output_8","text":"The result will be a Permission object","title":"Output"},{"location":"services/authz-server/#errors_8","text":"Status Description 403 The user is not allowed to perform the get 404 The permission doesnt exist (in the project)","title":"Errors"},{"location":"services/authz-server/#update-an-permission","text":"To update an permission POST /v1/project/:projectId/permission/:permissionId","title":"Update an Permission"},{"location":"services/authz-server/#body_3","text":"Field Type Description module [string] The module of the permission name [string] The name of the permission category [string] The optional category of the permission, usefuly for presenting only certain permissions to an end user UI displayName [string] The human readable name of the permission description [string] A user defined description of the permission","title":"Body"},{"location":"services/authz-server/#output_9","text":"The result will be a Permission object","title":"Output"},{"location":"services/authz-server/#errors_9","text":"Status Description 403 The user is not allowed to perform the update 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#delete-permission","text":"To delete an Permission from a project DELETE /v1/project/:projectId/permission/:permissionId","title":"Delete Permission"},{"location":"services/authz-server/#output_10","text":"The result will be a { status : 200, permissionId : <id>, deleted : true }","title":"Output"},{"location":"services/authz-server/#errors_10","text":"Status Description 403 The user is not allowed to perform the delete 404 The permission doesnt exist (in the project)","title":"Errors"},{"location":"services/authz-server/#modules","text":"","title":"Modules"},{"location":"services/authz-server/#create_2","text":"To create an module POST /v1/project/:projectId/module","title":"Create"},{"location":"services/authz-server/#body_4","text":"Field Type Description name string The name of the module displayName [string] A user readable name for the module description [string] A user defined description of the module","title":"Body"},{"location":"services/authz-server/#output_11","text":"The result will be a Role object Field Type Description domain string The domain of the module projectId string The id of the module name string The name of the module displayName string A user readable name for the module description string A user defined description of the module createdAt string A date string representing the creation time updatedAt string A date string representing the last update time","title":"Output"},{"location":"services/authz-server/#errors_11","text":"Status Description 403 The user is not allowed to perform the create 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#list_2","text":"To list the Modules in a project GET /v1/project/:projectId/module","title":"List"},{"location":"services/authz-server/#query-params_3","text":"The query parameters that can be used to filter the results are: Query Param Value Description limit [int] The number of results per page offset [int] The starting index of results","title":"Query Params"},{"location":"services/authz-server/#output_12","text":"The result will be an object Field Type Description count int The number of results offset int The starting index of the results limit int The number of results per page items array The array of Modules","title":"Output"},{"location":"services/authz-server/#errors_12","text":"Status Description 403 The user is not allowed to perform the list 400 The query params are invalid","title":"Errors"},{"location":"services/authz-server/#get-one-module","text":"To get a Module in a project GET /v1/project/:projectId/module/:moduleName","title":"Get one Module"},{"location":"services/authz-server/#output_13","text":"The result will be a Module object","title":"Output"},{"location":"services/authz-server/#errors_13","text":"Status Description 403 The user is not allowed to perform the get 404 The permission doesnt exist (in the project)","title":"Errors"},{"location":"services/authz-server/#update-an-module","text":"To update an Module POST /v1/project/:projectId/module/:moduleName","title":"Update an Module"},{"location":"services/authz-server/#body_5","text":"Field Type Description displayName [string] The human readable name of the permission description [string] A user defined description of the permission","title":"Body"},{"location":"services/authz-server/#output_14","text":"The result will be a Module object","title":"Output"},{"location":"services/authz-server/#errors_14","text":"Status Description 403 The user is not allowed to perform the update 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#delete-module","text":"To delete an Module from a project DELETE /v1/project/:projectId/module/:moduleName","title":"Delete Module"},{"location":"services/authz-server/#output_15","text":"The result will be a { status : 200, name : 'moduleName', deleted : true }","title":"Output"},{"location":"services/authz-server/#errors_15","text":"Status Description 403 The user is not allowed to perform the delete 404 The permission doesnt exist (in the project)","title":"Errors"},{"location":"services/authz-server/#userrole","text":"This defines the relationship between the User and Roles (which define what permissions can be performed).","title":"UserRole"},{"location":"services/authz-server/#create_3","text":"To create an UserRole POST /v1/userRole","title":"Create"},{"location":"services/authz-server/#body_6","text":"Field Type Description projectId string The id of the project userId string The id of the user role string, number, or object The id of the role or an object containing the role in the form of resourceId string The id of the resource resourceType [string] The type of the resource, used for user lookup","title":"Body"},{"location":"services/authz-server/#output_16","text":"The result will be a UserRole object Field Type Description projectId string The id of the project userId string The id of the user roleId number The role id resourceId string The resource Id. resourceType string The resource Type createdAt string A date string representing the creation time","title":"Output"},{"location":"services/authz-server/#errors_16","text":"Status Description 403 The user is not allowed to perform the create 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#get","text":"To get UserRoles GET /v1/userRole","title":"Get"},{"location":"services/authz-server/#query-params_4","text":"Query Param Type Description projectId string The id of the project userId string The id of the user roleId string or number The id of the role resourceId string The id of the resource of interest resourceType string The type of the resource of interest order string limit number The number per page offset number The starting index into the list to return format string Use 'userIds' to return only unique ids. Use 'includeRoles' to include the role object","title":"Query Params"},{"location":"services/authz-server/#output_17","text":"The result will be a UserRole object Field Type Description count number The number of results offset number The starting index of the results limit number The number of results per request items array The array of ids or user roles","title":"Output"},{"location":"services/authz-server/#errors_17","text":"Status Description 403 The user is not allowed to perform the create 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#delete","text":"To delete UserRoles DELETE /v1/userRole","title":"Delete"},{"location":"services/authz-server/#body_7","text":"Body Type Description projectId string The id of the project userId string The id of the user roleId string or number The id of the role role Role The role represented by resourceId string The id of the resource of interest resourceType string The type of the resource of interest confirm string If the delete does not supply a userId, roleId and resourceId, this must be set to 'bulkDelete'.","title":"Body"},{"location":"services/authz-server/#output_18","text":"The result will be a UserRole object Field Type Description status number 200 the fields given to the delete","title":"Output"},{"location":"services/authz-server/#errors_18","text":"Status Description 403 The user is not allowed to perform the create 400 The body params are invalid","title":"Errors"},{"location":"services/authz-server/#can","text":"POST v1/can Simple example. Sending this body will return true if user can perform permission on resource-id: { user : 'fred', permission : { projectId, module : 'Script', name : 'create' }, resourceId : 'resource-id' } Array example. Sending this body will return true if user can perform permission 1 on resource-id OR permission 2 on resource-id or parent-id: { user : 'fred', permissions : [ { permission : { projectId, module : 'Script', name : 'create' }, resource : 'resource-id' }, { permission : { projectId, module : 'Script', name : 'delete' }, resource : ['resource-id', 'parent-id'] } ] } Condition example. This will return true if user can perform permission 1 on resource-id AND permission 2 on resource-id or parent-d { user : 'fred', condition : { type : 'and', conditions : [ { permission : { projectId, module : 'Script', name : 'create' }, resource : 'resource-id' }, { permission : { projectId, module : 'Script', name : 'delete' }, resource : ['resource-id', 'parent-id'] } ] } }","title":"Can"},{"location":"services/db-curator/","text":"DB-Curator is the service that is responsible for deleting old or stale data from databases after a specified period of time. Structure \u00b6 DB-Curator is structured as a set of long-running loops that run once per day at a specified time. The processes that clean up individual databases are independent, and run in parallel with each other. Configuration \u00b6 DB-Curator is overall configured with a small set of environment variables that determine which cleanup scripts run. By making the value of these variables false, one can turn off selected cleanup scripts. ENV Variable Description SQL_CLEANUP_ENABLED whether or not the MySQL cleanup script is enabled FB_CLEANUP_ENABLED whether or not the firebase cleanup script is enabled PG_CLEANUP_ENABLED whether or not the PostgreSQL cleanup script is enabled Additionally, certain env variables are shared between clean up scripts. The most important of these is 'SQL_KEEP_PERIOD' which tells both the MySql and Postgres scripts how long to go back before deleting data. Firebase Configuration \u00b6 ENV Variable Description FIREBASE_CLEANUP_CONFIG a JSON string specifying what paths in firebase to clean up, and how long to keep data An example of the Firebase cleanup config is: [ { \"listPath\" : \"rt/*/dev/*/geolog\" , \"keepTime\" : 604800000 , \"itemTimePath\" : \"time\" }, { \"listPath\" : \"rt/*/dev/*/events/*\" , \"keepTime\" : 604800000 , \"itemTimePath\" : \"time\" } ] The config specifies locations (as an array) in firebase of time ordered lists to be cleaned up using the following three keys: Key Description listPath path in firebase (including *'s) to the list or set of lists to be cleaned up keepTime length of time to keep data before it is deleted (ms) itemTimePath since each list or set of lists should be comprised of similar objects, this specifies the time key of each item that can be used to determine the age of the object Postgres Configuration \u00b6 ENV Variable Description PG_POOL_CONNECTION_LIMIT max number of connections to make to postgres PG_HOST host the database is located at PG_PORT port the database is located at PG_USER postgres user name PG_PASSWORD postgres password PG_DATABASE database to connect to MySQL Configuration \u00b6 ENV Variable Description SQL_CONNECTION_LIMIT maximum connections to make to the sql server SQL_HOST host the db is located at SQL_PORT port the db is located at SQL_USER username to login with SQL_PASSWORD password to login with SQL_CA server's CA if it has ssl enabled SQL_CLIENT_CERT client's cert if the server has ssl enabled SQL_CLIENT_KEY client's key if the server has ssl enabled","title":"DB-Curator"},{"location":"services/db-curator/#structure","text":"DB-Curator is structured as a set of long-running loops that run once per day at a specified time. The processes that clean up individual databases are independent, and run in parallel with each other.","title":"Structure"},{"location":"services/db-curator/#configuration","text":"DB-Curator is overall configured with a small set of environment variables that determine which cleanup scripts run. By making the value of these variables false, one can turn off selected cleanup scripts. ENV Variable Description SQL_CLEANUP_ENABLED whether or not the MySQL cleanup script is enabled FB_CLEANUP_ENABLED whether or not the firebase cleanup script is enabled PG_CLEANUP_ENABLED whether or not the PostgreSQL cleanup script is enabled Additionally, certain env variables are shared between clean up scripts. The most important of these is 'SQL_KEEP_PERIOD' which tells both the MySql and Postgres scripts how long to go back before deleting data.","title":"Configuration"},{"location":"services/db-curator/#firebase-configuration","text":"ENV Variable Description FIREBASE_CLEANUP_CONFIG a JSON string specifying what paths in firebase to clean up, and how long to keep data An example of the Firebase cleanup config is: [ { \"listPath\" : \"rt/*/dev/*/geolog\" , \"keepTime\" : 604800000 , \"itemTimePath\" : \"time\" }, { \"listPath\" : \"rt/*/dev/*/events/*\" , \"keepTime\" : 604800000 , \"itemTimePath\" : \"time\" } ] The config specifies locations (as an array) in firebase of time ordered lists to be cleaned up using the following three keys: Key Description listPath path in firebase (including *'s) to the list or set of lists to be cleaned up keepTime length of time to keep data before it is deleted (ms) itemTimePath since each list or set of lists should be comprised of similar objects, this specifies the time key of each item that can be used to determine the age of the object","title":"Firebase Configuration"},{"location":"services/db-curator/#postgres-configuration","text":"ENV Variable Description PG_POOL_CONNECTION_LIMIT max number of connections to make to postgres PG_HOST host the database is located at PG_PORT port the database is located at PG_USER postgres user name PG_PASSWORD postgres password PG_DATABASE database to connect to","title":"Postgres Configuration"},{"location":"services/db-curator/#mysql-configuration","text":"ENV Variable Description SQL_CONNECTION_LIMIT maximum connections to make to the sql server SQL_HOST host the db is located at SQL_PORT port the db is located at SQL_USER username to login with SQL_PASSWORD password to login with SQL_CA server's CA if it has ssl enabled SQL_CLIENT_CERT client's cert if the server has ssl enabled SQL_CLIENT_KEY client's key if the server has ssl enabled","title":"MySQL Configuration"},{"location":"services/emailer/","text":"Emailer's function is to receive email requests in the form of an EmailMsg , queue them, and then forward them to a service that will send the email. Currently Emailer is configured to use the MailGun api , but Emailer can be configured for other services' apis such as SendGrid. Emailer listens to the messaging service set up with the Leverege Platform. The default message reading configuration is NSQ, and the topic that emailer listens to is email . The format of the message is an EmailMsg , which can be found in the Messages library, and an example is given below. A message starts by invoking the route method on an instance of a writer created by Message Queue with the email topic and an instance of an EmailMsg . Message Queue writes the message to the Transponder service, which then routes it to this service (Emailer). Emailer does a validation check to make sure the message received is an instance of an EmailMsg . If there is a templateId provided, Emailer fetches the HTML template, and Handlebars is used to insert values from the context into the template. The Message is then sent to Mailgun for the email to be sent to the client. Configuration \u00b6 Required \u00b6 There are 4 config variable that are required to run this service. Name Description MAIL_GUN_API_KEY Mailgun Api Key MAIL_GUN_DOMAIN Mailgun domain to send emails from MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database MODEL_FIREBASE_DATABASE_URL url to firebase database Optional \u00b6 There are 11 config variables that have defaults set that will need to be specified in the environment if the default is incorrect. Name Description Default EMAIL_TEMPLATES_DIR file path to where email templates are stored null TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150 CACHE_CONFIG configuration for the cache db { type, connection : { host, port } } LIMIT_CONFIG configuration for the limit db Limit MAIL_ATTEMPTS maximum number of delivery attempts before aborting 3 MAIL_ATTEMPT_DELAY delay enforced between email attempts 30000 MAIL_GUN_TEMPLATES string representing the path to the Mailgun templates './templates/email/' MAIL_GUN_DEFAULT_SENDER default sender email address 'team@leverege.com' API_SERVER url of the Leverege api 'https://imagine.leverege.com:8181' EmailMsg \u00b6 A more detailed message spec can be found in the Messages library. Name Type Description to string or array recipients' email addresses cc string or array carbon copy email list bcc string or array blind carbon copy email list subject string subject line text string optional message in plain text format html string optional message in html format template string id of the template to use context object object to give to the template attachments array attachment list attachments[n].filename string name of the attachment that the recipients will see attachments[n].url string source of a file to send as attachment attachments[n].content string base64 encoded content. Don't use this if url is specified attachments[n].encoding string normally \"base64\"","title":"Emailer"},{"location":"services/emailer/#configuration","text":"","title":"Configuration"},{"location":"services/emailer/#required","text":"There are 4 config variable that are required to run this service. Name Description MAIL_GUN_API_KEY Mailgun Api Key MAIL_GUN_DOMAIN Mailgun domain to send emails from MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database MODEL_FIREBASE_DATABASE_URL url to firebase database","title":"Required"},{"location":"services/emailer/#optional","text":"There are 11 config variables that have defaults set that will need to be specified in the environment if the default is incorrect. Name Description Default EMAIL_TEMPLATES_DIR file path to where email templates are stored null TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150 CACHE_CONFIG configuration for the cache db { type, connection : { host, port } } LIMIT_CONFIG configuration for the limit db Limit MAIL_ATTEMPTS maximum number of delivery attempts before aborting 3 MAIL_ATTEMPT_DELAY delay enforced between email attempts 30000 MAIL_GUN_TEMPLATES string representing the path to the Mailgun templates './templates/email/' MAIL_GUN_DEFAULT_SENDER default sender email address 'team@leverege.com' API_SERVER url of the Leverege api 'https://imagine.leverege.com:8181'","title":"Optional"},{"location":"services/emailer/#emailmsg","text":"A more detailed message spec can be found in the Messages library. Name Type Description to string or array recipients' email addresses cc string or array carbon copy email list bcc string or array blind carbon copy email list subject string subject line text string optional message in plain text format html string optional message in html format template string id of the template to use context object object to give to the template attachments array attachment list attachments[n].filename string name of the attachment that the recipients will see attachments[n].url string source of a file to send as attachment attachments[n].content string base64 encoded content. Don't use this if url is specified attachments[n].encoding string normally \"base64\"","title":"EmailMsg"},{"location":"services/geotile-server/","text":"Geotile API \u00b6 Generates geotiles for images. Usage \u00b6 See API Docs TODO: \u00b6 Extend dockreate script to enabled support for python3 and packages. Until then, run these commands by execing into the instance w/k8x: ln -s /usr/bin/python3 /usr/bin/python && pip3 install --global-option=build_ext --global-option=\"-I/usr/include/gdal\" GDAL==`gdal-config --version` Route the api-server w/authz Install (Local) \u00b6 Install Conda: brew cask install conda Setup python3 environment: conda create --name py3 python=3.7 Use python3 environment: conda activate py3 Install GDAL w/Conda: conda install gdal Install node modules: npm install Setup sql database: mysql -u root -p # pwd: root + create database geotile_server; quit; Run tests: npm test Run w/Local (MySQL) Database \u00b6 Uncomment option in .env Start server: npm start Run w/Dev (GCP Postgres) Database \u00b6 Uncomment option in .env Start port forwaring in seperate tab: npm run postgres-port-forward Start server: npm start Troubleshooting \u00b6 SequelizeConnectionRefusedError (connect ECONNREFUSED 127.0.0.1:3306): mysql.server start ImportError (libgdal.20.dylib): python --version && which python","title":"Geotile Server"},{"location":"services/geotile-server/#geotile-api","text":"Generates geotiles for images.","title":"Geotile API"},{"location":"services/geotile-server/#usage","text":"See API Docs","title":"Usage"},{"location":"services/geotile-server/#todo","text":"Extend dockreate script to enabled support for python3 and packages. Until then, run these commands by execing into the instance w/k8x: ln -s /usr/bin/python3 /usr/bin/python && pip3 install --global-option=build_ext --global-option=\"-I/usr/include/gdal\" GDAL==`gdal-config --version` Route the api-server w/authz","title":"TODO:"},{"location":"services/geotile-server/#install-local","text":"Install Conda: brew cask install conda Setup python3 environment: conda create --name py3 python=3.7 Use python3 environment: conda activate py3 Install GDAL w/Conda: conda install gdal Install node modules: npm install Setup sql database: mysql -u root -p # pwd: root + create database geotile_server; quit; Run tests: npm test","title":"Install (Local)"},{"location":"services/geotile-server/#run-wlocal-mysql-database","text":"Uncomment option in .env Start server: npm start","title":"Run w/Local (MySQL) Database"},{"location":"services/geotile-server/#run-wdev-gcp-postgres-database","text":"Uncomment option in .env Start port forwaring in seperate tab: npm run postgres-port-forward Start server: npm start","title":"Run w/Dev (GCP Postgres) Database"},{"location":"services/geotile-server/#troubleshooting","text":"SequelizeConnectionRefusedError (connect ECONNREFUSED 127.0.0.1:3306): mysql.server start ImportError (libgdal.20.dylib): python --version && which python","title":"Troubleshooting"},{"location":"services/message-processor/","text":"Message Processor is a service responsible for receiving, processing, and then forwarding inbound messages along into the Leverege Platform. The first step in processing a message is to check the type of id. If the device has an external network id attached, message-processor will look up the device against that network. The next step in processing the inbound message is to create a message with a type corresponding to the internal version of the message. Then, that message gets device data, system data, project data and blueprint data attached. If the blueprint has a Message Route defined, it will be assigned onto the message. The route that gets attached onto the message is responsible for telling the system which order and services the message will be sent to. A message route is created under the msg routes section of the Leverege UI, and then attached to a blueprint, or a specific device under their respective info tabs. The message route is a JSON structure that defines what the routes that different types of messages will follow. Configuration \u00b6 Required \u00b6 Name Description MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for models MODEL_FIREBASE_DATABASE_URL url to firebase database for models Optional \u00b6 Name Description Default TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150 CACHE_CONFIG configuration for the cache db { type, connection : { host, port } }","title":"Message Processor"},{"location":"services/message-processor/#configuration","text":"","title":"Configuration"},{"location":"services/message-processor/#required","text":"Name Description MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for models MODEL_FIREBASE_DATABASE_URL url to firebase database for models","title":"Required"},{"location":"services/message-processor/#optional","text":"Name Description Default TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150 CACHE_CONFIG configuration for the cache db { type, connection : { host, port } }","title":"Optional"},{"location":"services/messenger/","text":"Messenger's function is to receive SMS requests in the form of an SmsMsg , queue them, and then forward them to a service that will send the SMS. Currently Messenger is configured to use the Twilio api , but Messenger can be configured for other services' apis. Messenger listens to the messaging service set up with the Leverege Platform. The default message reading configuration is NSQ, and the topic that that Messenger listens to by default is messenger , but can be changed through environment configuration. The format of the message is a SmsMsg , which can be found in the Messages library, and an example is given below. A message starts by invoking the route method on an instance of a writer created by Message Queue with the topic Messenger is listening to and an instance of a SmsMsg . Message Queue writes the message to the transponder service, which then routes it to this service (Messenger). Messenger does a validation check to make sure the message received is an instance of an SmsMsg . The message is then sent to Twilio for the SMS to be sent to the client. Configuration \u00b6 Required \u00b6 There are 5 config variable that are required to run this service. Name Description TWILIO_SID id of Twilio account TWILIO_AUTH_TOKEN auth token for Twilio account TWILIO_PHONE phone number messages will be sent from MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database MODEL_FIREBASE_DATABASE_URL url to firebase database Optional \u00b6 Name Description Default SMS_TEMPLATES_DIR file path to where sms templates are stored null TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150 CACHE_CONFIG configuration for the cache db { type, connection : { host, port } } LIMIT_CONFIG configuration for the limit db Limit MESSENGER_TOPIC topic that this service will listen to 'messenger' MESSENGER_SENDER channel that this service will send messages on 'sender' SmsMsg \u00b6 A more detailed message spec can be found in the Messages library Name Type Description recipients string or array phone number/s that will receive the SMS message message string or null message that will be sent limitOptions limitOptions or null limiter options that will be used for the message templateId string or null id of the template that is to be used projectId string or null Leverege Platform projectId context object provides values to be inserted into the template limitOptions \u00b6 More detail about the limitOptions can be found in the Limit library Name Type Description count number maximum number of messages that will be sent in the given period period number time(ms) - period of time that the count will be tracked. if count is reached, stops sending the message to the user for the duration duration number time(ms) - if the count is reached in a period, user will not receive another of the message type for this amount of time key string namespace key for the message","title":"Messenger"},{"location":"services/messenger/#configuration","text":"","title":"Configuration"},{"location":"services/messenger/#required","text":"There are 5 config variable that are required to run this service. Name Description TWILIO_SID id of Twilio account TWILIO_AUTH_TOKEN auth token for Twilio account TWILIO_PHONE phone number messages will be sent from MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database MODEL_FIREBASE_DATABASE_URL url to firebase database","title":"Required"},{"location":"services/messenger/#optional","text":"Name Description Default SMS_TEMPLATES_DIR file path to where sms templates are stored null TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150 CACHE_CONFIG configuration for the cache db { type, connection : { host, port } } LIMIT_CONFIG configuration for the limit db Limit MESSENGER_TOPIC topic that this service will listen to 'messenger' MESSENGER_SENDER channel that this service will send messages on 'sender'","title":"Optional"},{"location":"services/messenger/#smsmsg","text":"A more detailed message spec can be found in the Messages library Name Type Description recipients string or array phone number/s that will receive the SMS message message string or null message that will be sent limitOptions limitOptions or null limiter options that will be used for the message templateId string or null id of the template that is to be used projectId string or null Leverege Platform projectId context object provides values to be inserted into the template","title":"SmsMsg"},{"location":"services/messenger/#limitoptions","text":"More detail about the limitOptions can be found in the Limit library Name Type Description count number maximum number of messages that will be sent in the given period period number time(ms) - period of time that the count will be tracked. if count is reached, stops sending the message to the user for the duration duration number time(ms) - if the count is reached in a period, user will not receive another of the message type for this amount of time key string namespace key for the message","title":"limitOptions"},{"location":"services/reason/","text":"Reason allows for scripts with custom logic to be run on the Leverege Platform. Scripts are written in Javascript, and can be triggered in 3 different ways. Scripts are very powerful, the main uses tend to be Generating reports and triggering alerts. In addition to this, they can be used to augment or change incoming data before it is saved to devices. A script is created in the Leverege UI under the script section. In this section of the UI, scripts can also be edited or deleted in addition to viewing their logs. Triggering \u00b6 The first way to trigger a reason script is through a Message Route. In the Leverege UI, under Msg Routes, create a new message route and in the metadata include { \"[message type]\" : { \"reason\" : { \"options\" : { \"reasoner\" : { \"id\" : \"[Reason Script Id]\" } } }, \"writer\" : true } } then copy the Message Route Id and attach it to the blueprint/s or device/s, in the info section the input labeled Msg Route . Whenever a device or device of that blueprint receives a message of that type, the message will first be sent to reasoner. Here, the reason script can examine, manipulate or run custom functionality before updating the device information. The second way to trigger a reason script is through a timer. Timer's can be set in two ways. The first way is through the Imagine UI, under the timer section. The second way to create a timer is to send a message on the scheduler topic with a SetTimerMsg . A more detailed explanation of timers can be found in scheduler . The Third way to trigger a reason script is by sending a message to the reason topic. Run \u00b6 By default reason scripts start with the function exports.run . This function has two main arguments, a message and a context . A description of the context can be found below. The message is the message that triggered the script. This is most useful when the script is set to trigger on a message route, since the message will have the incoming device data which can then be examined and manipulated. If the function is async, a third done argument needs to be called if there is no return statement. Additionally, when reasoner is triggered via a message route, the return can change of affect the message. When a null is returned, the message will not be changed, and the message will continue on its route as normal. When a false is returned, it will stop the message. If an object is returned, reasoner will forward that message on instead of the message that triggered it. If invoking a script through the reason topic. A different entry function can be specified than the run function. Every function that is intended to be used as an entry point needs to have a needs property attached to it. The needs is a JSON object that specifies all of the data the context needs in order for the function to be executed. If all of the parameters specified in needs cannot be met, the script will not run. Additionally, anything not specified by the needs is not guaranteed to be included in the context when the function is executed. Context \u00b6 Every time a reason script gets triggered, it is invoked with a context. The context changes slightly based on how the script is triggered. There are always several methods that can be invoked through the context (these do not have to be specified in the needs). More details about these methods can be found in the Reasoner library. In addition to this, device and system information can be attached to the context if it is triggered through a message route. If a timer is set to trigger a reason script, params will be attached to the context, which is a JSON object that can be specified in the UI, under the options of the timer. Logging and Debugging \u00b6 Logging can be done in reason scripts. A log and error method are attached to the context. Their output will go into GCP logging. They can be viewed there, additionally, they are scraped by StackDriver, and can be seen in the Leverege UI. Configuration \u00b6 Required \u00b6 Name Description CF_TRANSPORT_CONFIG transport config for the cloud function readers and writers MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for models MODEL_FIREBASE_DATABASE_URL url to firebase database for models DATA_FIREBASE_DATABASE_URL url to the firebase database for data DATA_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for data Optional \u00b6 Name Description Default TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150","title":"Reason"},{"location":"services/reason/#triggering","text":"The first way to trigger a reason script is through a Message Route. In the Leverege UI, under Msg Routes, create a new message route and in the metadata include { \"[message type]\" : { \"reason\" : { \"options\" : { \"reasoner\" : { \"id\" : \"[Reason Script Id]\" } } }, \"writer\" : true } } then copy the Message Route Id and attach it to the blueprint/s or device/s, in the info section the input labeled Msg Route . Whenever a device or device of that blueprint receives a message of that type, the message will first be sent to reasoner. Here, the reason script can examine, manipulate or run custom functionality before updating the device information. The second way to trigger a reason script is through a timer. Timer's can be set in two ways. The first way is through the Imagine UI, under the timer section. The second way to create a timer is to send a message on the scheduler topic with a SetTimerMsg . A more detailed explanation of timers can be found in scheduler . The Third way to trigger a reason script is by sending a message to the reason topic.","title":"Triggering"},{"location":"services/reason/#run","text":"By default reason scripts start with the function exports.run . This function has two main arguments, a message and a context . A description of the context can be found below. The message is the message that triggered the script. This is most useful when the script is set to trigger on a message route, since the message will have the incoming device data which can then be examined and manipulated. If the function is async, a third done argument needs to be called if there is no return statement. Additionally, when reasoner is triggered via a message route, the return can change of affect the message. When a null is returned, the message will not be changed, and the message will continue on its route as normal. When a false is returned, it will stop the message. If an object is returned, reasoner will forward that message on instead of the message that triggered it. If invoking a script through the reason topic. A different entry function can be specified than the run function. Every function that is intended to be used as an entry point needs to have a needs property attached to it. The needs is a JSON object that specifies all of the data the context needs in order for the function to be executed. If all of the parameters specified in needs cannot be met, the script will not run. Additionally, anything not specified by the needs is not guaranteed to be included in the context when the function is executed.","title":"Run"},{"location":"services/reason/#context","text":"Every time a reason script gets triggered, it is invoked with a context. The context changes slightly based on how the script is triggered. There are always several methods that can be invoked through the context (these do not have to be specified in the needs). More details about these methods can be found in the Reasoner library. In addition to this, device and system information can be attached to the context if it is triggered through a message route. If a timer is set to trigger a reason script, params will be attached to the context, which is a JSON object that can be specified in the UI, under the options of the timer.","title":"Context"},{"location":"services/reason/#logging-and-debugging","text":"Logging can be done in reason scripts. A log and error method are attached to the context. Their output will go into GCP logging. They can be viewed there, additionally, they are scraped by StackDriver, and can be seen in the Leverege UI.","title":"Logging and Debugging"},{"location":"services/reason/#configuration","text":"","title":"Configuration"},{"location":"services/reason/#required","text":"Name Description CF_TRANSPORT_CONFIG transport config for the cloud function readers and writers MODEL_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for models MODEL_FIREBASE_DATABASE_URL url to firebase database for models DATA_FIREBASE_DATABASE_URL url to the firebase database for data DATA_FIREBASE_SERVICE_ACCOUNT service account key to firebase database for data","title":"Required"},{"location":"services/reason/#optional","text":"Name Description Default TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, not needed if TRANSPORT_CONFIG is specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, not needed if TRANSPORT_CONFIG is specified 4150","title":"Optional"},{"location":"services/scheduler/","text":"Scheduler is a service that schedules timers to perform actions at a later time or in a repeating fashion. It is important to use scheduler over built in methods like setTimeout and setInterval for efficiency. Scheduler operates by saving timers to a SQL database. This database is checked at an interval specified in the config. During this check, all the timers that have reached their expiration have their functionality executed. Timers can be created, deleted and modified in three ways. The first way is manually through the Leverege UI, under the timers section. The second way is to start a timer message in the Leverege Platform. There are several of these messages and they can be found in detail in the Messages library. The last way to interact with Timers is through the api routes exposed by scheduler. Timers have two main attributes, the first attribute is focused on when the timers will run. Timers can be set to occur once, at a set interval or on a cron schedule. The second attribute determines what action will be taken. Actions can be to publish a message to a topic, trigger a script or query a url. Configuration \u00b6 All configuration options for scheduler have defaults set, but important information to set up a SQL connection is included that will almost always require reconfiguring. Optional \u00b6 Name Description Default TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, if TRANSPORT_CONFIG is not specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, if TRANSPORT_CONFIG is not specified 4150 SCHEDULER_TOPIC topic that scheduler will listen to for incoming messages 'scheduler' PORT port that scheduler will listen to 9937 CORS_ORIGIN acceptable CORS origin '*' POLL_EVERY time interval in ms that scheduler will check all timers 2000 SCHEDULER_LOOK_AHEAD ms that will be added to check if timers have passed 250 SCHEDULER_BATCH_SIZE size of the batches that will be processed 1000 RESCHEDULER_BATCH_SIZE size of batches when timers are rescheduled 100 SQL_HOST SQL host 'localhost' SQL_PORT SQL port 3306 SQL_USER SQL user 'root' SQL_PASSWORD SQL password 'root' SQL_DATABASE SQL database 'scheduler' SQL_TABLE SQL table 'scheduler' SQL_DIALECT SQL dialect 'sqlite' SQL_DIALECT_OPTIONS SQL dialect options undefined SQL_STORAGE SQL storage './database.sqlite3' SQL_TIMEZONE SQL timezone '+00:00' SQL_POOL_CONNECTION_LIMIT SQL pool connection limit 10 SQL_POOL_MAX_CONNECTION SQL pool max connections 10 SQL_POOL_MIN_CONNECTION SQL pool min connections 1 SQL_POOL_ACQUIRE SQL pool acquire 30000 SQL_POOL_IDLE SQL pool idle 10000 SQL_CA SQL ssl ca undefined SQL_CLIENT_CERT SQL ssl cert undefined SQL_CLIENT_KEY SQL ssl key undefined","title":"Scheduler"},{"location":"services/scheduler/#configuration","text":"All configuration options for scheduler have defaults set, but important information to set up a SQL connection is included that will almost always require reconfiguring.","title":"Configuration"},{"location":"services/scheduler/#optional","text":"Name Description Default TRANSPORT_CONFIG config for readers and writers { type, host, port } NSQ_HOST_ADDR host address for NSQ, if TRANSPORT_CONFIG is not specified '127.0.0.1' NSQ_HOST_PORT port for NSQ, if TRANSPORT_CONFIG is not specified 4150 SCHEDULER_TOPIC topic that scheduler will listen to for incoming messages 'scheduler' PORT port that scheduler will listen to 9937 CORS_ORIGIN acceptable CORS origin '*' POLL_EVERY time interval in ms that scheduler will check all timers 2000 SCHEDULER_LOOK_AHEAD ms that will be added to check if timers have passed 250 SCHEDULER_BATCH_SIZE size of the batches that will be processed 1000 RESCHEDULER_BATCH_SIZE size of batches when timers are rescheduled 100 SQL_HOST SQL host 'localhost' SQL_PORT SQL port 3306 SQL_USER SQL user 'root' SQL_PASSWORD SQL password 'root' SQL_DATABASE SQL database 'scheduler' SQL_TABLE SQL table 'scheduler' SQL_DIALECT SQL dialect 'sqlite' SQL_DIALECT_OPTIONS SQL dialect options undefined SQL_STORAGE SQL storage './database.sqlite3' SQL_TIMEZONE SQL timezone '+00:00' SQL_POOL_CONNECTION_LIMIT SQL pool connection limit 10 SQL_POOL_MAX_CONNECTION SQL pool max connections 10 SQL_POOL_MIN_CONNECTION SQL pool min connections 1 SQL_POOL_ACQUIRE SQL pool acquire 30000 SQL_POOL_IDLE SQL pool idle 10000 SQL_CA SQL ssl ca undefined SQL_CLIENT_CERT SQL ssl cert undefined SQL_CLIENT_KEY SQL ssl key undefined","title":"Optional"},{"location":"services/transponder/","text":"Transponder is the service that's responsible for writing device data and events to all of the databases that Api-Server is capable of querying. Structure \u00b6 Transponder is structured as a stateless Reader that employs a sub-set of its possible writers (configured through env variables) to put data in a set of databases in various formats. Each individual writer is stand-alone, and does not rely on any other writer Configuration \u00b6 Transponder is over-all configured with a run-mode that determines the set of writers that it uses. Some examples of different run-modes are historical, real time, firebase, sql, timescale, etc. Each individual writers configuration is based upon what is needed to connect to the pertinent database. For example, a writer that puts data into MySQL requires the host, port, username, and password of the CloudSQL instance in order to connect to it. There are certain environment variables that are either shared between all writers, or exist external to the writers themselves namely: ENV Variable description CACHE_CONFIG configuration for connecting to a caching service, see Cache TRANSPORT_CONFIG configuration for connecting to a message transport system, see Message Queue AUTO_ACK how to ack incoming messages ACK_ON_ERROR whether or not to ack messages that fail writing Writers \u00b6 There is at least one writer for the following message types: deviceDataEventMsg deviceAckMsg deviceEventAckMsg outboundInitMsg outboundSentMsg When a message is received by Transponder's processors, it routes every message through all of the configured writers. BigQuery \u00b6 BigQuery is Google's Data Warehouse/Analytics database. It is designed for querying massive amounts of data. The Leverege Platform currently uses BigQuery for its long-term data storage. Environment \u00b6 ENV Variable description DATA_BIGQUERY_SERVICE_ACCOUNT file location of the service account to access BigQuery DATA_BIGQUERY_PROJECT name of the Google Project to use when writing to BigQuery BQ_CACHE_TIMEOUT (optional) variable that controls the writer's table caching Util \u00b6 The BigQuery util is a generic writer for all of the possible formats and messages that are ultimately written to BigQuery. It is configured with a schema and template table. This is used to automatically create tables and insert data. This is not a writer in-and-of-itself, but is instead used by the other BigQuery writers JSON \u00b6 The BigQueryJson writers write device data and events into BigQuery using the BigQuery util. The basic structure for this data (which varies slightly across tables) is: column description id table primary key time time of the data deviceId id of the device data is for systemId system the device is in blueprintId type of the device data data associated with the original message userId user who create the data or event (if applicable) Flat (deprecated) \u00b6 The BigQueryJson writers write device data and events into BigQuery using the BigQuery util. The basic structure for this data is: column description time time of the data path path into the data this represents value updated value at that path in the data CloudSQL \u00b6 Writes data in both the JSON and flat data formats (described in the BigQuery section) to a MySQL database Environment \u00b6 ENV Variable description SQL_MAX_CONNECTIONS max size of the connection pool for writing SQL_HOST host of the SQL server SQL_PORT port of the SQL server SQL_USER user to authenticate as SQL_PASSWORD password to authenticate with SQL_KEEP_TIME used to throw out stale messages with old timestamps upon arrival SQL_CA used when the SQL server requires ssl connections SQL_CLIENT_CERT used when the SQL server requires ssl connections SQL_CLIENT_KEY used when the SQL server requires ssl connections SQL_CACHE_TIMEOUT (optional) used to control the writer's table caching Firebase \u00b6 Writes data into Firebase in several formats. Environment \u00b6 ENV Variable description DATA_FIREBASE_CONFIG location of the firebase config file to authenticate with DATA_FIREBASE_SERVICE_ACCOUNT location of the firebase service account to authenticate with FB_CACHE_TIMEOUT (optional) unused by firebase events \u00b6 Writes events into Firebase using the JSON format (described in the BigQuery section) Outbound messaging \u00b6 Writes outbound messages and their current state using: outboundInitMsg outboundSentMsg deviceAckMsg This handles the entire outbound messaging system RT \u00b6 Writes the data portion of the deviceDataEventMsg to firebase. Each data array is treated as a path to update the overall realtime state of the device. TimescaleDB \u00b6 Writes data in the JSON data format (described in the BigQuery section) to a TimescaleDB database Environment \u00b6 ENV Variable description PG_POOL_CONNECTION_LIMIT max size of the connection pool for writing PG_PORT port of the postgres server PG_HOST host of the postgres server PG_USER postgres username PG_PASSWORD postgres password PG_DATABASE postgres database to connect to TDB_CACHE_TIMEOUT (optional) used to control the writer's table caching","title":"Transponder"},{"location":"services/transponder/#structure","text":"Transponder is structured as a stateless Reader that employs a sub-set of its possible writers (configured through env variables) to put data in a set of databases in various formats. Each individual writer is stand-alone, and does not rely on any other writer","title":"Structure"},{"location":"services/transponder/#configuration","text":"Transponder is over-all configured with a run-mode that determines the set of writers that it uses. Some examples of different run-modes are historical, real time, firebase, sql, timescale, etc. Each individual writers configuration is based upon what is needed to connect to the pertinent database. For example, a writer that puts data into MySQL requires the host, port, username, and password of the CloudSQL instance in order to connect to it. There are certain environment variables that are either shared between all writers, or exist external to the writers themselves namely: ENV Variable description CACHE_CONFIG configuration for connecting to a caching service, see Cache TRANSPORT_CONFIG configuration for connecting to a message transport system, see Message Queue AUTO_ACK how to ack incoming messages ACK_ON_ERROR whether or not to ack messages that fail writing","title":"Configuration"},{"location":"services/transponder/#writers","text":"There is at least one writer for the following message types: deviceDataEventMsg deviceAckMsg deviceEventAckMsg outboundInitMsg outboundSentMsg When a message is received by Transponder's processors, it routes every message through all of the configured writers.","title":"Writers"},{"location":"services/transponder/#bigquery","text":"BigQuery is Google's Data Warehouse/Analytics database. It is designed for querying massive amounts of data. The Leverege Platform currently uses BigQuery for its long-term data storage.","title":"BigQuery"},{"location":"services/transponder/#environment","text":"ENV Variable description DATA_BIGQUERY_SERVICE_ACCOUNT file location of the service account to access BigQuery DATA_BIGQUERY_PROJECT name of the Google Project to use when writing to BigQuery BQ_CACHE_TIMEOUT (optional) variable that controls the writer's table caching","title":"Environment"},{"location":"services/transponder/#util","text":"The BigQuery util is a generic writer for all of the possible formats and messages that are ultimately written to BigQuery. It is configured with a schema and template table. This is used to automatically create tables and insert data. This is not a writer in-and-of-itself, but is instead used by the other BigQuery writers","title":"Util"},{"location":"services/transponder/#json","text":"The BigQueryJson writers write device data and events into BigQuery using the BigQuery util. The basic structure for this data (which varies slightly across tables) is: column description id table primary key time time of the data deviceId id of the device data is for systemId system the device is in blueprintId type of the device data data associated with the original message userId user who create the data or event (if applicable)","title":"JSON"},{"location":"services/transponder/#flat-deprecated","text":"The BigQueryJson writers write device data and events into BigQuery using the BigQuery util. The basic structure for this data is: column description time time of the data path path into the data this represents value updated value at that path in the data","title":"Flat (deprecated)"},{"location":"services/transponder/#cloudsql","text":"Writes data in both the JSON and flat data formats (described in the BigQuery section) to a MySQL database","title":"CloudSQL"},{"location":"services/transponder/#environment_1","text":"ENV Variable description SQL_MAX_CONNECTIONS max size of the connection pool for writing SQL_HOST host of the SQL server SQL_PORT port of the SQL server SQL_USER user to authenticate as SQL_PASSWORD password to authenticate with SQL_KEEP_TIME used to throw out stale messages with old timestamps upon arrival SQL_CA used when the SQL server requires ssl connections SQL_CLIENT_CERT used when the SQL server requires ssl connections SQL_CLIENT_KEY used when the SQL server requires ssl connections SQL_CACHE_TIMEOUT (optional) used to control the writer's table caching","title":"Environment"},{"location":"services/transponder/#firebase","text":"Writes data into Firebase in several formats.","title":"Firebase"},{"location":"services/transponder/#environment_2","text":"ENV Variable description DATA_FIREBASE_CONFIG location of the firebase config file to authenticate with DATA_FIREBASE_SERVICE_ACCOUNT location of the firebase service account to authenticate with FB_CACHE_TIMEOUT (optional) unused by firebase","title":"Environment"},{"location":"services/transponder/#events","text":"Writes events into Firebase using the JSON format (described in the BigQuery section)","title":"events"},{"location":"services/transponder/#outbound-messaging","text":"Writes outbound messages and their current state using: outboundInitMsg outboundSentMsg deviceAckMsg This handles the entire outbound messaging system","title":"Outbound messaging"},{"location":"services/transponder/#rt","text":"Writes the data portion of the deviceDataEventMsg to firebase. Each data array is treated as a path to update the overall realtime state of the device.","title":"RT"},{"location":"services/transponder/#timescaledb","text":"Writes data in the JSON data format (described in the BigQuery section) to a TimescaleDB database","title":"TimescaleDB"},{"location":"services/transponder/#environment_3","text":"ENV Variable description PG_POOL_CONNECTION_LIMIT max size of the connection pool for writing PG_PORT port of the postgres server PG_HOST host of the postgres server PG_USER postgres username PG_PASSWORD postgres password PG_DATABASE postgres database to connect to TDB_CACHE_TIMEOUT (optional) used to control the writer's table caching","title":"Environment"},{"location":"stable/api/api/","text":"The Leverege Api Library was created for developers, external servers and web pages to quickly interface with the Leverege REST API. The api library provides chains of properties methods that often return promises which represent the result of a corresponding api calls. api-service \u00b6 The api-service library forms a wrapper around the api library. The api-service library should be used instead of the api-library in all environments that are not run in a browser. The wrapper adds node-fetch and uses it for all its queries. In addition to this, the wrapper adds support for OAuth Key/Secret access. Setup \u00b6 Currently, access libraries only exist in Javascript and libraries for other languages are coming soon. Create a new node project and install the @leverege/api library for developing in web settings, or @leverege/api-service for developing in Node.js settings. npm install --save @leverege/api or npm install --save @leverege/api-service Import the library into a node file in the project: const Api = require ( '@leverege/api' ) or const Api = require ( '@leverege/api-service' ) Authentication \u00b6 To obtain access to the Leverege api, authentication is required. There are two major types of authentication, OAuth Key/Secret, and username/password. OAuth Key/Secret \u00b6 Note: @leverege/api-service must be used for key/secret authentication To begin, create an Api Access Key. Login to the Leverege UI, Select the Settings option on the lower left and press the Add Api Access button. There will be a new Api Access item in the list. It is recommended to name the item with something meaningful. Back in the node file, copy the apiKey and secret into the file as constants: const apiKey = 'YOUR_API_KEY' const secret = 'YOUR_API_SECRET' It is recommended that the apiKey/secret are not directly committed into code and that these are set up as environment variables or command line parameters. Next create the Api instance let api = Api . init ( { host : 'https://js-imagine-api.leverege.com' , apiKey , secret } ) Username/Password Access \u00b6 Note: this is accessible in both the api and api-service libraries In order to start using the username and password access to the api, first create an account in the User tab of your project. Clicking the add user button will prompt you for all the user information needed. Once this is completed the user can login using: const Api = require ( '@leverege/api' ) const api = api . init ( { host : 'https://js-imagine-api.leverege.com' } ) // username and password created const username = 'test' const password = 'password123' // the id of your project const projectId = 'test-project' await api . login ( username , password , projectId ) It is recommended that the username/password are not directly committed into code and that these are set up as environment variables or command line parameters. At this point the user can login, but they do not have access to anything. Once a user has been created, they must be given access to parts of the system in order to be able to see any data after they login. A user can be added and given permissions on a system or a device . In the ui a user can be given permissions on a system using the Access tab. To give a user permissions on a device use the Lol library. OpenId Connect Login \u00b6 To login using an OpenId Connect server as the authentication client, you must add an OidcClient object to your project. This would look something like: const client = await api.project( project ).oidcClients().create( { name : 'Id4', alias : 'id4', discoveryUrl : 'https://demo.identityserver.io/.well-known/openid-configuration', clientId : 'server.code', clientSecret : 'secret', } ) If you want to dynamically present available OpenId clients, get the projects available clients and make a button for each url const oidc = await api.project( projectId ).oidcClients().list() oidc.items.forEach( oidc => { const url = api.oidcLoginUrl( oidc, successUrl, failureUrl ) // make button linking to url }) If you have assigned aliases to the OidcClients, you can simply make a link using the projectId and alias: const url = api.oidcLoginUrl( { projectId, alias : 'google' }, successUrl, failureUrl ) } On successful login, the successUrl will be called with 'token' as a query parameter. Call ``` api.setToken( token ) ``` to set the header. You should now be able to make Api queries. When an OpenId User firsts logins in, the system will construct a new User for that OpenId. TO give that user permissions prior to them loggin in, the PendingUser calls can be used. To add a PendingUser: await api.project( prjId ).pendingUsers().add( { issuer : 'https://accounts.google.com', emailOrName : 'me@there.com', perm : 'role', permValue : 'user', contextType : 'system', // project, system, or device contextId : systemId // the id of the project, system or device } ) To Remove the user, send the exact parameters to delete() await api.project( prjId ).pendingUsers().delete( { issuer : 'https://accounts.google.com', emailOrName : 'me@there.com', perm : 'role', permValue : 'user', contextType : 'system', // project, system, or device contextId : systemId // the id of the project, system or device } ) To see the PendingUsers permissions, call list() ap.project( prjId ).pendingUsers().list() Making Calls \u00b6 At this point, we're ready to make some calls into the platform. As a note, many of these calls require a projectId, which can be found on the same Settings page that the Api Access Keys are made. The final code looks like this: const Api = require ( '@leverege/api-service' ) const apiKey = 'YOUR_API_KEY' const secret = 'YOUR_API_SECRET' let api = Api . init ( { apiKey , secret }) Most calls in the Api return a Promise. The pattern used when making calls will look something like this: let result try { result = await api . project ( projectId ). get () } catch ( ex ) { // do something } // data See further documentation for all available calls.","title":"API"},{"location":"stable/api/api/#api-service","text":"The api-service library forms a wrapper around the api library. The api-service library should be used instead of the api-library in all environments that are not run in a browser. The wrapper adds node-fetch and uses it for all its queries. In addition to this, the wrapper adds support for OAuth Key/Secret access.","title":"api-service"},{"location":"stable/api/api/#setup","text":"Currently, access libraries only exist in Javascript and libraries for other languages are coming soon. Create a new node project and install the @leverege/api library for developing in web settings, or @leverege/api-service for developing in Node.js settings. npm install --save @leverege/api or npm install --save @leverege/api-service Import the library into a node file in the project: const Api = require ( '@leverege/api' ) or const Api = require ( '@leverege/api-service' )","title":"Setup"},{"location":"stable/api/api/#authentication","text":"To obtain access to the Leverege api, authentication is required. There are two major types of authentication, OAuth Key/Secret, and username/password.","title":"Authentication"},{"location":"stable/api/api/#oauth-keysecret","text":"Note: @leverege/api-service must be used for key/secret authentication To begin, create an Api Access Key. Login to the Leverege UI, Select the Settings option on the lower left and press the Add Api Access button. There will be a new Api Access item in the list. It is recommended to name the item with something meaningful. Back in the node file, copy the apiKey and secret into the file as constants: const apiKey = 'YOUR_API_KEY' const secret = 'YOUR_API_SECRET' It is recommended that the apiKey/secret are not directly committed into code and that these are set up as environment variables or command line parameters. Next create the Api instance let api = Api . init ( { host : 'https://js-imagine-api.leverege.com' , apiKey , secret } )","title":"OAuth Key/Secret"},{"location":"stable/api/api/#usernamepassword-access","text":"Note: this is accessible in both the api and api-service libraries In order to start using the username and password access to the api, first create an account in the User tab of your project. Clicking the add user button will prompt you for all the user information needed. Once this is completed the user can login using: const Api = require ( '@leverege/api' ) const api = api . init ( { host : 'https://js-imagine-api.leverege.com' } ) // username and password created const username = 'test' const password = 'password123' // the id of your project const projectId = 'test-project' await api . login ( username , password , projectId ) It is recommended that the username/password are not directly committed into code and that these are set up as environment variables or command line parameters. At this point the user can login, but they do not have access to anything. Once a user has been created, they must be given access to parts of the system in order to be able to see any data after they login. A user can be added and given permissions on a system or a device . In the ui a user can be given permissions on a system using the Access tab. To give a user permissions on a device use the Lol library.","title":"Username/Password Access"},{"location":"stable/api/api/#openid-connect-login","text":"To login using an OpenId Connect server as the authentication client, you must add an OidcClient object to your project. This would look something like: const client = await api.project( project ).oidcClients().create( { name : 'Id4', alias : 'id4', discoveryUrl : 'https://demo.identityserver.io/.well-known/openid-configuration', clientId : 'server.code', clientSecret : 'secret', } ) If you want to dynamically present available OpenId clients, get the projects available clients and make a button for each url const oidc = await api.project( projectId ).oidcClients().list() oidc.items.forEach( oidc => { const url = api.oidcLoginUrl( oidc, successUrl, failureUrl ) // make button linking to url }) If you have assigned aliases to the OidcClients, you can simply make a link using the projectId and alias: const url = api.oidcLoginUrl( { projectId, alias : 'google' }, successUrl, failureUrl ) } On successful login, the successUrl will be called with 'token' as a query parameter. Call ``` api.setToken( token ) ``` to set the header. You should now be able to make Api queries. When an OpenId User firsts logins in, the system will construct a new User for that OpenId. TO give that user permissions prior to them loggin in, the PendingUser calls can be used. To add a PendingUser: await api.project( prjId ).pendingUsers().add( { issuer : 'https://accounts.google.com', emailOrName : 'me@there.com', perm : 'role', permValue : 'user', contextType : 'system', // project, system, or device contextId : systemId // the id of the project, system or device } ) To Remove the user, send the exact parameters to delete() await api.project( prjId ).pendingUsers().delete( { issuer : 'https://accounts.google.com', emailOrName : 'me@there.com', perm : 'role', permValue : 'user', contextType : 'system', // project, system, or device contextId : systemId // the id of the project, system or device } ) To see the PendingUsers permissions, call list() ap.project( prjId ).pendingUsers().list()","title":"OpenId Connect Login"},{"location":"stable/api/api/#making-calls","text":"At this point, we're ready to make some calls into the platform. As a note, many of these calls require a projectId, which can be found on the same Settings page that the Api Access Keys are made. The final code looks like this: const Api = require ( '@leverege/api-service' ) const apiKey = 'YOUR_API_KEY' const secret = 'YOUR_API_SECRET' let api = Api . init ( { apiKey , secret }) Most calls in the Api return a Promise. The pattern used when making calls will look something like this: let result try { result = await api . project ( projectId ). get () } catch ( ex ) { // do something } // data See further documentation for all available calls.","title":"Making Calls"},{"location":"stable/api/blueprint/","text":"Blueprints are structs for devices, they define what data a device will hold. Blueprint methods \u00b6 get() - gets the blueprint update( patchOps ) - patches the blueprint delete() - deletes the blueprint simulate() - runs a simulation on the blueprint attributes() - gets all the attributes for the blueprint","title":"Blueprint"},{"location":"stable/api/blueprint/#blueprint-methods","text":"get() - gets the blueprint update( patchOps ) - patches the blueprint delete() - deletes the blueprint simulate() - runs a simulation on the blueprint attributes() - gets all the attributes for the blueprint","title":"Blueprint methods"},{"location":"stable/api/child/","text":"Child is a generic way to interact with the Leverege api. It takes a path argument which it uses as the url in http requests. Most times it is used, the path is determined and maintained by the api library. Methods \u00b6 get( options ) - makes a get request to the path intended for a single object list( options ) - makes a get request to the path intended for a group or collection post( body, options ) - make a post request to the path put( body, options ) - make a put request to the path patch( body, options ) - make a patch request to the path patchRaw( body, options ) - make a patch request to the path, will erase all old data delete( body, options ) - make a delete request to the path child( subpath ) - new Child instance with the subpath appended to the current path path() - returns the current path api() - returns the instance of the api","title":"Child"},{"location":"stable/api/child/#methods","text":"get( options ) - makes a get request to the path intended for a single object list( options ) - makes a get request to the path intended for a group or collection post( body, options ) - make a post request to the path put( body, options ) - make a put request to the path patch( body, options ) - make a patch request to the path patchRaw( body, options ) - make a patch request to the path, will erase all old data delete( body, options ) - make a delete request to the path child( subpath ) - new Child instance with the subpath appended to the current path path() - returns the current path api() - returns the instance of the api","title":"Methods"},{"location":"stable/api/collection/","text":"A collection represents a group of objects on the Leverege platform. Collections contain generic methods to interact with all the different collections. Methods \u00b6 list() - returns a list with all of the objects in the collection create( object ) - creates an object in the collection get( id ) - gets the object in the collection update( id, patchOps ) - updates the object with the provided data updateRaw( id, patchOps ) - removes the old data for the object delete( id ) - deletes the object from the collection child( id ) - returns an instance of child with the id appended to the current path","title":"Collection"},{"location":"stable/api/collection/#methods","text":"list() - returns a list with all of the objects in the collection create( object ) - creates an object in the collection get( id ) - gets the object in the collection update( id, patchOps ) - updates the object with the provided data updateRaw( id, patchOps ) - removes the old data for the object delete( id ) - deletes the object from the collection child( id ) - returns an instance of child with the id appended to the current path","title":"Methods"},{"location":"stable/api/device/","text":"Through the api, a devices can be created, deleted and changed. Query Device \u00b6 let device try { device = await api . device ( deviceId ). get () } catch ( ex ) { // error handling } The device variable in the script will then have all of the device information. That information will include Metadata, ids, data and all other information associated with the device. Other methods are: update( newDeviceInfo ) - patches the device with the newDeviceInfo delete() - deletes the device history( options ) - options should include a query to make against the device history events() - has a list method, create and ack method getHistorical( options ) - makes a get request to the historical path of the device setValue( path, value, options ) - sets value on the device at path startSim( options ) - starts a simulation on the device pauseSim( options ) - pauses a simulation on the device restSim( options ) - resets a simulation on the device playScenario( options ) - plays a scenario on the device stopScenario( options ) - stops a scenario on the device getUpdater( path ) - makes a call to get an updater getUpdaters( options ) - makes a call to get the updaters for a device verify( options ) users() - object to interact with the system users Creating a Device \u00b6 To create a new device, the system method of api needs to be used. const device = { ... newDeviceInfo } try { api . system ( systemId ). devices (). create ( device ) } catch ( ex ) { // error handling } after this call has been resolved, there will be a new device for the system.","title":"Device"},{"location":"stable/api/device/#query-device","text":"let device try { device = await api . device ( deviceId ). get () } catch ( ex ) { // error handling } The device variable in the script will then have all of the device information. That information will include Metadata, ids, data and all other information associated with the device. Other methods are: update( newDeviceInfo ) - patches the device with the newDeviceInfo delete() - deletes the device history( options ) - options should include a query to make against the device history events() - has a list method, create and ack method getHistorical( options ) - makes a get request to the historical path of the device setValue( path, value, options ) - sets value on the device at path startSim( options ) - starts a simulation on the device pauseSim( options ) - pauses a simulation on the device restSim( options ) - resets a simulation on the device playScenario( options ) - plays a scenario on the device stopScenario( options ) - stops a scenario on the device getUpdater( path ) - makes a call to get an updater getUpdaters( options ) - makes a call to get the updaters for a device verify( options ) users() - object to interact with the system users","title":"Query Device"},{"location":"stable/api/device/#creating-a-device","text":"To create a new device, the system method of api needs to be used. const device = { ... newDeviceInfo } try { api . system ( systemId ). devices (). create ( device ) } catch ( ex ) { // error handling } after this call has been resolved, there will be a new device for the system.","title":"Creating a Device"},{"location":"stable/api/history/","text":"The history interface allows a user to make complex queries against the historical and event data stored by the system against devices, systems, and projects. Api Endpoints \u00b6 Using the @leverege/api npm package, there are several available functions that will allow for fetching history and event data. They are: // For device level history/events api.device( 'deviceId' ).history( opts ) api.device( 'deviceId' ).events( opts ) // For system level history/events api.system( 'systemId' ).device.history( opts ) api.system( 'systemId' ).device.events( opts ) // For project level history/events api.project( 'projectId' ).device.history( opts ) api.project( 'projectId' ).device.events( opts ) Query Format \u00b6 Overall Format \u00b6 Option Description required default start the time to start getting data true n/a end the time to stop getting data false now paths the list of attributes/data paths to get: details false all data where the list of logical comparisons to filter returned rows: details false null group the set of data to group by: details false null order the set of data to order by: details false null Paths Format \u00b6 Paths are formatted as an array of either arrays or strings where each sub-array/string represents a path to get from the data string paths \u00b6 string paths are the simplest way of referencing data in each row. Paths can reference one of two things, either a root column or a deep path into a JSON data object. examples \u00b6 given a row: id time data userId test-id 2019-02-20T15:52:00.525Z { \"foo\" : { \"bar\" : \"baz\" } } test-user the following paths return the following data: $id - the id column $time - the time column $data - the data column foo/bar - the deep foo/bar path in the data column, in this case \"baz\" $data/foo - the deep foo path in the data column, returned as a string given a more complex schema, such as the events, which have both an \"event\" model and and an \"ack\" model: event \u00b6 id time type data userId test-id 2019-02-20T15:52:00.525Z default { \"foo\" : { \"bar\" : \"baz\" } } test-user ack \u00b6 id time data userId test-id 2019-02-20T16:14:01.483Z { \"ackedBy\" : \"James\" } test-user the paths default to referencing the \"event\" model, so in order to reference the \"ack\" model, special syntax must be used: $ack.time - the time the event was acknowledged $event.time - the time of the event $time - the time of the event (same as $event.time ) $ack.data/ackedBy - the ackedBy field in the data column of the ack model, \"James\" in this instance $event.data/foo/bar - \"baz\" array paths \u00b6 Array paths work much like string paths, but in an extended format. The basic format of an array path is: [ \"accessor string\" , \"alias\" , { fun c t io n : \"function name\" , args : [ \"@COLUMN()\" ] }, \"cast type\" ] Index Description required default 0 the accessor string as mentioned above true n/a 1 the alias (the key in the returned object) false the last key in the path 2 a function descriptor object (see here ) false null 3 a type to cast the result to (see here ) false null examples \u00b6 The array path: [ \"$time\", \"hourTime\", { \"function\" : \"BUCKET_TIME\", \"args\" : [ 3600000 ] } ] Will get the time for each row, aggregate it based on a bucket of time it falls into, and return it in the field \"hourTime\". The width of each bucket in this instance is one hour, so all data points from the same hour would be bucketed into a single final data point. This is a good way to get averages or other metrics across other fields through time. function descriptors \u00b6 In order to run calculations on data in rows, or aggregregate data across rows, you have to use function descriptors. The format of function descriptors is an object as follows: Option Description required default function the function to perform true n/a args the arguments to pass into the function false [ \"@COLUMN()\"] type the type to cast the input COLUMN to (see here ) false null TODO : type is not specific enough for multi-argument functions examples \u00b6 { \"function\" : \"+\", \"arguments\" : [ \"@COLUMN()\", 1 ] } This function takes the existing referenced column and adds 1 to each value { \"function\" : \"AVG\", \"type\" : \"float\" } This function takes the referenced column, casts it as a column of floats, and averages it. Aggregating functions typically require some form of grouping to be useful. *** Despite the work to make all backend db's seem identical, there are occasionally subtle differences in the db's that may require different casting or different function names depending on which database is being referenced. *** Where \u00b6 The where clause is used to filter the returned rows in your dataset. the general syntax is: [ <path specifier>, <operator>, <value or different path specifier***> ] in this, a column identifier looks identical to a path from the previous section *** in the second part of a comparison the path specifier must be wrapped as such: { type : 'field', value : <path specifier> } . This is to accomadate the fact that an array here might relate to an \"IN\" operator, and must then be differentiated examples \u00b6 simple \u00b6 The following will return rows where the number of gps satellites is greater than 9. [ 'vessel/location/numSatellites' , '>' , 9 ] IN \u00b6 The IN clause makes things a bit more complex, and requires use of an array. The following will return any rows where the deviceId is in the set of 'val1' and 'val2'. [ '$deviceId', 'IN', [ 'val1', 'val2' ] ] complex \u00b6 The following will filter for where the time of a row in msTime is greater than the msTime of the previous row + 50 ms. This will ultimately only return rows where the gaps between rows is greater than 50 milliseconds. [ '$time' , null , { function : 'UNIX_MILLIS' } ], '>' , { type : 'field' , value : [ '$lagTime' , null , { function : '+' , args : [ { function : 'UNIX_MILLIS' }, 50 ] } ] } ] Casting \u00b6 The type is the last optional entry in a path array, or the specified value in a function descriptor. Supported types right now are: boolean integer float string time msTime object the two special types are msTime and object. msTime will ensure that a date field is returned in ms since the epoch format, and object will ensure that an object is returned as such instead of as a JSON string. Grouping \u00b6 Grouping is the option that is responsible for ensuring that aggregate functions occur over sets of data where a specific value is the same. A great example of this is bucket time. Take the following path array: [ \"$time\", \"bTime\", { \"function\" : \"BUCKET_TIME\", \"args\" : [ 900000 ] } ] All BUCKET_TIME does on its own is essentially truncate a timestamp to a certain level of accuracy. In the example above, each timestamp is essentially floored to its last 15 minute value. So with two rows like: id time data userId test-id 2019-02-20T16:14:01.483Z { \"foo\" : 5 } test-user test-id-2 2019-02-20T16:03:01.420Z { \"foo\" : 15 } test-user they will both be turned into id bTime data userId test-id 2019-02-20T16:00:00.000Z { \"foo\" : 5 } test-user test-id-2 2019-02-20T16:00:00.000Z { \"foo\" : 15 } test-user Note that in the returned data the time is the same, but two rows were returned. Now imagine that your group options look like: [ \"bTime\" ] and your full paths array looked like: [ [ \"$time\" , \"bTime\" , { \"function\" : \"BUCKET_TIME\" , \"args\" : [ 900000 ] } ], [ \"foo\" , \"aFoo\" , ], { \"function\" : \"AVG\" } ] ] the returned rows would now be: bTime aFoo 2019-02-20T16:00:00.000Z 10 where the time is bucketed and the result of aFoo is the average of the foo value for the two rows Ordering \u00b6 Because the nature of our data is almost exclusively time-series, it is likely you will want to order by time in some fashion. In order to do so one must add an array at the order key in the query. Each entry of this array will be either a string referencing an alias or root column, or an array where the first element is the alias or root column, and the second element is the direction of ordering, 'ASC' or 'DESC'. examples \u00b6 To sort by time in ascending order [ \"$time\" ] To sort by time in descending order [ [ \"$time\" , \"DESC\" ] ] To sort by a calculated field in descending order [ [ \"myFancyAlias\" , \"DESC\" ] ] Window Functions \u00b6 all window functions share similar structure in querying. They look much like other functions but have a couple extra fields. structure \u00b6 Option Description required default function the function to perform true n/a args the arguments to pass into the function false [ \"@COLUMN()\"] type the type to cast the input COLUMN to (see here ) false null isWindow is the function a window function (must be true) true null order what to order the window by false null partition what field to split the window into sections by false null example \u00b6 { fun c t io n : 'LAG' , isWi n dow : true , order : '$ t ime' , par t i t io n : '$deviceId' } As you can see in addition to normal function fields, window functions also have fields for isWindow, which must be set to true, order, to set the window order, and partition, to filter the resultant rows to relevant items. Partition is especially useful when you are querying against a set of devices and only want the window function to see rows from the same device. Convenience Functions \u00b6 In order to further unify the experience between the different databases, certain functions and argument sets were mapped from each database and exposed BUCKET_TIME \u00b6 This function allows for grouping and aggregation of data into windows of time. arguments \u00b6 name description required default value windowSize the size of each window of data in ms false 900000 (15 minutes) column the column to call the function on false @COLUMN() (the column from earlier in the attribute) DATE_FORMAT \u00b6 Date Format allows for dates and times to be returned as formatted strings arguments \u00b6 name description required default value date the date column to use ( usually @COLUMN() ) true n/a format the format to use when converting the time true n/a format specification \u00b6 the format string uses string escapes to figure out how to convert the time into a string name description YEAR MONTH DAY HOUR MINUTE SECOND SECOND_PART the second with its partial second e.g. 45.145 s Examples: @YEAR()-@MONTH()-@DAY()T@HOUR():@MINUTE():@SECOND_PART()Z returns 2019-02-27T21:19:14.153Z STR_TO_DATE \u00b6 The same as DATE_FORMAT but instead reads a string field into a date. arguments \u00b6 name description required default value string the string column to use ( usually @COLUMN() ) true n/a format the format to use when converting the time true n/a UNIX_MILLIS \u00b6 Turns any date field into its ms since epoch format. arguments \u00b6 name description required default value date the date column to use ( usually @COLUMN() ) true n/a Arithmetic Operators \u00b6 Because arithmetic requires expressions, arithmetic operators have been added in the form of functions. The functions can be referenced as + , - , * , and / . Inside, the arguments will all be combined using the operator specified examples \u00b6 function: { fun c t io n : \"*\" , args : [ 2 , 3 ] } result: 6 function: { fun c t io n : \"+\" , args : [ 2 , { fun c t io n : \"/\" , args : [ 8 , 2 ] }, { fun c t io n : \"*\" , args : [ 3 , 20 ] } ] } result: 2 + ( 8 / 2 ) + ( 3 * 20 ) 66","title":"History"},{"location":"stable/api/history/#api-endpoints","text":"Using the @leverege/api npm package, there are several available functions that will allow for fetching history and event data. They are: // For device level history/events api.device( 'deviceId' ).history( opts ) api.device( 'deviceId' ).events( opts ) // For system level history/events api.system( 'systemId' ).device.history( opts ) api.system( 'systemId' ).device.events( opts ) // For project level history/events api.project( 'projectId' ).device.history( opts ) api.project( 'projectId' ).device.events( opts )","title":"Api Endpoints"},{"location":"stable/api/history/#query-format","text":"","title":"Query Format"},{"location":"stable/api/history/#overall-format","text":"Option Description required default start the time to start getting data true n/a end the time to stop getting data false now paths the list of attributes/data paths to get: details false all data where the list of logical comparisons to filter returned rows: details false null group the set of data to group by: details false null order the set of data to order by: details false null","title":"Overall Format"},{"location":"stable/api/history/#paths-format","text":"Paths are formatted as an array of either arrays or strings where each sub-array/string represents a path to get from the data","title":"Paths Format "},{"location":"stable/api/history/#string-paths","text":"string paths are the simplest way of referencing data in each row. Paths can reference one of two things, either a root column or a deep path into a JSON data object.","title":"string paths"},{"location":"stable/api/history/#examples","text":"given a row: id time data userId test-id 2019-02-20T15:52:00.525Z { \"foo\" : { \"bar\" : \"baz\" } } test-user the following paths return the following data: $id - the id column $time - the time column $data - the data column foo/bar - the deep foo/bar path in the data column, in this case \"baz\" $data/foo - the deep foo path in the data column, returned as a string given a more complex schema, such as the events, which have both an \"event\" model and and an \"ack\" model:","title":"examples"},{"location":"stable/api/history/#event","text":"id time type data userId test-id 2019-02-20T15:52:00.525Z default { \"foo\" : { \"bar\" : \"baz\" } } test-user","title":"event"},{"location":"stable/api/history/#ack","text":"id time data userId test-id 2019-02-20T16:14:01.483Z { \"ackedBy\" : \"James\" } test-user the paths default to referencing the \"event\" model, so in order to reference the \"ack\" model, special syntax must be used: $ack.time - the time the event was acknowledged $event.time - the time of the event $time - the time of the event (same as $event.time ) $ack.data/ackedBy - the ackedBy field in the data column of the ack model, \"James\" in this instance $event.data/foo/bar - \"baz\"","title":"ack"},{"location":"stable/api/history/#array-paths","text":"Array paths work much like string paths, but in an extended format. The basic format of an array path is: [ \"accessor string\" , \"alias\" , { fun c t io n : \"function name\" , args : [ \"@COLUMN()\" ] }, \"cast type\" ] Index Description required default 0 the accessor string as mentioned above true n/a 1 the alias (the key in the returned object) false the last key in the path 2 a function descriptor object (see here ) false null 3 a type to cast the result to (see here ) false null","title":"array paths"},{"location":"stable/api/history/#examples_1","text":"The array path: [ \"$time\", \"hourTime\", { \"function\" : \"BUCKET_TIME\", \"args\" : [ 3600000 ] } ] Will get the time for each row, aggregate it based on a bucket of time it falls into, and return it in the field \"hourTime\". The width of each bucket in this instance is one hour, so all data points from the same hour would be bucketed into a single final data point. This is a good way to get averages or other metrics across other fields through time.","title":"examples"},{"location":"stable/api/history/#function-descriptors","text":"In order to run calculations on data in rows, or aggregregate data across rows, you have to use function descriptors. The format of function descriptors is an object as follows: Option Description required default function the function to perform true n/a args the arguments to pass into the function false [ \"@COLUMN()\"] type the type to cast the input COLUMN to (see here ) false null TODO : type is not specific enough for multi-argument functions","title":"function descriptors"},{"location":"stable/api/history/#examples_2","text":"{ \"function\" : \"+\", \"arguments\" : [ \"@COLUMN()\", 1 ] } This function takes the existing referenced column and adds 1 to each value { \"function\" : \"AVG\", \"type\" : \"float\" } This function takes the referenced column, casts it as a column of floats, and averages it. Aggregating functions typically require some form of grouping to be useful. *** Despite the work to make all backend db's seem identical, there are occasionally subtle differences in the db's that may require different casting or different function names depending on which database is being referenced. ***","title":"examples"},{"location":"stable/api/history/#where","text":"The where clause is used to filter the returned rows in your dataset. the general syntax is: [ <path specifier>, <operator>, <value or different path specifier***> ] in this, a column identifier looks identical to a path from the previous section *** in the second part of a comparison the path specifier must be wrapped as such: { type : 'field', value : <path specifier> } . This is to accomadate the fact that an array here might relate to an \"IN\" operator, and must then be differentiated","title":"Where "},{"location":"stable/api/history/#examples_3","text":"","title":"examples"},{"location":"stable/api/history/#simple","text":"The following will return rows where the number of gps satellites is greater than 9. [ 'vessel/location/numSatellites' , '>' , 9 ]","title":"simple"},{"location":"stable/api/history/#in","text":"The IN clause makes things a bit more complex, and requires use of an array. The following will return any rows where the deviceId is in the set of 'val1' and 'val2'. [ '$deviceId', 'IN', [ 'val1', 'val2' ] ]","title":"IN"},{"location":"stable/api/history/#complex","text":"The following will filter for where the time of a row in msTime is greater than the msTime of the previous row + 50 ms. This will ultimately only return rows where the gaps between rows is greater than 50 milliseconds. [ '$time' , null , { function : 'UNIX_MILLIS' } ], '>' , { type : 'field' , value : [ '$lagTime' , null , { function : '+' , args : [ { function : 'UNIX_MILLIS' }, 50 ] } ] } ]","title":"complex"},{"location":"stable/api/history/#casting","text":"The type is the last optional entry in a path array, or the specified value in a function descriptor. Supported types right now are: boolean integer float string time msTime object the two special types are msTime and object. msTime will ensure that a date field is returned in ms since the epoch format, and object will ensure that an object is returned as such instead of as a JSON string.","title":"Casting "},{"location":"stable/api/history/#grouping","text":"Grouping is the option that is responsible for ensuring that aggregate functions occur over sets of data where a specific value is the same. A great example of this is bucket time. Take the following path array: [ \"$time\", \"bTime\", { \"function\" : \"BUCKET_TIME\", \"args\" : [ 900000 ] } ] All BUCKET_TIME does on its own is essentially truncate a timestamp to a certain level of accuracy. In the example above, each timestamp is essentially floored to its last 15 minute value. So with two rows like: id time data userId test-id 2019-02-20T16:14:01.483Z { \"foo\" : 5 } test-user test-id-2 2019-02-20T16:03:01.420Z { \"foo\" : 15 } test-user they will both be turned into id bTime data userId test-id 2019-02-20T16:00:00.000Z { \"foo\" : 5 } test-user test-id-2 2019-02-20T16:00:00.000Z { \"foo\" : 15 } test-user Note that in the returned data the time is the same, but two rows were returned. Now imagine that your group options look like: [ \"bTime\" ] and your full paths array looked like: [ [ \"$time\" , \"bTime\" , { \"function\" : \"BUCKET_TIME\" , \"args\" : [ 900000 ] } ], [ \"foo\" , \"aFoo\" , ], { \"function\" : \"AVG\" } ] ] the returned rows would now be: bTime aFoo 2019-02-20T16:00:00.000Z 10 where the time is bucketed and the result of aFoo is the average of the foo value for the two rows","title":"Grouping "},{"location":"stable/api/history/#ordering","text":"Because the nature of our data is almost exclusively time-series, it is likely you will want to order by time in some fashion. In order to do so one must add an array at the order key in the query. Each entry of this array will be either a string referencing an alias or root column, or an array where the first element is the alias or root column, and the second element is the direction of ordering, 'ASC' or 'DESC'.","title":"Ordering "},{"location":"stable/api/history/#examples_4","text":"To sort by time in ascending order [ \"$time\" ] To sort by time in descending order [ [ \"$time\" , \"DESC\" ] ] To sort by a calculated field in descending order [ [ \"myFancyAlias\" , \"DESC\" ] ]","title":"examples"},{"location":"stable/api/history/#window-functions","text":"all window functions share similar structure in querying. They look much like other functions but have a couple extra fields.","title":"Window Functions"},{"location":"stable/api/history/#structure","text":"Option Description required default function the function to perform true n/a args the arguments to pass into the function false [ \"@COLUMN()\"] type the type to cast the input COLUMN to (see here ) false null isWindow is the function a window function (must be true) true null order what to order the window by false null partition what field to split the window into sections by false null","title":"structure"},{"location":"stable/api/history/#example","text":"{ fun c t io n : 'LAG' , isWi n dow : true , order : '$ t ime' , par t i t io n : '$deviceId' } As you can see in addition to normal function fields, window functions also have fields for isWindow, which must be set to true, order, to set the window order, and partition, to filter the resultant rows to relevant items. Partition is especially useful when you are querying against a set of devices and only want the window function to see rows from the same device.","title":"example"},{"location":"stable/api/history/#convenience-functions","text":"In order to further unify the experience between the different databases, certain functions and argument sets were mapped from each database and exposed","title":"Convenience Functions "},{"location":"stable/api/history/#bucket_time","text":"This function allows for grouping and aggregation of data into windows of time.","title":"BUCKET_TIME"},{"location":"stable/api/history/#arguments","text":"name description required default value windowSize the size of each window of data in ms false 900000 (15 minutes) column the column to call the function on false @COLUMN() (the column from earlier in the attribute)","title":"arguments"},{"location":"stable/api/history/#date_format","text":"Date Format allows for dates and times to be returned as formatted strings","title":"DATE_FORMAT"},{"location":"stable/api/history/#arguments_1","text":"name description required default value date the date column to use ( usually @COLUMN() ) true n/a format the format to use when converting the time true n/a","title":"arguments"},{"location":"stable/api/history/#format-specification","text":"the format string uses string escapes to figure out how to convert the time into a string name description YEAR MONTH DAY HOUR MINUTE SECOND SECOND_PART the second with its partial second e.g. 45.145 s Examples: @YEAR()-@MONTH()-@DAY()T@HOUR():@MINUTE():@SECOND_PART()Z returns 2019-02-27T21:19:14.153Z","title":"format specification"},{"location":"stable/api/history/#str_to_date","text":"The same as DATE_FORMAT but instead reads a string field into a date.","title":"STR_TO_DATE"},{"location":"stable/api/history/#arguments_2","text":"name description required default value string the string column to use ( usually @COLUMN() ) true n/a format the format to use when converting the time true n/a","title":"arguments"},{"location":"stable/api/history/#unix_millis","text":"Turns any date field into its ms since epoch format.","title":"UNIX_MILLIS"},{"location":"stable/api/history/#arguments_3","text":"name description required default value date the date column to use ( usually @COLUMN() ) true n/a","title":"arguments"},{"location":"stable/api/history/#arithmetic-operators","text":"Because arithmetic requires expressions, arithmetic operators have been added in the form of functions. The functions can be referenced as + , - , * , and / . Inside, the arguments will all be combined using the operator specified","title":"Arithmetic Operators"},{"location":"stable/api/history/#examples_5","text":"function: { fun c t io n : \"*\" , args : [ 2 , 3 ] } result: 6 function: { fun c t io n : \"+\" , args : [ 2 , { fun c t io n : \"/\" , args : [ 8 , 2 ] }, { fun c t io n : \"*\" , args : [ 3 , 20 ] } ] } result: 2 + ( 8 / 2 ) + ( 3 * 20 ) 66","title":"examples"},{"location":"stable/api/interface/","text":"Interface is designed to query for devices in complex relationships. Interface works with groups and objects, where groups are a grouping of devices and objects are a singular device. Queries can use * as a wildcard to query for all devices or objects. Query Interface \u00b6 Currently, the Interface methods require the systemId to work. This id can be found under the info tab of the system. In addition to the systemId , blueprintIds are required for Interface to work. let deviceInfo try { deviceInfo = await api . interface ( systemId , blueprintId ). obj ( deviceId ). get () } catch ( ex ) { // error handling } The systemId and blueprintId can be substituted with the blueprint alias and system alias. The above query will result in deviceInfo having all of the information for the device. In addition to this, let devicesInfo try { devicesInfo = await api . interface ( systemId , blueprintId ). obj ( '*' ). get () } catch ( ex ) { // error handling } will result in devicesInfo containing all the info for devices with blueprintId . In addition, interface queries can chain on relationships. let devicesInfo try { devicesInfo = await api . interface ( systemId , blueprint ). obj ( deviceId ). grp ( relationshipAttributeName ). list () } catch ( ex ) { // error handling } devicesInfo will result in all the devices that are in a many to one relationship with the device. Many to one relationships use group and one to one relationships and specific devices use object Interface methods \u00b6 list() - return all the devices with the blueprintId create( device ) - creates a device with the blueprintId get( id ) - return the device with the id delete( id ) - deletes the device with the id obj( id ) - returns an instance of object model() - returns the blueprint object methods \u00b6 get() - gets the device update( patchOps ) - updates the devices information delete() - deletes the device create( device ) - creates a device with a one to one relationships grp( relationship ) - returns and instance of a group obj( id ) - returns an instance of an object users() - results in methods to interact with the users model() - returns the blueprint history( options ) - makes a history query events() - returns a list method to get all the events for the device group methods \u00b6 get( id ) - gets the device list() - gets all the devices in the group create( device ) - creates a device in the group update( id, patchOps ) - updates the devices information delete( id ) - deletes the device obj( id ) - returns an instance of an object model() - returns the blueprint","title":"Interface"},{"location":"stable/api/interface/#query-interface","text":"Currently, the Interface methods require the systemId to work. This id can be found under the info tab of the system. In addition to the systemId , blueprintIds are required for Interface to work. let deviceInfo try { deviceInfo = await api . interface ( systemId , blueprintId ). obj ( deviceId ). get () } catch ( ex ) { // error handling } The systemId and blueprintId can be substituted with the blueprint alias and system alias. The above query will result in deviceInfo having all of the information for the device. In addition to this, let devicesInfo try { devicesInfo = await api . interface ( systemId , blueprintId ). obj ( '*' ). get () } catch ( ex ) { // error handling } will result in devicesInfo containing all the info for devices with blueprintId . In addition, interface queries can chain on relationships. let devicesInfo try { devicesInfo = await api . interface ( systemId , blueprint ). obj ( deviceId ). grp ( relationshipAttributeName ). list () } catch ( ex ) { // error handling } devicesInfo will result in all the devices that are in a many to one relationship with the device. Many to one relationships use group and one to one relationships and specific devices use object","title":"Query Interface"},{"location":"stable/api/interface/#interface-methods","text":"list() - return all the devices with the blueprintId create( device ) - creates a device with the blueprintId get( id ) - return the device with the id delete( id ) - deletes the device with the id obj( id ) - returns an instance of object model() - returns the blueprint","title":"Interface methods"},{"location":"stable/api/interface/#object-methods","text":"get() - gets the device update( patchOps ) - updates the devices information delete() - deletes the device create( device ) - creates a device with a one to one relationships grp( relationship ) - returns and instance of a group obj( id ) - returns an instance of an object users() - results in methods to interact with the users model() - returns the blueprint history( options ) - makes a history query events() - returns a list method to get all the events for the device","title":"object methods"},{"location":"stable/api/interface/#group-methods","text":"get( id ) - gets the device list() - gets all the devices in the group create( device ) - creates a device in the group update( id, patchOps ) - updates the devices information delete( id ) - deletes the device obj( id ) - returns an instance of an object model() - returns the blueprint","title":"group methods"},{"location":"stable/api/project/","text":"A project holds most of the pieces in the Leverege Platform, the project method of api can be used to interact with many of these pieces. Query Project \u00b6 Currently, the Project methods require the projectId to work. This id can be found under the settings tab. let project try { project = await api . project ( projectId ). get () } catch ( ex ) { // error handling } The project variable in the script will then have all of the project information. That information will include systems, users, blueprints and all other information associated with the Project. Other available methods are: update( newProjectInfo ) - patches the project with the newProjectInfo delete() - deletes the project users() - object to interact with the project users device() - has two methods, one for history and one for events members() - collection to interact with the project members accounts() - collection to interact with project accounts apiAccess() - collection to interact with apiAccess keys networks() - collection to interact with networks systems() - collection to interact with systems messageRoutes() - collection to interact with messageRoutes scenarios() - collection to interact with scenarios timers() - collection to interact with timers blueprints() - collection to interact with the blueprints events - lists out the events for the project verify( options ) getByUsername( username ) - gets a user by their username","title":"Project"},{"location":"stable/api/project/#query-project","text":"Currently, the Project methods require the projectId to work. This id can be found under the settings tab. let project try { project = await api . project ( projectId ). get () } catch ( ex ) { // error handling } The project variable in the script will then have all of the project information. That information will include systems, users, blueprints and all other information associated with the Project. Other available methods are: update( newProjectInfo ) - patches the project with the newProjectInfo delete() - deletes the project users() - object to interact with the project users device() - has two methods, one for history and one for events members() - collection to interact with the project members accounts() - collection to interact with project accounts apiAccess() - collection to interact with apiAccess keys networks() - collection to interact with networks systems() - collection to interact with systems messageRoutes() - collection to interact with messageRoutes scenarios() - collection to interact with scenarios timers() - collection to interact with timers blueprints() - collection to interact with the blueprints events - lists out the events for the project verify( options ) getByUsername( username ) - gets a user by their username","title":"Query Project"},{"location":"stable/api/scenario/","text":"Scenarios are simulations that can be run on devices to illustrate data. Scenario Methods \u00b6 get() - gets the scenario update( patchOps ) - patches the scenario delete() - deletes the scenario","title":"Scenario"},{"location":"stable/api/scenario/#scenario-methods","text":"get() - gets the scenario update( patchOps ) - patches the scenario delete() - deletes the scenario","title":"Scenario Methods"},{"location":"stable/api/script/","text":"Scripts are custom logic that can be run and triggered different ways on the Leverege platform Script Methods \u00b6 get() - gets the script update( patch ) - update the script delete() - deletes the script deploy() - deploys the script undeploy() - undeploys the script setCode( code ) - sets the code for the script logs() - gets the logs for the script metrics() - gets the metrics for the script","title":"Script"},{"location":"stable/api/script/#script-methods","text":"get() - gets the script update( patch ) - update the script delete() - deletes the script deploy() - deploys the script undeploy() - undeploys the script setCode( code ) - sets the code for the script logs() - gets the logs for the script metrics() - gets the metrics for the script","title":"Script Methods"},{"location":"stable/api/system/","text":"A System is a bucket for Devices. Systems have their own metadata, users and contacts. Systems are important organizational structures in the Leverege system. Query System \u00b6 Currently, the System methods require the systemId to work. This id can be found under the info tab of the system. let system try { system = await api . system ( systemId ). get () } catch ( ex ) { // error handling } The system variable in the script will then have all of the system information. That information will include Metadata, users, contacts and all other information associated with the system. Other methods are: update( newSystemInfo ) - patches the system with the newSystemInfo delete() - deletes the system users() - object to interact with the system users devices() - collection to interact with system devices contacts() - collection to interact with the system contacts interface() - refer to interface documentation userDevices() - collection of userDevices device() - has two methods, one for history and one for events verify( options ) Creating a System \u00b6 To create a new System, the project method of api needs to be used. const system = { ... newSystemInfo } try { api . project ( projectId ). systems (). create ( system ) } catch ( ex ) { // error handling }","title":"System"},{"location":"stable/api/system/#query-system","text":"Currently, the System methods require the systemId to work. This id can be found under the info tab of the system. let system try { system = await api . system ( systemId ). get () } catch ( ex ) { // error handling } The system variable in the script will then have all of the system information. That information will include Metadata, users, contacts and all other information associated with the system. Other methods are: update( newSystemInfo ) - patches the system with the newSystemInfo delete() - deletes the system users() - object to interact with the system users devices() - collection to interact with system devices contacts() - collection to interact with the system contacts interface() - refer to interface documentation userDevices() - collection of userDevices device() - has two methods, one for history and one for events verify( options )","title":"Query System"},{"location":"stable/api/system/#creating-a-system","text":"To create a new System, the project method of api needs to be used. const system = { ... newSystemInfo } try { api . project ( projectId ). systems (). create ( system ) } catch ( ex ) { // error handling }","title":"Creating a System"},{"location":"stable/api/template/","text":"Templates are for emails and sms messages. Template Methods \u00b6 get() - gets the template update( patchOps ) - patches the template delete() - deletes the template","title":"Template"},{"location":"stable/api/template/#template-methods","text":"get() - gets the template update( patchOps ) - patches the template delete() - deletes the template","title":"Template Methods"},{"location":"stable/api/timer/","text":"Timers trigger various logic. Timers can be set up to trigger once at a given time, at a repeating interval or on a schedule. Timer Methods \u00b6 get() - gets the timer update( patchOps ) - patches the timer delete() - deletes the timer start() - starts the timer stop() - stops the timer","title":"Timer"},{"location":"stable/api/timer/#timer-methods","text":"get() - gets the timer update( patchOps ) - patches the timer delete() - deletes the timer start() - starts the timer stop() - stops the timer","title":"Timer Methods"},{"location":"stable/api/user/","text":"The user portion of api is designed to interact with a specific user. User Methods \u00b6 get() - gets the user getByUserName() - used to the the user if created with a username update( patchOps ) - patches the user delete() - deletes the user systems() - gets the systems that the user is permissioned on devices() - gets the devices that the user is permissioned on changePassword( oldPassword, newPassword ) - changes the users password child( id ) - returns an instance of a child","title":"User"},{"location":"stable/api/user/#user-methods","text":"get() - gets the user getByUserName() - used to the the user if created with a username update( patchOps ) - patches the user delete() - deletes the user systems() - gets the systems that the user is permissioned on devices() - gets the devices that the user is permissioned on changePassword( oldPassword, newPassword ) - changes the users password child( id ) - returns an instance of a child","title":"User Methods"},{"location":"stable/api/users/","text":"Users are several methods to interact with all the users of a project Users Methods \u00b6 list() - gets all the users create( user ) - creates a user forgotPassword( username, projectId ) - sends a forgot password to the user resetPassword( patch ) - changes a user's password with a token child( id ) - returns an instance of a child Project, System and Device Users \u00b6 When interacting with the users method of a project, system or device, there are different methods available. These new methods revolve around adding, removing and changing the user's permissions on the project, system or device. list() - gets all the users specifically permissioned on the project, system or device create( user, permissions, metadata, options ) - creates and permissions a user on the project, system or device add( id, permissions, metadata ) - permissions an existing user on the project, system or device get( id, options ) - gets a user and their information that is permissioned on the project, system or device update( id, patchOps ) - updates the user remove( id ) and delete( id ) - removes the user's permissions on the project, system or device","title":"Users"},{"location":"stable/api/users/#users-methods","text":"list() - gets all the users create( user ) - creates a user forgotPassword( username, projectId ) - sends a forgot password to the user resetPassword( patch ) - changes a user's password with a token child( id ) - returns an instance of a child","title":"Users Methods"},{"location":"stable/api/users/#project-system-and-device-users","text":"When interacting with the users method of a project, system or device, there are different methods available. These new methods revolve around adding, removing and changing the user's permissions on the project, system or device. list() - gets all the users specifically permissioned on the project, system or device create( user, permissions, metadata, options ) - creates and permissions a user on the project, system or device add( id, permissions, metadata ) - permissions an existing user on the project, system or device get( id, options ) - gets a user and their information that is permissioned on the project, system or device update( id, patchOps ) - updates the user remove( id ) and delete( id ) - removes the user's permissions on the project, system or device","title":"Project, System and Device Users"},{"location":"stable/platform-overview/api-ui/","text":"The Leverege UI interacts intimately with the Leverege API. The Leverege UI is the recommended way to interact with the API to do project setup. For instructions on how to setup your project on the Leverege Platform, refer to the getting started guide. The following are guides for common actions taken with the Leverege UI. Run API UI locally \u00b6 npm run decrypt Change the IMAGINE_HOST_URL in your .env accordingly (local vs hosted API Server URL) npm start Reason Scripts \u00b6 To create a script, navigate to the scripts tab of the UI, create a script and add functionality to the script. Reasoner documentation will assist when writing functionality. Save the script, and deploy it. Next determine the trigger for the script. This trigger can be a timer, message route or a message on the reason topic. For creating a timer, read the scheduling timers section . Create this timer with the Trigger Script functionality, and copy the script id into the input. Input in JSON, the options that are needed, these will be passed into the script through the params key the context . Details for setting up a message route can be found in the Reason documentation . When set up this way, the context will contain deviceData, system, device and blueprint information. The third was is to send a message from inside the platform on the reason topic. This is the least supported way, and not recommended. Creating Users \u00b6 To create a user, navigate to the users tab, and click + new user . The required fields are username and password . Passwords need to be at least 6 characters. Once a user has been created, they can be permissioned on systems or devices. To permission a user on a system navigate to the system, then in the access tab, add the user and define their permissions. To permission a user on a device, use the api library, call api.device( deviceId ).users().add( userId, permissions, metadata ) or use Interface . To create a project user, the an api call is required. api.project( projectId ).users.create( user, permissions, metadata ) . Changing Device Attributes \u00b6 Under the Blueprints tab, all the current Blueprints are visible, click on the Blueprint name to open an attribute editor. Here attributes can be deleted and created, and can be minimally edited. when adding an attribute, a type and units can be designated. Deep attributes can be created by including / separating the keys. If a attribute is created with an incorrect type or unit, it has to be deleted and recreated. Creating a Simulation \u00b6 Simulations are created under the Scenarios tab. The Scenarios are all tied to a blueprint. The UI allows for values to be changed on an hour time line. Click on the timeline to create a specific value at that time. Clicking on the line between two value points opens an editor for the interpolation, which also allows the update frequency to be changed. When creating a device, it can be specified as simulated, or a device can have its simulator turned on in its info tab. When clicking into the device details, when a device is simulated, it has a play button in the upper right, clicking on this opens a selector for scenarios. Once the simulation is run, it will begin to change the value as designated in the scenario. Scheduling Timers \u00b6 Timers are created under the Timers tab of the UI. The Scheduler service manages all of the timers. Timers can have several different end functionalities. these functionalities are Trigger Script Query Url Publish Topic and specified through a dropdown. These functionalities have additional parameters that can be specified in the UI. The other main Attribute of Timers are when they are run. They can be specified to run once, at an interval, or a specific time everyday. This is changed in the Timer section of a timer.","title":"Api ui"},{"location":"stable/platform-overview/api-ui/#run-api-ui-locally","text":"npm run decrypt Change the IMAGINE_HOST_URL in your .env accordingly (local vs hosted API Server URL) npm start","title":"Run API UI locally"},{"location":"stable/platform-overview/api-ui/#reason-scripts","text":"To create a script, navigate to the scripts tab of the UI, create a script and add functionality to the script. Reasoner documentation will assist when writing functionality. Save the script, and deploy it. Next determine the trigger for the script. This trigger can be a timer, message route or a message on the reason topic. For creating a timer, read the scheduling timers section . Create this timer with the Trigger Script functionality, and copy the script id into the input. Input in JSON, the options that are needed, these will be passed into the script through the params key the context . Details for setting up a message route can be found in the Reason documentation . When set up this way, the context will contain deviceData, system, device and blueprint information. The third was is to send a message from inside the platform on the reason topic. This is the least supported way, and not recommended.","title":"Reason Scripts"},{"location":"stable/platform-overview/api-ui/#creating-users","text":"To create a user, navigate to the users tab, and click + new user . The required fields are username and password . Passwords need to be at least 6 characters. Once a user has been created, they can be permissioned on systems or devices. To permission a user on a system navigate to the system, then in the access tab, add the user and define their permissions. To permission a user on a device, use the api library, call api.device( deviceId ).users().add( userId, permissions, metadata ) or use Interface . To create a project user, the an api call is required. api.project( projectId ).users.create( user, permissions, metadata ) .","title":"Creating Users"},{"location":"stable/platform-overview/api-ui/#changing-device-attributes","text":"Under the Blueprints tab, all the current Blueprints are visible, click on the Blueprint name to open an attribute editor. Here attributes can be deleted and created, and can be minimally edited. when adding an attribute, a type and units can be designated. Deep attributes can be created by including / separating the keys. If a attribute is created with an incorrect type or unit, it has to be deleted and recreated.","title":"Changing Device Attributes"},{"location":"stable/platform-overview/api-ui/#creating-a-simulation","text":"Simulations are created under the Scenarios tab. The Scenarios are all tied to a blueprint. The UI allows for values to be changed on an hour time line. Click on the timeline to create a specific value at that time. Clicking on the line between two value points opens an editor for the interpolation, which also allows the update frequency to be changed. When creating a device, it can be specified as simulated, or a device can have its simulator turned on in its info tab. When clicking into the device details, when a device is simulated, it has a play button in the upper right, clicking on this opens a selector for scenarios. Once the simulation is run, it will begin to change the value as designated in the scenario.","title":"Creating a Simulation"},{"location":"stable/platform-overview/api-ui/#scheduling-timers","text":"Timers are created under the Timers tab of the UI. The Scheduler service manages all of the timers. Timers can have several different end functionalities. these functionalities are Trigger Script Query Url Publish Topic and specified through a dropdown. These functionalities have additional parameters that can be specified in the UI. The other main Attribute of Timers are when they are run. They can be specified to run once, at an interval, or a specific time everyday. This is changed in the Timer section of a timer.","title":"Scheduling Timers"},{"location":"stable/platform-overview/architecture/","text":"The Leverege Platform provides a comprehensive web-based user interface that allows developers and system integrators to define data models, data processing pipelines, and customizable dashboards powered by real device data or simulated dataflows. The diagram below shows the high-level view of Leverege Platform. The Leverege Platform is built using a set of micro-services (services) that communicate via a message queueing system (like Google's PubSub or NSQ ) or, in some cases HTTP connections. Each service is a Docker image that is executed using Kubernetes, allowing them to be scaled up or down as needed. The queueing system works on the principle of a topic and a channel. When multiple readers of a queue have the same topic and channel, only one of them is notified with the message. This allows a service to scale horizontally when the number of messages increase. If two readers listen for messages on the same topic but with different channels, they will both receive the message. The flow of data from a source device generally follows the following steps: Data from source devices enter the system through either custom ingestion server or Rest Server. The ingestion server is a custom business logic server that knows how to talk to the data source. This includes managing the transport (HTTP, TCP, UDP, PubSub, etc) for the source and knowing how to parse the incoming data. The ingestion server will then publish an InboundDataEventMsg to the default-processor message topic. At this point the inbound msg only has a reference to the external name of the device: the network id, alias key, and value. The Message Processor will receive this InboundDataEventMsg and attempt to resolve the external name (network id, alias key, and value) to an internal Device object. If the internal Device cannot be found, the message will be discarded. If the Device is found, the Message Processor will look up the routing steps for the message and send a DeviceDataEventMsg to the defined routes. By default, the message will be sent to the writer topic. If special business logic is needed, the message route for a Blueprint (or Device) can be configure to send the message to other topics such as a Reason script or custom topic. By default the message is sent to the writer topic, which will be received by the Transponder service. Transponder will write the data to the current and historical databases. If the message was also routed to a Reason script, that script may send out emails or SMS messages, or may schedule some action to occur at a later time. Services \u00b6 API Server - Rest server that encompasses the core platform functionality Message Processor - Routes inbound messages Transponder - Writes data to databases Reason - Enables FaaS capabilities Scheduler - Creates, runs and maintains timers Messenger - Sends SMS messages Emailer - Sends emails Simulator - Simulates data being sent into the platform Rest-server - Generic ingestion service using HTTP DB-curator - Cleans old/stale data from the Databases","title":"Architecture"},{"location":"stable/platform-overview/architecture/#services","text":"API Server - Rest server that encompasses the core platform functionality Message Processor - Routes inbound messages Transponder - Writes data to databases Reason - Enables FaaS capabilities Scheduler - Creates, runs and maintains timers Messenger - Sends SMS messages Emailer - Sends emails Simulator - Simulates data being sent into the platform Rest-server - Generic ingestion service using HTTP DB-curator - Cleans old/stale data from the Databases","title":"Services"},{"location":"stable/platform-overview/concepts/","text":"This describes the various objects used in the Leverege Platform. All of these can be accessed and manipulated using the api library. Project \u00b6 Projects hold most of the other pieces in the platform, such as blueprints, systems, message routes, and scripts. A Project has have Project specific Users (accounts), that are used to assign access to Systems and Devices. Blueprint \u00b6 A Blueprint is used to define both IoT device data, and organizational data and relationships. Blueprints can have an alias that is useful when accessing data via the Model Interface API. The alias defines a human readable unique name for the model. Each Blueprint has a set of attributes. Messages for a Device can be routed based on its Blueprint type, allowing different handling of different data types. Attribute \u00b6 An Attribute defines a field in a Blueprint. There are several types of Attributes such as string, numbers, units, timestamps and relationships. The Attributes can be used to dynamically interpret data in a Device. Device \u00b6 A Device represents a thing, either an IoT device or an organizational construct. Each Device has a Blueprint that defines what kind of thing it is. Internally, a Device has an platform generated id. The Device can have multiple source specific ids as well. It is captured through the concept of a network alias. The network alias consists of a Network Id, a alias key, and a value. An example of this triplet might be ('boat-network', 'esn', '5551212' ). This triplet allows the platform to map the external id to the internal id. Project Users can also be assigned permissions on a Device. System \u00b6 A System holds a collection of Devices. Project Users can be given access rights to a System, allowing different users to access different collections of Devices. Network \u00b6 A Network represents a source of device data. When a device needs to be contacted, the Network is used as the target. In particular, messages are published to the <networkId>-outbound topic. The ingestion service setup to manage that network's data can listen on this topic and then communicate appropriately to the source server. The Network Id is also part external identity of a Device. Message Route \u00b6 A Message Route defines how a particular Message type ( deviceDataEventMsg for example ) is acted upon. This is used to select which reason scripts will run when a new data for a Device is received. Reason Script \u00b6 A Reason Script is code that can be run in response to new Message or a Timer. These Scripts are used to supply business logic to your project. Actions like Sending out a daily status email or calculating alert states and sending notifications when they occur would fall under the domain of a Reason Script. These Scripts can also interface with external systems, as needed. Reason Trigger \u00b6 The Reason Triggers are like Scripts, but without having to write code. For many simple tasks such as sending alerts when a device is in a certain state for a certain amount of time, triggers will be easier to setup than a Script. Timer \u00b6 A Timer represents a task that needs to be trigger in a certain amount of time, or repeatedly. These are often used to tell a Reason Script to run periodically. Template \u00b6 Templates are used to create branded emails or SMS notifications. When Reason triggers a send email request, a Template Id and contextual data can be supplied to produce the branded email. Scenario \u00b6 A Scenario defines how data values for a particular Blueprint change over time. It uses a key framing system to allow the user to set values at given points in time. The intermediate values are computed by a function that exists between two key frame values. A Scenario can be played on a Simulated device. User \u00b6 There are two types of users in the Leverege Platform: * Platform User * Project User Platform Users are users that can create and edit a Project and all of the supporting pieces such as Blueprints, Networks, Scripts. Project Users are end users of the resulting product. They have access to Systems and Devices. Project User names are unique to a Project, so two different Projects can have a user with name 'demo'. Api Access \u00b6 The Api Access key is used to allow a server to communicate with the Api Server as an admin of the project without having to login. This key is used with the api-service library.","title":"Core Concepts"},{"location":"stable/platform-overview/concepts/#project","text":"Projects hold most of the other pieces in the platform, such as blueprints, systems, message routes, and scripts. A Project has have Project specific Users (accounts), that are used to assign access to Systems and Devices.","title":"Project"},{"location":"stable/platform-overview/concepts/#blueprint","text":"A Blueprint is used to define both IoT device data, and organizational data and relationships. Blueprints can have an alias that is useful when accessing data via the Model Interface API. The alias defines a human readable unique name for the model. Each Blueprint has a set of attributes. Messages for a Device can be routed based on its Blueprint type, allowing different handling of different data types.","title":"Blueprint"},{"location":"stable/platform-overview/concepts/#attribute","text":"An Attribute defines a field in a Blueprint. There are several types of Attributes such as string, numbers, units, timestamps and relationships. The Attributes can be used to dynamically interpret data in a Device.","title":"Attribute"},{"location":"stable/platform-overview/concepts/#device","text":"A Device represents a thing, either an IoT device or an organizational construct. Each Device has a Blueprint that defines what kind of thing it is. Internally, a Device has an platform generated id. The Device can have multiple source specific ids as well. It is captured through the concept of a network alias. The network alias consists of a Network Id, a alias key, and a value. An example of this triplet might be ('boat-network', 'esn', '5551212' ). This triplet allows the platform to map the external id to the internal id. Project Users can also be assigned permissions on a Device.","title":"Device"},{"location":"stable/platform-overview/concepts/#system","text":"A System holds a collection of Devices. Project Users can be given access rights to a System, allowing different users to access different collections of Devices.","title":"System"},{"location":"stable/platform-overview/concepts/#network","text":"A Network represents a source of device data. When a device needs to be contacted, the Network is used as the target. In particular, messages are published to the <networkId>-outbound topic. The ingestion service setup to manage that network's data can listen on this topic and then communicate appropriately to the source server. The Network Id is also part external identity of a Device.","title":"Network"},{"location":"stable/platform-overview/concepts/#message-route","text":"A Message Route defines how a particular Message type ( deviceDataEventMsg for example ) is acted upon. This is used to select which reason scripts will run when a new data for a Device is received.","title":"Message Route"},{"location":"stable/platform-overview/concepts/#reason-script","text":"A Reason Script is code that can be run in response to new Message or a Timer. These Scripts are used to supply business logic to your project. Actions like Sending out a daily status email or calculating alert states and sending notifications when they occur would fall under the domain of a Reason Script. These Scripts can also interface with external systems, as needed.","title":"Reason Script"},{"location":"stable/platform-overview/concepts/#reason-trigger","text":"The Reason Triggers are like Scripts, but without having to write code. For many simple tasks such as sending alerts when a device is in a certain state for a certain amount of time, triggers will be easier to setup than a Script.","title":"Reason Trigger"},{"location":"stable/platform-overview/concepts/#timer","text":"A Timer represents a task that needs to be trigger in a certain amount of time, or repeatedly. These are often used to tell a Reason Script to run periodically.","title":"Timer"},{"location":"stable/platform-overview/concepts/#template","text":"Templates are used to create branded emails or SMS notifications. When Reason triggers a send email request, a Template Id and contextual data can be supplied to produce the branded email.","title":"Template"},{"location":"stable/platform-overview/concepts/#scenario","text":"A Scenario defines how data values for a particular Blueprint change over time. It uses a key framing system to allow the user to set values at given points in time. The intermediate values are computed by a function that exists between two key frame values. A Scenario can be played on a Simulated device.","title":"Scenario"},{"location":"stable/platform-overview/concepts/#user","text":"There are two types of users in the Leverege Platform: * Platform User * Project User Platform Users are users that can create and edit a Project and all of the supporting pieces such as Blueprints, Networks, Scripts. Project Users are end users of the resulting product. They have access to Systems and Devices. Project User names are unique to a Project, so two different Projects can have a user with name 'demo'.","title":"User"},{"location":"stable/platform-overview/concepts/#api-access","text":"The Api Access key is used to allow a server to communicate with the Api Server as an admin of the project without having to login. This key is used with the api-service library.","title":"Api Access"},{"location":"stable/platform-overview/deployment/","text":"The Leverege Platform is a collection of services that implements various IoT-related tasks. Each services is available via a Docker image on Google Container Registry and can be deployed via Docker, Docker Swarm, or Kubernetes. The reference architecture is based on Google Kubernetes Engine (GKE), but the Leverege Platform can be deployed onto other cloud providers by configuring it with equivalent services (e.g. NSQ in lieu of Google Pub/Sub for message queue). At the time of writing, the supported services are as follows: * Message Queue: NSQ, Pub/Sub * Database: MySQL, Postgres, TimeseriesDB, Firebase/FireStore, BigQuery * Cache: Redis, Memcache * Search: ElasticSearch * Notification: Twilio, Mailgun * Serverless: Cloud Functions, Cloud Run (Knative), OpenFaaS * Monitoring: Prometheus, StackDriver Deploying \u00b6 The following deployment guide is optimized for Kubernetes via Helm. Standard Kubernetes manifests are also supported. If custom components must be added, new Deployments/DaemonSets can be added via Helm/Kubernetes or translate docker-compose files via Kompose . Getting Started \u00b6 Baseline GCP setup can also be configured via Terraform, but if modifications are necessary, following the setup guide below. Required Setup \u00b6 GCP Project with Billing Enabled: Kubernetes Instance (GKE or GCE with Kubernetes installed) SQL (Cloud SQL or self-hosted MySQL, Postgres, or TimeseriesDB) Access to Pub/Sub, Container Registry, GCS, Firebase On Firebase: Link Firebase project to GCP project Make sure to enable backups: Database \u2192 Backups Configure access based on security needs Third-Party Services : SMS: Twilio Email: Mailgun gcloud kubectl Helm DNS Provider and SSL certificate for HTTPS support (e.g. Cloudflare) Installing \u00b6 Install ChartMuseum and obtain a service account to access Helm charts for Leverege Platform Docker images. Clone platform-k8s and fill out overwhelm.yaml with project specific sections (e.g. host name, projectId, etc). In the project directory run: npm install Deployment \u00b6 Obtain Service Accounts and Configs from all the Firebase databases: Go to the Firebase console Click on the desired project Click on the settings icon in the top-left corner of the page Click on \"Project Settings\" in the subsequent dropdown Click on the tab for \"service accounts\" Ensure that the tab for \"Firebase Admin SDK\" is selected Click the button \"GENERATE NEW PRIVATE KEY\" (project owner/admin permission level required) Click \"GENERATE KEY\" Go back to the top and click the tab \"GENERAL\" Click the button \"Add Firebase to your web app\" Copy out the config object into a local file Convert the config object into actual JSON (surround the object keys with \") Repeat for all Firebase databases Move Service Account and Config files into scripts/platform-secrets inside this folder Rename RT database service account -> rt-data Rename RT database config -> rt-data-config Rename Service database service account -> srv-data Rename Service database config -> srv-data-config Rename Model database service account -> model-data Rename Model database config -> model-data-config Obtain a Service Account for CloudSQL and BigQuery: Get access to leverege-docker-images Container Registry Go to your project \u2192 IAM \u2192 Service Accounts and copy the default Compute Engine Service Account email address (e.g. <123123-compute>@developer.gserviceaccount.com) Go to leverege-docker-images \u2192 IAM \u2192 Add \u2192 Paste the above and give it Storage Object Viewer Role Go to the GCP console Switch projects to the project you are using for deployment In the sidebar go to the IAM & Admin section Click on the service accounts option in the sidebar Create three service accounts (one for BQ, one for CloudSQL, one for Pub/Sub) CloudSQL: SQL Client Pub/Sub: Pub/Sub Editor BigQuery: Data Editor & Job User Create and download a key for each account in JSON Move Service Account credentials into a secure location to load onto Kubernetes as secrets. Obtain Twilio and Mailgun API keys Create a user for CloudSQL Obtain SSL Certificate Get credentials for the GKE cluster - GKE Documentation $ gcloud config set compute/zone <zone-name> $ gcloud config set project <project-name> $ gcloud container clusters get-credentials <cluster-name> Initialize Helm with RBAC First make tiller.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \"\" $ kubectl create -f tiller.yaml $ helm init --service-account tiller --upgrade Run Deploy_Platform script under platform-k8s/scripts and follow the prompt Deploy Platform UI via Firebase Hosting or GCS.","title":"Platform Setup"},{"location":"stable/platform-overview/deployment/#deploying","text":"The following deployment guide is optimized for Kubernetes via Helm. Standard Kubernetes manifests are also supported. If custom components must be added, new Deployments/DaemonSets can be added via Helm/Kubernetes or translate docker-compose files via Kompose .","title":"Deploying"},{"location":"stable/platform-overview/deployment/#getting-started","text":"Baseline GCP setup can also be configured via Terraform, but if modifications are necessary, following the setup guide below.","title":"Getting Started"},{"location":"stable/platform-overview/deployment/#required-setup","text":"GCP Project with Billing Enabled: Kubernetes Instance (GKE or GCE with Kubernetes installed) SQL (Cloud SQL or self-hosted MySQL, Postgres, or TimeseriesDB) Access to Pub/Sub, Container Registry, GCS, Firebase On Firebase: Link Firebase project to GCP project Make sure to enable backups: Database \u2192 Backups Configure access based on security needs Third-Party Services : SMS: Twilio Email: Mailgun gcloud kubectl Helm DNS Provider and SSL certificate for HTTPS support (e.g. Cloudflare)","title":"Required Setup"},{"location":"stable/platform-overview/deployment/#installing","text":"Install ChartMuseum and obtain a service account to access Helm charts for Leverege Platform Docker images. Clone platform-k8s and fill out overwhelm.yaml with project specific sections (e.g. host name, projectId, etc). In the project directory run: npm install","title":"Installing"},{"location":"stable/platform-overview/deployment/#deployment","text":"Obtain Service Accounts and Configs from all the Firebase databases: Go to the Firebase console Click on the desired project Click on the settings icon in the top-left corner of the page Click on \"Project Settings\" in the subsequent dropdown Click on the tab for \"service accounts\" Ensure that the tab for \"Firebase Admin SDK\" is selected Click the button \"GENERATE NEW PRIVATE KEY\" (project owner/admin permission level required) Click \"GENERATE KEY\" Go back to the top and click the tab \"GENERAL\" Click the button \"Add Firebase to your web app\" Copy out the config object into a local file Convert the config object into actual JSON (surround the object keys with \") Repeat for all Firebase databases Move Service Account and Config files into scripts/platform-secrets inside this folder Rename RT database service account -> rt-data Rename RT database config -> rt-data-config Rename Service database service account -> srv-data Rename Service database config -> srv-data-config Rename Model database service account -> model-data Rename Model database config -> model-data-config Obtain a Service Account for CloudSQL and BigQuery: Get access to leverege-docker-images Container Registry Go to your project \u2192 IAM \u2192 Service Accounts and copy the default Compute Engine Service Account email address (e.g. <123123-compute>@developer.gserviceaccount.com) Go to leverege-docker-images \u2192 IAM \u2192 Add \u2192 Paste the above and give it Storage Object Viewer Role Go to the GCP console Switch projects to the project you are using for deployment In the sidebar go to the IAM & Admin section Click on the service accounts option in the sidebar Create three service accounts (one for BQ, one for CloudSQL, one for Pub/Sub) CloudSQL: SQL Client Pub/Sub: Pub/Sub Editor BigQuery: Data Editor & Job User Create and download a key for each account in JSON Move Service Account credentials into a secure location to load onto Kubernetes as secrets. Obtain Twilio and Mailgun API keys Create a user for CloudSQL Obtain SSL Certificate Get credentials for the GKE cluster - GKE Documentation $ gcloud config set compute/zone <zone-name> $ gcloud config set project <project-name> $ gcloud container clusters get-credentials <cluster-name> Initialize Helm with RBAC First make tiller.yaml apiVersion: v1 kind: ServiceAccount metadata: name: tiller namespace: kube-system --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata: name: tiller-clusterrolebinding subjects: - kind: ServiceAccount name: tiller namespace: kube-system roleRef: kind: ClusterRole name: cluster-admin apiGroup: \"\" $ kubectl create -f tiller.yaml $ helm init --service-account tiller --upgrade Run Deploy_Platform script under platform-k8s/scripts and follow the prompt Deploy Platform UI via Firebase Hosting or GCS.","title":"Deployment"},{"location":"stable/platform-overview/getting-started/","text":"Leverege Platform organizes resource hierarchy by projects, systems, and devices . Each instance of the Leverege Platform can support one or more projects, which can each hold multiple systems and devices. In other words, multi-tenancy is supported at each resource level. IAM roles can be granted on each level of the resource hierarchy. An admin user can create projects, systems, devices, and other users. A member of a project is restricted access to specified projects, but have read/write access to all resources defined at the project level (e.g. systems, metadata, users). Finally, accounts are users with read/write access defined at the system and device level. If an account has admin access, it can create other accounts and control permissions for allowed systems. Accounts without admin access can interact with devices or device relationships. Getting Started \u00b6 Create a new project Define the Blueprint (digital representation) for each device with attribute fields and simulation items (if necessary) Create organizational Blueprints to represent organization relationships if necessary Organizational devices can be powerful ways to grant permissions to users. In addition to this, queries can be constructed on relationships. I.E. A company can have divisions, which can have employees. This way, A CEO or president can have access to all the devices in their company, and each division leader can have access to all devices in their division. There also enables a quick way to query for all the employees in one division Create a System. A System is a collection of one or more devices. Provide a system alias for a human readable label for systems Create user accounts as needed Users can be added to the system through the Access tab of the system To add users to specific devices, use the API Ingest Data \u00b6 In order to ingest data onto the Leverege Platform, data must be sent in the inboundDataEventMsg format via a REST endpoint or by custom servers. In order to create a custom ingestion server: Create a copy of the server template in dev-utils Define necessary ingestion, parsing, and authentication protocols Define the api and imagine fields within Config.js to point the server to the correct project/system. Deployment \u00b6 Ingestion servers can be deployed as a Docker container or a serverless function. To deploy as a Docker image, run the dockreate script from build-tools to create an image on GCR. Now the server can be deployed on a VM with Docker support or on Kubernetes via Kubernetes manifests or Helm. To deploy as a function, host the server on any managed serverless services (e.g. Google Cloud Functions, AWS Lambda) or if the project supports OpenFaaS, use the CLI to deploy to OpenFaaS . For long running jobs, Kubernetes deployment is recommended. Creating the UI \u00b6 There are several front-end libraries and templates to assist and expedite creating a UI. Libraries \u00b6 The combination of UI Builder and UI-elements allows for quickly styling and re-styling the UI. UI-elements has many react-widgets to help quickly make new UIs. api and api-redux are libraries enabling quick ajax calls to the Leverege API. There are also several other different libraries to help make UIs.","title":"Building a Project"},{"location":"stable/platform-overview/getting-started/#getting-started","text":"Create a new project Define the Blueprint (digital representation) for each device with attribute fields and simulation items (if necessary) Create organizational Blueprints to represent organization relationships if necessary Organizational devices can be powerful ways to grant permissions to users. In addition to this, queries can be constructed on relationships. I.E. A company can have divisions, which can have employees. This way, A CEO or president can have access to all the devices in their company, and each division leader can have access to all devices in their division. There also enables a quick way to query for all the employees in one division Create a System. A System is a collection of one or more devices. Provide a system alias for a human readable label for systems Create user accounts as needed Users can be added to the system through the Access tab of the system To add users to specific devices, use the API","title":"Getting Started"},{"location":"stable/platform-overview/getting-started/#ingest-data","text":"In order to ingest data onto the Leverege Platform, data must be sent in the inboundDataEventMsg format via a REST endpoint or by custom servers. In order to create a custom ingestion server: Create a copy of the server template in dev-utils Define necessary ingestion, parsing, and authentication protocols Define the api and imagine fields within Config.js to point the server to the correct project/system.","title":"Ingest Data"},{"location":"stable/platform-overview/getting-started/#deployment","text":"Ingestion servers can be deployed as a Docker container or a serverless function. To deploy as a Docker image, run the dockreate script from build-tools to create an image on GCR. Now the server can be deployed on a VM with Docker support or on Kubernetes via Kubernetes manifests or Helm. To deploy as a function, host the server on any managed serverless services (e.g. Google Cloud Functions, AWS Lambda) or if the project supports OpenFaaS, use the CLI to deploy to OpenFaaS . For long running jobs, Kubernetes deployment is recommended.","title":"Deployment"},{"location":"stable/platform-overview/getting-started/#creating-the-ui","text":"There are several front-end libraries and templates to assist and expedite creating a UI.","title":"Creating the UI"},{"location":"stable/platform-overview/getting-started/#libraries","text":"The combination of UI Builder and UI-elements allows for quickly styling and re-styling the UI. UI-elements has many react-widgets to help quickly make new UIs. api and api-redux are libraries enabling quick ajax calls to the Leverege API. There are also several other different libraries to help make UIs.","title":"Libraries"},{"location":"stable/platform-overview/overview/","text":"The Leverege Platform is an integrated suite of cloud-based tools that provides a complete and easy-to-use IoT application framework for System Integrators and customers; greatly accelerating time to market and driving down implementation costs. It employs the latest advances in predictive data analytics, machine learning, containerized software design, cloud computing, and scalable pub/sub architectures. Imagine \u00b6 Imagine is the component of the Leverege Platform that models and simulates data streams and communications links for rapid prototyping, continuous integration and testing, and use case refinement. Imagine begins the process of virtualizing any IoT application by defining the data model, interactions, and establishing simulated dataflows as necessary. At a high-level, Imagine encompasses three major components: Blueprint/Data Modeling Connectivity/Network Management Simulation Engine The Simulation provides tremendous value in expediting the product development cycle by enabling parallel development of hardware, communications, and software. Data can be simulated upfront to stress test and deliver UI prototypes while the hardware and communication layers are finalized and certified. It is also a key element to enabling continuous development and delivery of new system features. Reason \u00b6 Reason ingests, provisions, stores, and analyzes data plus allows custom business logic to be written and uploaded. High velocity publish/subscribe (pub/sub) message queues quickly capture data from various sources (both simulated and real) and make that data available on a message bus to other logical components. Reason\u2019s internal message processor then routes each message according to its type. For example, telemetry data will update the real-time database for state management and be stored in a historical, time-series database for batch and stream analytic jobs. Users can provision their own algorithms known as reasoners that contain custom logic for alerting, machine learning, or analytics applications. The serverless nature of these reasoners allows each codebase to scale with the use case. Reason is the big data, message processing, and data analytics \u201cbrain\u201d of the Leverege Platform. Reason supports stream analytics, batch processing, custom business logic, file storage, and device state management. It is also responsible for device provisioning and supports integrations with other critical business systems such as CRM, retail management, and inventory systems. Reason has been built with the following goals in mind: * Complete data integrity with zero data loss * Real-time and batch process support * Autoscaling based on need * Support for client-specific business implementations * Generation of data-driven insights for customers * Maximum data portability * Simple 3 rd party data and tools integration Vision \u00b6 No IoT solution is complete without a polished user interface. Vision equips users with pre-made templates, customizable UI components, and apps that can be readily tailored to each IoT solution. End users can modify an existing app, customize a template, or start from scratch by defining a theme. Vision\u2019s built-in UI builder provides fine-grain level of customization at the UI component level to support any use case or application requirement.","title":"Overview"},{"location":"stable/platform-overview/overview/#imagine","text":"Imagine is the component of the Leverege Platform that models and simulates data streams and communications links for rapid prototyping, continuous integration and testing, and use case refinement. Imagine begins the process of virtualizing any IoT application by defining the data model, interactions, and establishing simulated dataflows as necessary. At a high-level, Imagine encompasses three major components: Blueprint/Data Modeling Connectivity/Network Management Simulation Engine The Simulation provides tremendous value in expediting the product development cycle by enabling parallel development of hardware, communications, and software. Data can be simulated upfront to stress test and deliver UI prototypes while the hardware and communication layers are finalized and certified. It is also a key element to enabling continuous development and delivery of new system features.","title":"Imagine"},{"location":"stable/platform-overview/overview/#reason","text":"Reason ingests, provisions, stores, and analyzes data plus allows custom business logic to be written and uploaded. High velocity publish/subscribe (pub/sub) message queues quickly capture data from various sources (both simulated and real) and make that data available on a message bus to other logical components. Reason\u2019s internal message processor then routes each message according to its type. For example, telemetry data will update the real-time database for state management and be stored in a historical, time-series database for batch and stream analytic jobs. Users can provision their own algorithms known as reasoners that contain custom logic for alerting, machine learning, or analytics applications. The serverless nature of these reasoners allows each codebase to scale with the use case. Reason is the big data, message processing, and data analytics \u201cbrain\u201d of the Leverege Platform. Reason supports stream analytics, batch processing, custom business logic, file storage, and device state management. It is also responsible for device provisioning and supports integrations with other critical business systems such as CRM, retail management, and inventory systems. Reason has been built with the following goals in mind: * Complete data integrity with zero data loss * Real-time and batch process support * Autoscaling based on need * Support for client-specific business implementations * Generation of data-driven insights for customers * Maximum data portability * Simple 3 rd party data and tools integration","title":"Reason"},{"location":"stable/platform-overview/overview/#vision","text":"No IoT solution is complete without a polished user interface. Vision equips users with pre-made templates, customizable UI components, and apps that can be readily tailored to each IoT solution. End users can modify an existing app, customize a template, or start from scratch by defining a theme. Vision\u2019s built-in UI builder provides fine-grain level of customization at the UI component level to support any use case or application requirement.","title":"Vision"},{"location":"ui/api-redux/","text":"API redux is a front end library that uses redux thunk to make api calls and store their result in redux. It supports Auth, Interface, event, history and user calls. If set up correctly, it makes api calls immediately accessible anywhere dispatch is accessible. Installation \u00b6 npm install --save @leverege/api-redux Reducer Setup \u00b6 Import the reducers that you are going to use: * InterfaceReducer - reducer that will handle imagine interface queries * AuthReducer - reducer that handles authentication * HistoryReducer - reducer that handles queries for getting and updating imagine history * EventReducer - reducer that handles queries for getting and updating imagine events * UsersReducer - reducer that handles queries for getting and updating imagine Users import { combineReducers } from 'redux' import { InterfaceReducer, AuthReducer, HistoryReducer, EventReducer, UsersReducer } from '@leverege/api-redux' ... const reducers = combineReducers( { ... interface : InterfaceReducer auth : AuthReducer history : HistoryReducer event : EventReducer users : UsersReducer } ) export default reducers Incorporate api-redux into your react component \u00b6 import { Interface } from 'leverege/api-redux' class MyScreen extends React.Component { constructor( props ) { super( props ) const { dispatch } = props this.items = Interface.create( system, 'myType' ).getList( interface ) dispatch( items.list() ) } render() { const { model } = this.props const items = this.items.getList( model ) return ( <div> {items.map( item => item.name )} </div> ) } } export default connect( state => ( { model : state.interface } ) )( MyScreen ); Auth \u00b6 Auth has several methods all relating to authenticating the user and managing their password ( these need to be dispatched ) 1) verify( [ targetLocation ] ) - gets and verifies the bearer token 2) login( username, password, [ projectId ] ) - logs in the user 3) logout() - logs out the user 4) forgotPassword( username, projectId ) - sends an email to the user's email address with instructions to reset their password 5) changePassword( oldPassword, newPassword ) - changes the current logged in user's password 6) resetPassword( username, password, confirm, token, projectId ) - uses an api generated token to allow a user to change their password In addition to several methods to check the current state 1) isLoggedIn( state ) - returns a boolean representing if there is a user logged in 2) isLoading( state ) - returns a boolean representing if a Auth query is loading 3) isVerifying( state ) - returns a boolean representing if a verify query is loading 4) isError( state ) - returns a boolean representing if there was an Auth error 5) getErrorMessage( state ) - returns the last error message 6) getUsername( state ) - returns the current user's username 7) getUserId( state ) - returns the current user's imagine userId 8) getProfile( state ) - returns the current user's profile Users \u00b6 Users has several methods all relating to maintaining and giving permissions to users ( these need to be dispatched; almost all the methods are for admins, but some allow for the user to make changes to themselves ) 1) list( opts ) - gets all the current users for the project 2) create( user ) - creates a new project user 3) get( userId, opts ) - gets a user 4) remove( userId ) - removes a user 5) update( userId, values, opts ) - updates the user with the values 6) forcePWReset( userId, projectId ) - sends an email to the user to reset their password Interface \u00b6 Interface is used to interact with devices and groups on the system and corresponds to the Interface queries. it is possible to chain together objects and groups to get a desired device. create \u00b6 create( systemId, blueprintAlias ) - creates an Interface object with several methods 1) list( options ) - makes an api call to get all the devices in the system with the blueprintAlias 2) create( device, options ) - makes an api call to create a device 3) delete( id, options ) - makes an api call to delete a device 4) model( options ) - makes an api call to get the model 1) obj( id ) - creates an instance of the Obj class with the id 2) getList( state, opts ) - gets the list from the state 3) getModel( state, opts ) - gets the model from the state 4) isLoading( state, opts ) - returns a boolean based on if there is a query being made 5) isError( state ) - returns a boolean based on if there was an error for a query 6) isDone( state ) - returns a boolean based on if a query is finished Object \u00b6 Obj( parentPath, deviceId, attribute ) - creates an instance of the Obj class - created by Interface.create( 'mySystem', 'myBP' ).obj( 'myDeviceId' ) which will correctly manage the path for you This class is used to interact with the api for a specific device 1) get( options ) - makes an api call to get the device 2) history( query, options ) - makes an api call to get the history for the device. query is an object with a queryName and query object, refer to history documentation for how to structure the query 4) create( device, options ) - creates a child device 5) update( obj, options ) - updates the current device 6) delete( options ) - deletes the current device 7) model( options ) - gets the model for the current device 8) obj( id ) - new instance of the Obj class pointed at the new device 9) grp( attribute ) - new instance of the Grp class, attribute should be a relationship attribute 10) users() - gets all the users permissioned on the device 11) getObject( state ) - gets the device from the state after an api call has been made 12) getTable( state, opts ) - gets the table for the device from the state after an api call 13) getModel( state, opts ) - gets the model from the state after an api call 14) getHistory( state, queryName ) - gets the history query result corresponding to the queryName 15) isTable( state ) - boolean on whether the api result is a table 16) isLoading( state ) - boolean on whether an api call is being made 17) isError( state ) - boolean on whether an api call returned an error 18) isDone( state ) - boolean on whether an api call is done Group \u00b6 Grp( parentPath, deviceId, attribute ) - creates an instance of the Grp class - created by Interface.create( 'mySystem', 'myBP' ).obj( 'myDeviceId' ).grp( 'myChildDevices' ) which will correctly manage the path for you A group represents a group of devices often by their relationship to a parent device 1) list( options ) - api call to get the list of the group of devices 2) create( obj, options ) - api call to create a new device for the group 3) get( id, options ) - api call to get a specific device from the group 4) update( id, data, options ) - api call to update a specific device from the group 5) delete( id, options ) - deletes a device 6) model( options ) - gets the model for the devices 7) obj( id ) - Obj class Instance 8) getList( state, opts ) - gets the list from the state 9) isTable( state, opts ) - returns a boolean based on if the api call result is a table 10) getObject( state, id, opts ) - returns a device from the state 11) getModel( state ) - returns the model from the state 12) isLoading( state ) - returns if an api call is loading 13) isError( state ) - return if an api call made an error 14) isDone( state ) - returns if an api call is finished Filters \u00b6 You can install custom filter functions by calling Interface.setFilter( 'myFilterType', ( filter, items ) => { return items.filter( item => item.whatever == filter.myOption ) } )","title":"API Redux"},{"location":"ui/api-redux/#installation","text":"npm install --save @leverege/api-redux","title":"Installation"},{"location":"ui/api-redux/#reducer-setup","text":"Import the reducers that you are going to use: * InterfaceReducer - reducer that will handle imagine interface queries * AuthReducer - reducer that handles authentication * HistoryReducer - reducer that handles queries for getting and updating imagine history * EventReducer - reducer that handles queries for getting and updating imagine events * UsersReducer - reducer that handles queries for getting and updating imagine Users import { combineReducers } from 'redux' import { InterfaceReducer, AuthReducer, HistoryReducer, EventReducer, UsersReducer } from '@leverege/api-redux' ... const reducers = combineReducers( { ... interface : InterfaceReducer auth : AuthReducer history : HistoryReducer event : EventReducer users : UsersReducer } ) export default reducers","title":"Reducer Setup"},{"location":"ui/api-redux/#incorporate-api-redux-into-your-react-component","text":"import { Interface } from 'leverege/api-redux' class MyScreen extends React.Component { constructor( props ) { super( props ) const { dispatch } = props this.items = Interface.create( system, 'myType' ).getList( interface ) dispatch( items.list() ) } render() { const { model } = this.props const items = this.items.getList( model ) return ( <div> {items.map( item => item.name )} </div> ) } } export default connect( state => ( { model : state.interface } ) )( MyScreen );","title":"Incorporate api-redux into your react component"},{"location":"ui/api-redux/#auth","text":"Auth has several methods all relating to authenticating the user and managing their password ( these need to be dispatched ) 1) verify( [ targetLocation ] ) - gets and verifies the bearer token 2) login( username, password, [ projectId ] ) - logs in the user 3) logout() - logs out the user 4) forgotPassword( username, projectId ) - sends an email to the user's email address with instructions to reset their password 5) changePassword( oldPassword, newPassword ) - changes the current logged in user's password 6) resetPassword( username, password, confirm, token, projectId ) - uses an api generated token to allow a user to change their password In addition to several methods to check the current state 1) isLoggedIn( state ) - returns a boolean representing if there is a user logged in 2) isLoading( state ) - returns a boolean representing if a Auth query is loading 3) isVerifying( state ) - returns a boolean representing if a verify query is loading 4) isError( state ) - returns a boolean representing if there was an Auth error 5) getErrorMessage( state ) - returns the last error message 6) getUsername( state ) - returns the current user's username 7) getUserId( state ) - returns the current user's imagine userId 8) getProfile( state ) - returns the current user's profile","title":"Auth"},{"location":"ui/api-redux/#users","text":"Users has several methods all relating to maintaining and giving permissions to users ( these need to be dispatched; almost all the methods are for admins, but some allow for the user to make changes to themselves ) 1) list( opts ) - gets all the current users for the project 2) create( user ) - creates a new project user 3) get( userId, opts ) - gets a user 4) remove( userId ) - removes a user 5) update( userId, values, opts ) - updates the user with the values 6) forcePWReset( userId, projectId ) - sends an email to the user to reset their password","title":"Users"},{"location":"ui/api-redux/#interface","text":"Interface is used to interact with devices and groups on the system and corresponds to the Interface queries. it is possible to chain together objects and groups to get a desired device.","title":"Interface"},{"location":"ui/api-redux/#create","text":"create( systemId, blueprintAlias ) - creates an Interface object with several methods 1) list( options ) - makes an api call to get all the devices in the system with the blueprintAlias 2) create( device, options ) - makes an api call to create a device 3) delete( id, options ) - makes an api call to delete a device 4) model( options ) - makes an api call to get the model 1) obj( id ) - creates an instance of the Obj class with the id 2) getList( state, opts ) - gets the list from the state 3) getModel( state, opts ) - gets the model from the state 4) isLoading( state, opts ) - returns a boolean based on if there is a query being made 5) isError( state ) - returns a boolean based on if there was an error for a query 6) isDone( state ) - returns a boolean based on if a query is finished","title":"create"},{"location":"ui/api-redux/#object","text":"Obj( parentPath, deviceId, attribute ) - creates an instance of the Obj class - created by Interface.create( 'mySystem', 'myBP' ).obj( 'myDeviceId' ) which will correctly manage the path for you This class is used to interact with the api for a specific device 1) get( options ) - makes an api call to get the device 2) history( query, options ) - makes an api call to get the history for the device. query is an object with a queryName and query object, refer to history documentation for how to structure the query 4) create( device, options ) - creates a child device 5) update( obj, options ) - updates the current device 6) delete( options ) - deletes the current device 7) model( options ) - gets the model for the current device 8) obj( id ) - new instance of the Obj class pointed at the new device 9) grp( attribute ) - new instance of the Grp class, attribute should be a relationship attribute 10) users() - gets all the users permissioned on the device 11) getObject( state ) - gets the device from the state after an api call has been made 12) getTable( state, opts ) - gets the table for the device from the state after an api call 13) getModel( state, opts ) - gets the model from the state after an api call 14) getHistory( state, queryName ) - gets the history query result corresponding to the queryName 15) isTable( state ) - boolean on whether the api result is a table 16) isLoading( state ) - boolean on whether an api call is being made 17) isError( state ) - boolean on whether an api call returned an error 18) isDone( state ) - boolean on whether an api call is done","title":"Object"},{"location":"ui/api-redux/#group","text":"Grp( parentPath, deviceId, attribute ) - creates an instance of the Grp class - created by Interface.create( 'mySystem', 'myBP' ).obj( 'myDeviceId' ).grp( 'myChildDevices' ) which will correctly manage the path for you A group represents a group of devices often by their relationship to a parent device 1) list( options ) - api call to get the list of the group of devices 2) create( obj, options ) - api call to create a new device for the group 3) get( id, options ) - api call to get a specific device from the group 4) update( id, data, options ) - api call to update a specific device from the group 5) delete( id, options ) - deletes a device 6) model( options ) - gets the model for the devices 7) obj( id ) - Obj class Instance 8) getList( state, opts ) - gets the list from the state 9) isTable( state, opts ) - returns a boolean based on if the api call result is a table 10) getObject( state, id, opts ) - returns a device from the state 11) getModel( state ) - returns the model from the state 12) isLoading( state ) - returns if an api call is loading 13) isError( state ) - return if an api call made an error 14) isDone( state ) - returns if an api call is finished","title":"Group"},{"location":"ui/api-redux/#filters","text":"You can install custom filter functions by calling Interface.setFilter( 'myFilterType', ( filter, items ) => { return items.filter( item => item.whatever == filter.myOption ) } )","title":"Filters"},{"location":"ui/ui-active-theme/","text":"The UIActiveTheme is a theme at gets its values from UI Builder's firebase database. Installation \u00b6 npm install --save-dev @leverege/ui-active-library Setup \u00b6 import { Theme } from '@leverege/ui-elements' import ActiveTheme from '@leverege/ui-active-theme' import AppTheme from './theme' // this is the normal, exported theme. let theme if ( process.env.NODE_ENV === 'development' ) { theme = new ActiveTheme( { altTheme : AppTheme, appearanceProjectId : <UUID> } ) } else { theme = AppTheme } ... <Theme theme={theme}> <MyApp /> </Theme>","title":"UI Active Theme"},{"location":"ui/ui-active-theme/#installation","text":"npm install --save-dev @leverege/ui-active-library","title":"Installation"},{"location":"ui/ui-active-theme/#setup","text":"import { Theme } from '@leverege/ui-elements' import ActiveTheme from '@leverege/ui-active-theme' import AppTheme from './theme' // this is the normal, exported theme. let theme if ( process.env.NODE_ENV === 'development' ) { theme = new ActiveTheme( { altTheme : AppTheme, appearanceProjectId : <UUID> } ) } else { theme = AppTheme } ... <Theme theme={theme}> <MyApp /> </Theme>","title":"Setup"},{"location":"ui/ui-attributes-graphs/","text":"ui-attributes-graphs Library \u00b6 Purpose \u00b6 The purpose of this library is to allow for the graphing of ui-attributes based values. Getting Started \u00b6 First, install this library: npm i @leverege/ui-attributes-graphs Next, define Attributes for the data that you are hoping to graph. For instance, the below example defines three attributes for a fictious 'test.history' data type: Plugins.add( 'Attribute', { name : 'name', displayName : 'Name', valueType : 'string', objectType : 'test.history', placement : { group : [ ] }, get : ( obj ) => { return obj?.name } } ) Plugins.add( 'Attribute', { name : 'test.history.time', displayName : 'Time', valueType : 'timestamp', objectType : 'test.history', placement : { group : [ ] }, get : ( obj ) => { const time = obj && obj.time return time == null ? null : new Date( time ) } } ) Plugins.add( 'Attribute', { name : 'test.history.temperature', displayName : 'Temperature', valueType : 'temperature', objectType : 'test.history', baseUnit : 'degC', placement : { group : [ ] }, get : ( obj ) => { const temp = obj?.data?.temperature return temp == null ? null : { type : 'temperature', value : temp, unit : 'degC' } } } ) In this case, time and temperature will be the two data points that we will be graphing against one another. Name is (in this case) a unique identifier for the objects that these data represent and will be used to key the data into series to facilitate graphing. Using the useSeriesData hook or the withSeriesData HOC exported by this library, prepare your data for graphing. The hook/HOC expect to be passed an array of data that you want to be graphed, an attribute name that will be used to chunk that data, and an options object with possible properties of obejctType and sortAttribute. The hook/HOC will tarnsform and memoize this data into an object keyed on the values of the chunk attribute that you have passed. If you pass an objectType in the options, it will be used to help disambiguate the chunk and sort attributes rather than using the first attribute registered with that particular name. If you pass a sortAttribute in the options, it will be used to sort the data in your series. Example \u00b6 const dataArray = [ { \"type\": \"test.history\", \"id\": \"1\", \"data\": { \"time\": 1642444110759, \"name\": \"Device 1\", \"data\": { \"temperature\": 45.7891 } } }, { \"type\": \"test.history\", \"id\": \"2\", \"data\": { \"time\": 1642461033498, \"name\": \"Device 1\", \"data\": { \"temperature\": 40.7705 } } }, { \"type\": \"test.history\", \"id\": \"3\", \"data\": { \"time\": 1642411955689, \"name\": \"Device 1\", \"data\": { \"temperature\": 48.7092 } } }, { \"type\": \"test.history\", \"id\": \"7\", \"data\": { \"time\": 1642474048199, \"name\": \"Device 2\", \"data\": { \"temperature\": 12.299 } } }, { \"type\": \"test.history\", \"id\": \"8\", \"data\": { \"time\": 1642407220379, \"name\": \"Device 2\", \"data\": { \"temperature\": 5.4862 } } }, { \"type\": \"test.history\", \"id\": \"9\", \"data\": { \"time\": 1642478679300, \"name\": \"Device 2\", \"data\": { \"temperature\": 4.5652 } } } ] const seriesData = useSeriesData( dataArray, 'name', { sortAttribute : 'test.time' } ) /* seriesData => { \"Device 2\": [ { \"type\": \"test.history\", \"id\": \"8\", \"data\": { \"time\": 1642407220379, \"name\": \"Device 2\", \"data\": { \"temperature\": 5.4862 } } }, { \"type\": \"test.history\", \"id\": \"7\", \"data\": { \"time\": 1642474048199, \"name\": \"Device 2\", \"data\": { \"temperature\": 12.299 } } }, { \"type\": \"test.history\", \"id\": \"9\", \"data\": { \"time\": 1642478679300, \"name\": \"Device 2\", \"data\": { \"temperature\": 4.5652 } } } ], \"Device 1\": [ { \"type\": \"test.history\", \"id\": \"3\", \"data\": { \"time\": 1642411955689, \"name\": \"Device 1\", \"data\": { \"temperature\": 48.7092 } } }, { \"type\": \"test.history\", \"id\": \"1\", \"data\": { \"time\": 1642444110759, \"name\": \"Device 1\", \"data\": { \"temperature\": 45.7891 } } }, { \"type\": \"test.history\", \"id\": \"2\", \"data\": { \"time\": 1642461033498, \"name\": \"Device 1\", \"data\": { \"temperature\": 40.7705 } } } ] } */ This hook/HOC will format your data into series in a manner that can be used by the ui-attributes-graphs library and will memoize the data to prevent unnecessart re-renders. If you can't use the hook/HOC because of the complexity of your data or formatting needs, a normal object keyed on some unique property will work fine, just remember to memoize the data yourself.","title":"UI Graph Attributes"},{"location":"ui/ui-attributes-graphs/#ui-attributes-graphs-library","text":"","title":"ui-attributes-graphs  Library"},{"location":"ui/ui-attributes-graphs/#purpose","text":"The purpose of this library is to allow for the graphing of ui-attributes based values.","title":"Purpose"},{"location":"ui/ui-attributes-graphs/#getting-started","text":"First, install this library: npm i @leverege/ui-attributes-graphs Next, define Attributes for the data that you are hoping to graph. For instance, the below example defines three attributes for a fictious 'test.history' data type: Plugins.add( 'Attribute', { name : 'name', displayName : 'Name', valueType : 'string', objectType : 'test.history', placement : { group : [ ] }, get : ( obj ) => { return obj?.name } } ) Plugins.add( 'Attribute', { name : 'test.history.time', displayName : 'Time', valueType : 'timestamp', objectType : 'test.history', placement : { group : [ ] }, get : ( obj ) => { const time = obj && obj.time return time == null ? null : new Date( time ) } } ) Plugins.add( 'Attribute', { name : 'test.history.temperature', displayName : 'Temperature', valueType : 'temperature', objectType : 'test.history', baseUnit : 'degC', placement : { group : [ ] }, get : ( obj ) => { const temp = obj?.data?.temperature return temp == null ? null : { type : 'temperature', value : temp, unit : 'degC' } } } ) In this case, time and temperature will be the two data points that we will be graphing against one another. Name is (in this case) a unique identifier for the objects that these data represent and will be used to key the data into series to facilitate graphing. Using the useSeriesData hook or the withSeriesData HOC exported by this library, prepare your data for graphing. The hook/HOC expect to be passed an array of data that you want to be graphed, an attribute name that will be used to chunk that data, and an options object with possible properties of obejctType and sortAttribute. The hook/HOC will tarnsform and memoize this data into an object keyed on the values of the chunk attribute that you have passed. If you pass an objectType in the options, it will be used to help disambiguate the chunk and sort attributes rather than using the first attribute registered with that particular name. If you pass a sortAttribute in the options, it will be used to sort the data in your series.","title":"Getting Started"},{"location":"ui/ui-attributes-graphs/#example","text":"const dataArray = [ { \"type\": \"test.history\", \"id\": \"1\", \"data\": { \"time\": 1642444110759, \"name\": \"Device 1\", \"data\": { \"temperature\": 45.7891 } } }, { \"type\": \"test.history\", \"id\": \"2\", \"data\": { \"time\": 1642461033498, \"name\": \"Device 1\", \"data\": { \"temperature\": 40.7705 } } }, { \"type\": \"test.history\", \"id\": \"3\", \"data\": { \"time\": 1642411955689, \"name\": \"Device 1\", \"data\": { \"temperature\": 48.7092 } } }, { \"type\": \"test.history\", \"id\": \"7\", \"data\": { \"time\": 1642474048199, \"name\": \"Device 2\", \"data\": { \"temperature\": 12.299 } } }, { \"type\": \"test.history\", \"id\": \"8\", \"data\": { \"time\": 1642407220379, \"name\": \"Device 2\", \"data\": { \"temperature\": 5.4862 } } }, { \"type\": \"test.history\", \"id\": \"9\", \"data\": { \"time\": 1642478679300, \"name\": \"Device 2\", \"data\": { \"temperature\": 4.5652 } } } ] const seriesData = useSeriesData( dataArray, 'name', { sortAttribute : 'test.time' } ) /* seriesData => { \"Device 2\": [ { \"type\": \"test.history\", \"id\": \"8\", \"data\": { \"time\": 1642407220379, \"name\": \"Device 2\", \"data\": { \"temperature\": 5.4862 } } }, { \"type\": \"test.history\", \"id\": \"7\", \"data\": { \"time\": 1642474048199, \"name\": \"Device 2\", \"data\": { \"temperature\": 12.299 } } }, { \"type\": \"test.history\", \"id\": \"9\", \"data\": { \"time\": 1642478679300, \"name\": \"Device 2\", \"data\": { \"temperature\": 4.5652 } } } ], \"Device 1\": [ { \"type\": \"test.history\", \"id\": \"3\", \"data\": { \"time\": 1642411955689, \"name\": \"Device 1\", \"data\": { \"temperature\": 48.7092 } } }, { \"type\": \"test.history\", \"id\": \"1\", \"data\": { \"time\": 1642444110759, \"name\": \"Device 1\", \"data\": { \"temperature\": 45.7891 } } }, { \"type\": \"test.history\", \"id\": \"2\", \"data\": { \"time\": 1642461033498, \"name\": \"Device 1\", \"data\": { \"temperature\": 40.7705 } } } ] } */ This hook/HOC will format your data into series in a manner that can be used by the ui-attributes-graphs library and will memoize the data to prevent unnecessart re-renders. If you can't use the hook/HOC because of the complexity of your data or formatting needs, a normal object keyed on some unique property will work fine, just remember to memoize the data yourself.","title":"Example"},{"location":"ui/ui-color-elements/","text":"This library is a react color picker element. In addition to this, there is an alpha picker and a field editor for hex values Install Library \u00b6 npm install @leverege/ui-color-elements --save Install CSS \u00b6 In the global-styles.css, import @import '../../node_modules/@leverege/ui-color-elements/lib/ui-color-elements.less'; ColorSelector \u00b6 import React from 'react' import { ColorSelector } from '@leverege/ui-color-elements' class Example extends React.component { render { return( <ColorSelector /> ) } } props \u00b6 onChange - function - called whenever there is a change to the widget by the user allowsNull - boolean - if true allows empty inputs eventData - any - returned as data prop in the onChange callback placeholder - string - input placeholder value color - string - variable or hex corresponding to the current color id - any - identifier of the returned jsx style - object - css styles of the returned jsx className - string - css class of the returned jsx showInput - boolean - if false will not show the color input disableAlpha - boolean - does not currently do anything variables - object - color variables CompactAlphaPicker \u00b6 import React from 'react' import { CompactAlphaPicker } from '@leverege/ui-color-elements' class Example extends React.component { render { return( <CompactAlphaPicker /> ) } } props \u00b6 onChange - function - callback attached to the input for the alpha onChangeComplete - function - callback for hex inputs and when the alpha is finished changing color - string - selected alpha onSwatchHover - function - callback when the swatch is hovered over colors - array - possible colors used to render the swatches hex - string - hex of the selected color rgb - string - rgb of the selected color className - string - css class of the element CompactFields \u00b6 import React from 'react' import { CompactFields } from '@leverege/ui-color-elements' class Example extends React.component { render { return( <CompactFields /> ) } } Props \u00b6 hex - string - hex value of the selected color rgb - string - rgb value of the selected color onChange - function - onChange callback of the input","title":"UI Color Elements"},{"location":"ui/ui-color-elements/#install-library","text":"npm install @leverege/ui-color-elements --save","title":"Install Library"},{"location":"ui/ui-color-elements/#install-css","text":"In the global-styles.css, import @import '../../node_modules/@leverege/ui-color-elements/lib/ui-color-elements.less';","title":"Install CSS"},{"location":"ui/ui-color-elements/#colorselector","text":"import React from 'react' import { ColorSelector } from '@leverege/ui-color-elements' class Example extends React.component { render { return( <ColorSelector /> ) } }","title":"ColorSelector"},{"location":"ui/ui-color-elements/#props","text":"onChange - function - called whenever there is a change to the widget by the user allowsNull - boolean - if true allows empty inputs eventData - any - returned as data prop in the onChange callback placeholder - string - input placeholder value color - string - variable or hex corresponding to the current color id - any - identifier of the returned jsx style - object - css styles of the returned jsx className - string - css class of the returned jsx showInput - boolean - if false will not show the color input disableAlpha - boolean - does not currently do anything variables - object - color variables","title":"props"},{"location":"ui/ui-color-elements/#compactalphapicker","text":"import React from 'react' import { CompactAlphaPicker } from '@leverege/ui-color-elements' class Example extends React.component { render { return( <CompactAlphaPicker /> ) } }","title":"CompactAlphaPicker"},{"location":"ui/ui-color-elements/#props_1","text":"onChange - function - callback attached to the input for the alpha onChangeComplete - function - callback for hex inputs and when the alpha is finished changing color - string - selected alpha onSwatchHover - function - callback when the swatch is hovered over colors - array - possible colors used to render the swatches hex - string - hex of the selected color rgb - string - rgb of the selected color className - string - css class of the element","title":"props"},{"location":"ui/ui-color-elements/#compactfields","text":"import React from 'react' import { CompactFields } from '@leverege/ui-color-elements' class Example extends React.component { render { return( <CompactFields /> ) } }","title":"CompactFields"},{"location":"ui/ui-color-elements/#props_2","text":"hex - string - hex value of the selected color rgb - string - rgb value of the selected color onChange - function - onChange callback of the input","title":"Props"},{"location":"ui/ui-elements/","text":"UI-Elements is intended to be used with the Leverege UI-builder. UI-Elements contains react classes for many commonly used elements. In addition to this, with variants, they can quickly be styled and restyled using the UI-builder. Install Library \u00b6 npm install @leverege/ui-elements --save Install CSS \u00b6 In the global-styles css (or a less file that will not be run through CSSModules), add an import of the ui-elements.css file. You will also want to include the css of the theme that will be used. If the theme exists in a library like the DefaultTheme, add it to the global-style.css file too. @import '../../node_modules/@leverege/ui-elements/lib/ui-elements.css'; @import '../../node_modules/@leverege/ui-elements/lib/theme/default.css'; Install Theme \u00b6 The Theme used should be installed near the root. import { Theme, DefaultTheme } from '@leverege/ui-elements' const theme = new DefaultTheme() ReactDOM.render( <Provider store={store}> <Theme theme={theme}> <App /> </Theme> </Provider>, document.getElementById( 'root' )) Within App, the theme will be available to the UI elements. EventData \u00b6 An element such as a button, dropdown, etc that send an event on a click or user action will send an object that has data about the action as well as an eventData object that can be supplied in props to the element. onClick = ( event ) => { console.log( event ) } render() { return <Button eventData={myData} onClick={this.onClick}>Click</Button> } When the button is clicked, onClick will be invoked with { data : myData, sourceEvent }. The data will contain the value passed into eventData. If the element holds value, such as a checkbox, radio, input field or selector, that value should be sent in a field called 'value'. This gives most events the minimum shape of: { data : <eventData> value : <value> }","title":"UI Elements"},{"location":"ui/ui-elements/#install-library","text":"npm install @leverege/ui-elements --save","title":"Install Library"},{"location":"ui/ui-elements/#install-css","text":"In the global-styles css (or a less file that will not be run through CSSModules), add an import of the ui-elements.css file. You will also want to include the css of the theme that will be used. If the theme exists in a library like the DefaultTheme, add it to the global-style.css file too. @import '../../node_modules/@leverege/ui-elements/lib/ui-elements.css'; @import '../../node_modules/@leverege/ui-elements/lib/theme/default.css';","title":"Install CSS"},{"location":"ui/ui-elements/#install-theme","text":"The Theme used should be installed near the root. import { Theme, DefaultTheme } from '@leverege/ui-elements' const theme = new DefaultTheme() ReactDOM.render( <Provider store={store}> <Theme theme={theme}> <App /> </Theme> </Provider>, document.getElementById( 'root' )) Within App, the theme will be available to the UI elements.","title":"Install Theme"},{"location":"ui/ui-elements/#eventdata","text":"An element such as a button, dropdown, etc that send an event on a click or user action will send an object that has data about the action as well as an eventData object that can be supplied in props to the element. onClick = ( event ) => { console.log( event ) } render() { return <Button eventData={myData} onClick={this.onClick}>Click</Button> } When the button is clicked, onClick will be invoked with { data : myData, sourceEvent }. The data will contain the value passed into eventData. If the element holds value, such as a checkbox, radio, input field or selector, that value should be sent in a field called 'value'. This gives most events the minimum shape of: { data : <eventData> value : <value> }","title":"EventData"},{"location":"ui/ui-linear-view-elements/","text":"This library contains several react elements that are designed to work together to create a gantt chart. These elements are Block , LinearView , MomentBackground , MomentHeader and Row . A class Projection determines the widths of elements based off of start and stop times. Install Library \u00b6 npm install @leverege/ui-linear-view-elements --save Install CSS \u00b6 In the global-styles.css, import @import '../node_modules/@leverege/ui-linear-view-elements/lib/ui-linear-view-elements.css'; Usage \u00b6 Data \u00b6 The data object is passed into the LinearView using the data prop. The rowAccess prop can be given to extract the array of row data from the data prop, or the data prop can be an array of row data. Each item in this row array can be either an array of block items, or a blockAccess prop can be given to resolve the array of block items. For example the data could simply be: const data = [ [ blockItem1, blockItem2 ], [ blockItem3 ] ] or somthing more structured/complicated like: const data = { minTime : 0, maxTime : 1000000, items : [ { id : 1, events : [ { }, { } ] }, { id : 2, events : [ { }, { } ] } ] } // Supply these as rowAccess and blockAccess function rowAccess( data ) { return data.items } function blockAccess( rowData ) { return rowData.events } Components \u00b6 By default, the css for the block and rows do not have any appearance. The do have layout attributes should as width, height, scroll, display and position set to support the scrolling of the view. Use rowClass , blockClass , blockStylizers and/or ui-elements to supply appearance. The block will have a top and bottom set on it to allow it to have size in the row. Thes can be override using the important, specitivity value, or by setting the variable shown below. top : var( --size-linearViewBlockTop, 8px ); bottom : var( --size-linearViewBlockBottom, 8px ); import { LinearView, Projection, MomentBackground, MomentHeader } from '@leverege/ui-linear-view-elements' import S from './MyComponent.css' render() { // This defines the view and scale. startValue and stopVale are milliseconds. pixelValue is the number // of milliseconds in one pixel. Changing pixelValue amounts to scaling. const proj = new Projection( { pixelValue : 30000, startValue, stopValue } ) <LinearView projection={proj} data={rows} blockStylizer={blockStylizer} rowClass={S.myRowClass} blockClass={getEventClass} startAccess={getStart} stopAccess={getStop}> <MomentBackground projection={proj} unit=\"hour\"/> <MomentHeader className={S.month} cellClassName={S.monthCell} projection={proj} unit=\"hour\" format=\"h a\" sticky /> } MomentHeaders \u00b6 The MomentHeader in the above example will rendering headers for time sections. You can stack several of these together. Using UI-Elements \u00b6 By default, the Row and Blocks are divs. They can be changed to UI-Elements (or other) components. Use rowComponent to define the row level component. To send extra props to the row component, use rowProps . Each row holds blocks. Use blockComponent to define the block level component. Each block can have children. To send extra props to the block component, use blockProps . The blockStylizer can override these values per item by returning in its object a component and/or componentProps value. These can be used to use UI-Element's Pane and variant structures. Here is an example that sets UI elements and variantes for all blocks moment headers. <LinearView projection={proj} data={rows} rowComponent={Pane} rowProps={{ variant : 'linearViewRow' }} blockComponent={Pane} blockProps={{ variant : 'linearViewBlock' }} blockStylizer={blockStylizer} blockClass={getEventClass} startAccess={getStart} stopAccess={getStop}> <MomentBackground projection={proj} unit=\"hour\"/> <MomentHeader className={S.month} cellComponent={Pane} cellComponentProps={{ variant : 'linearViewHeaderCell1' }} labelComponent={Text} labelComponentProps={{ variant : 'linearViewHeaderCell1' }} projection={proj} unit=\"day\" format=\"MMM DD\" sticky/> <MomentHeader className={S.month} cellComponent={Pane} cellComponentProps={{ variant : 'linearViewHeaderCell2' }} labelComponent={Text} labelComponentProps={{ variant : 'linearViewHeaderCell2' }} projection={proj} unit=\"hour\" format=\"h A\" sticky/> </LinearView> function blockStylizer( item, project, startValue, stopValue, x1, x2 ) { // Change the variant here based on the item properties. // Note, returning the component and componentProps make the block // and blockProps specified in the LinearView unnecessary. return { component : Pane, componentProps : { variant : 'linearViewBlock' } } }","title":"UI Linear View Elements"},{"location":"ui/ui-linear-view-elements/#install-library","text":"npm install @leverege/ui-linear-view-elements --save","title":"Install Library"},{"location":"ui/ui-linear-view-elements/#install-css","text":"In the global-styles.css, import @import '../node_modules/@leverege/ui-linear-view-elements/lib/ui-linear-view-elements.css';","title":"Install CSS"},{"location":"ui/ui-linear-view-elements/#usage","text":"","title":"Usage"},{"location":"ui/ui-linear-view-elements/#data","text":"The data object is passed into the LinearView using the data prop. The rowAccess prop can be given to extract the array of row data from the data prop, or the data prop can be an array of row data. Each item in this row array can be either an array of block items, or a blockAccess prop can be given to resolve the array of block items. For example the data could simply be: const data = [ [ blockItem1, blockItem2 ], [ blockItem3 ] ] or somthing more structured/complicated like: const data = { minTime : 0, maxTime : 1000000, items : [ { id : 1, events : [ { }, { } ] }, { id : 2, events : [ { }, { } ] } ] } // Supply these as rowAccess and blockAccess function rowAccess( data ) { return data.items } function blockAccess( rowData ) { return rowData.events }","title":"Data"},{"location":"ui/ui-linear-view-elements/#components","text":"By default, the css for the block and rows do not have any appearance. The do have layout attributes should as width, height, scroll, display and position set to support the scrolling of the view. Use rowClass , blockClass , blockStylizers and/or ui-elements to supply appearance. The block will have a top and bottom set on it to allow it to have size in the row. Thes can be override using the important, specitivity value, or by setting the variable shown below. top : var( --size-linearViewBlockTop, 8px ); bottom : var( --size-linearViewBlockBottom, 8px ); import { LinearView, Projection, MomentBackground, MomentHeader } from '@leverege/ui-linear-view-elements' import S from './MyComponent.css' render() { // This defines the view and scale. startValue and stopVale are milliseconds. pixelValue is the number // of milliseconds in one pixel. Changing pixelValue amounts to scaling. const proj = new Projection( { pixelValue : 30000, startValue, stopValue } ) <LinearView projection={proj} data={rows} blockStylizer={blockStylizer} rowClass={S.myRowClass} blockClass={getEventClass} startAccess={getStart} stopAccess={getStop}> <MomentBackground projection={proj} unit=\"hour\"/> <MomentHeader className={S.month} cellClassName={S.monthCell} projection={proj} unit=\"hour\" format=\"h a\" sticky /> }","title":"Components"},{"location":"ui/ui-linear-view-elements/#momentheaders","text":"The MomentHeader in the above example will rendering headers for time sections. You can stack several of these together.","title":"MomentHeaders"},{"location":"ui/ui-linear-view-elements/#using-ui-elements","text":"By default, the Row and Blocks are divs. They can be changed to UI-Elements (or other) components. Use rowComponent to define the row level component. To send extra props to the row component, use rowProps . Each row holds blocks. Use blockComponent to define the block level component. Each block can have children. To send extra props to the block component, use blockProps . The blockStylizer can override these values per item by returning in its object a component and/or componentProps value. These can be used to use UI-Element's Pane and variant structures. Here is an example that sets UI elements and variantes for all blocks moment headers. <LinearView projection={proj} data={rows} rowComponent={Pane} rowProps={{ variant : 'linearViewRow' }} blockComponent={Pane} blockProps={{ variant : 'linearViewBlock' }} blockStylizer={blockStylizer} blockClass={getEventClass} startAccess={getStart} stopAccess={getStop}> <MomentBackground projection={proj} unit=\"hour\"/> <MomentHeader className={S.month} cellComponent={Pane} cellComponentProps={{ variant : 'linearViewHeaderCell1' }} labelComponent={Text} labelComponentProps={{ variant : 'linearViewHeaderCell1' }} projection={proj} unit=\"day\" format=\"MMM DD\" sticky/> <MomentHeader className={S.month} cellComponent={Pane} cellComponentProps={{ variant : 'linearViewHeaderCell2' }} labelComponent={Text} labelComponentProps={{ variant : 'linearViewHeaderCell2' }} projection={proj} unit=\"hour\" format=\"h A\" sticky/> </LinearView> function blockStylizer( item, project, startValue, stopValue, x1, x2 ) { // Change the variant here based on the item properties. // Note, returning the component and componentProps make the block // and blockProps specified in the LinearView unnecessary. return { component : Pane, componentProps : { variant : 'linearViewBlock' } } }","title":"Using UI-Elements"},{"location":"ui/ui-mapbox-elements/","text":"UI-mapbox-elements is a wrapper for Mapbox. It abstracts away Mapbox, making it quick to implement. Install Library \u00b6 npm install @leverege/ui-mapbox-elements --save Install CSS \u00b6 In the global-styles.css, import @import '../../node_modules/@leverege/ui-mapbox-elements/lib/ui-mapbox-elements.css'; In the projects .env , add these: MAPBOX_APIKEY=<MY_API_KEY> MAPBOX_SATELLITE=<Satellite url, defauls to 'mapbox://styles/mapbox/satellite-v9'> MAPBOX_VECTOR=<Vector url, defaults to 'mapbox://styles/mapbox/light-v9'> GeoAdapter and LayerCreator \u00b6 These classes are meant to make construction of the geoshapes easier. To use, make a Render component that extends React.Component and add it to the maps layers: layers = [ <ZoneMapRenderer key=\"zoneRenderer\" map={this.map} items={zones} />, ... ] ... <Map layers={layers} ...moreOptions /> In this case the ZoneMapRenderer looks something like this: import React from 'react' import { connect } from 'react-redux' import { LayerCreator, DrawControl } from '@leverege/ui-mapbox-elements' class ZoneMapRenderer extends React.Component { constructor( options ) { super( options ) this.zoneAdapter = new ZoneAdapter( ) // Your specific shapes' styling and data access this.zones = new LayerCreator( { id : 'zone', // Give prefix for layer types dataType : 'zone', // Give type of dataTypes adapter : this.zoneAdapter // Give adapter } ) } render() { const { items, interaction, rollover, selected, extraData, editing, mode } = this.props const opts = { ...extraData, rollover, selected } // Add rollover and selected to allow highlighting interaction.setSelectableId( 'zone-fill' ) // Add this as a selectable thing if desired interaction.addRolloverId( 'zone-fill' ) // Add this as a rollover thing if desired if ( !editing ) { return this.zones.getLayers( items, opts ) } // Can do something like this to add an editor const dc = ( <DrawControl key=\"zone-drawcontrol\" mapName={'map'} mode={mode} features={this.zones.getFeatureCollection( items, opts )} // get feature collection ref={ interaction.setDrawControl } selectedIds={selected} ...eventListeners /> ) return [ ...l, dc ] } } export default connect( ( state, props ) => { // Do some calculation/data retreival as necessary return { selected : state.select.selectable, rollover : state.select.rollover, } } )( ZoneMapRenderer ) Example \u00b6 import React from 'react' import PropTypes from 'prop-types' import { connect } from 'react-redux' import { Select, UI } from '@leverege/ui-redux' import { Button } from '@leverege/ui-elements' import { Map, MapStyle, Symbols, SymbolLayer, Symbolizer, DrawControl, GeoshapeControl } from '@leverege/ui-mapbox-elements' import S from './MyMap.less' class MyMap extends React.Component { static contextTypes = { store : PropTypes.object } constructor( props ) { super( props ) this.state = {} this.myItemSymbolizer = Symbolizer.create( 'myItem' ) const layout = SymbolLayer.createLayout() layout['text-allow-overlap'] = false layout['text-ignore-placement'] = false layout['text-optional'] = true this.itemLayer = new SymbolLayer({ dataType : 'myItem', layout, alwaysPushToNormal : true, latAccess : '/data/location/latitude', lonAccess : '/data/location/longitude', nameAccess : '/data/name' }) // Load the symbols that the symbolizers reference. const sm = 32 const lg = 40 this.symbols = new Symbols(( s ) => { this.setState({ symbols : s.getImagesArray() }) }) this.symbols.addImage( 'myItem', { src : '/images/myItem.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-rollover', { src : '/images/myItem.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-selected', { src : '/images/myItem-selected.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-selected-rollover', { src : '/images/myItem-selected.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-alert', { src : '/images/myItem-alert.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-alert-rollover', { src : '/images/myItem-alert.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-alert-targeted', { src : '/images/myItem-alert-targeted.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-alert-targeted-rollover', { src : '/images/myItem-alert-targeted.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-targeted', { src : '/images/myItem-targeted.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-targeted-rollover', { src : '/images/myItem-targeted.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-targeted-selected', { src : '/images/myItem-targeted-selected.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-targeted-selected-rollover', { src : '/images/myItem-targeted-selected.png', width : lg, height : lg }) // Create a geoshape controller this.geoshapes = new GeoshapeControl( this ) } onMapStyle = ( e ) => { const { mapStyle } = this.props const { store } = this.context store.dispatch( UI.set( 'map/style', MapStyle.next( mapStyle ))) } /** * A selection occurred or off object click occurred. **/ onMapClick = ( e ) => { const { store } = this.context if ( e.type == null ) { // Clear all items store.dispatch( Select.set( null, 'myItemsSelected' )) } else if ( e.type === 'myItem' ) { store.dispatch( Select.set( e.id, 'myItemsSelected' )) } // and others } onRollover = ( e ) => { const { rollover } = this.props // Is rollover the same? if ( ( e.id == null && Select.isEmpty( rollover )) || Select.isSelected( rollover, e.id ) ) { return } const { store } = this.context store.dispatch( Select.set( e.id || null, 'rollover' )) } render() { const { myItems, selected, targeted, rollover, mapStyle, mapMode, geoshapeSelected, geoshapesVisible } = this.props const { symbols } = this.state const gsf = this.geoshapes.getFeatureCollectionFor( geoshapeSelected ) const dc = ( <DrawControl key=\"drawControl\" mode={mapMode} features={gsf} ref={( ref ) => { this.drawControl = ref }} selectedIds={geoshapeSelected} /> ) const layers = symbols == null ? [] : [ geoshapesVisible === false ? [] : this.geoshapes.getLayers(), this.myItemsLayer.getLayers( myItesm, { symbolizer : this.myItemSymbolizer, selected, targeted, rollover }), dc ] // to style zoom rotate control config pass in additional css const zoomRotateControlConfig = { enabled: true, // defaults to false position: 'top-left' // defaults to top-right containerStyle: { // default values below border: 'none', 'box-shadow': '0px 1px 3px rgba(41, 50, 61, 0.16)' } } return ( <div> <Map mapName=\"map\" mapStyle={MapStyle.url( mapStyle )} symbols={symbols} layers={layers} popups={popups} drawControl={this.drawControl} onClick={this.onMapClick} zoomRotateControl={zoomRotateControlConfig} onRollover={this.onRollover} onDrawCreate={this.geoshapes.onCreateShape} onDrawCombine={this.geoshapes.onCombineShapes} onDrawUpdate={this.geoshapes.onShapeUpdate} onDrawModeChange={this.geoshapes.onModeChange} /> { /* Add Other widgets if desired */ } <Button className={S.styleButton} onClick={this.onMapStyle}><i className=\"sm-globe\" /></Button> </div> ) } } export default connect(( state ) => { const myItems = state.myModel.myItems const selected = state.select.myItemsSelected || [] const targeted = state.select.myItemsTargeted || [] return { myItems, targeted, selected, rollover : state.select.rollover, geoshapes : state.myModel.geoshapes, geoshapesActive : state.ui.geoshapesActive, geoshapesVisible : state.fleet.userOptions.geoshapesVisible, geoshapeSelected : state.select.geoshapeSelected, mapMode : state.ui.map.mode, mapStyle : state.ui.map.style } })( MyMap )","title":"UI Mapbox Elements"},{"location":"ui/ui-mapbox-elements/#install-library","text":"npm install @leverege/ui-mapbox-elements --save","title":"Install Library"},{"location":"ui/ui-mapbox-elements/#install-css","text":"In the global-styles.css, import @import '../../node_modules/@leverege/ui-mapbox-elements/lib/ui-mapbox-elements.css'; In the projects .env , add these: MAPBOX_APIKEY=<MY_API_KEY> MAPBOX_SATELLITE=<Satellite url, defauls to 'mapbox://styles/mapbox/satellite-v9'> MAPBOX_VECTOR=<Vector url, defaults to 'mapbox://styles/mapbox/light-v9'>","title":"Install CSS"},{"location":"ui/ui-mapbox-elements/#geoadapter-and-layercreator","text":"These classes are meant to make construction of the geoshapes easier. To use, make a Render component that extends React.Component and add it to the maps layers: layers = [ <ZoneMapRenderer key=\"zoneRenderer\" map={this.map} items={zones} />, ... ] ... <Map layers={layers} ...moreOptions /> In this case the ZoneMapRenderer looks something like this: import React from 'react' import { connect } from 'react-redux' import { LayerCreator, DrawControl } from '@leverege/ui-mapbox-elements' class ZoneMapRenderer extends React.Component { constructor( options ) { super( options ) this.zoneAdapter = new ZoneAdapter( ) // Your specific shapes' styling and data access this.zones = new LayerCreator( { id : 'zone', // Give prefix for layer types dataType : 'zone', // Give type of dataTypes adapter : this.zoneAdapter // Give adapter } ) } render() { const { items, interaction, rollover, selected, extraData, editing, mode } = this.props const opts = { ...extraData, rollover, selected } // Add rollover and selected to allow highlighting interaction.setSelectableId( 'zone-fill' ) // Add this as a selectable thing if desired interaction.addRolloverId( 'zone-fill' ) // Add this as a rollover thing if desired if ( !editing ) { return this.zones.getLayers( items, opts ) } // Can do something like this to add an editor const dc = ( <DrawControl key=\"zone-drawcontrol\" mapName={'map'} mode={mode} features={this.zones.getFeatureCollection( items, opts )} // get feature collection ref={ interaction.setDrawControl } selectedIds={selected} ...eventListeners /> ) return [ ...l, dc ] } } export default connect( ( state, props ) => { // Do some calculation/data retreival as necessary return { selected : state.select.selectable, rollover : state.select.rollover, } } )( ZoneMapRenderer )","title":"GeoAdapter and LayerCreator"},{"location":"ui/ui-mapbox-elements/#example","text":"import React from 'react' import PropTypes from 'prop-types' import { connect } from 'react-redux' import { Select, UI } from '@leverege/ui-redux' import { Button } from '@leverege/ui-elements' import { Map, MapStyle, Symbols, SymbolLayer, Symbolizer, DrawControl, GeoshapeControl } from '@leverege/ui-mapbox-elements' import S from './MyMap.less' class MyMap extends React.Component { static contextTypes = { store : PropTypes.object } constructor( props ) { super( props ) this.state = {} this.myItemSymbolizer = Symbolizer.create( 'myItem' ) const layout = SymbolLayer.createLayout() layout['text-allow-overlap'] = false layout['text-ignore-placement'] = false layout['text-optional'] = true this.itemLayer = new SymbolLayer({ dataType : 'myItem', layout, alwaysPushToNormal : true, latAccess : '/data/location/latitude', lonAccess : '/data/location/longitude', nameAccess : '/data/name' }) // Load the symbols that the symbolizers reference. const sm = 32 const lg = 40 this.symbols = new Symbols(( s ) => { this.setState({ symbols : s.getImagesArray() }) }) this.symbols.addImage( 'myItem', { src : '/images/myItem.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-rollover', { src : '/images/myItem.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-selected', { src : '/images/myItem-selected.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-selected-rollover', { src : '/images/myItem-selected.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-alert', { src : '/images/myItem-alert.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-alert-rollover', { src : '/images/myItem-alert.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-alert-targeted', { src : '/images/myItem-alert-targeted.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-alert-targeted-rollover', { src : '/images/myItem-alert-targeted.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-targeted', { src : '/images/myItem-targeted.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-targeted-rollover', { src : '/images/myItem-targeted.png', width : lg, height : lg }) this.symbols.addImage( 'myItem-targeted-selected', { src : '/images/myItem-targeted-selected.png', width : sm, height : sm }) this.symbols.addImage( 'myItem-targeted-selected-rollover', { src : '/images/myItem-targeted-selected.png', width : lg, height : lg }) // Create a geoshape controller this.geoshapes = new GeoshapeControl( this ) } onMapStyle = ( e ) => { const { mapStyle } = this.props const { store } = this.context store.dispatch( UI.set( 'map/style', MapStyle.next( mapStyle ))) } /** * A selection occurred or off object click occurred. **/ onMapClick = ( e ) => { const { store } = this.context if ( e.type == null ) { // Clear all items store.dispatch( Select.set( null, 'myItemsSelected' )) } else if ( e.type === 'myItem' ) { store.dispatch( Select.set( e.id, 'myItemsSelected' )) } // and others } onRollover = ( e ) => { const { rollover } = this.props // Is rollover the same? if ( ( e.id == null && Select.isEmpty( rollover )) || Select.isSelected( rollover, e.id ) ) { return } const { store } = this.context store.dispatch( Select.set( e.id || null, 'rollover' )) } render() { const { myItems, selected, targeted, rollover, mapStyle, mapMode, geoshapeSelected, geoshapesVisible } = this.props const { symbols } = this.state const gsf = this.geoshapes.getFeatureCollectionFor( geoshapeSelected ) const dc = ( <DrawControl key=\"drawControl\" mode={mapMode} features={gsf} ref={( ref ) => { this.drawControl = ref }} selectedIds={geoshapeSelected} /> ) const layers = symbols == null ? [] : [ geoshapesVisible === false ? [] : this.geoshapes.getLayers(), this.myItemsLayer.getLayers( myItesm, { symbolizer : this.myItemSymbolizer, selected, targeted, rollover }), dc ] // to style zoom rotate control config pass in additional css const zoomRotateControlConfig = { enabled: true, // defaults to false position: 'top-left' // defaults to top-right containerStyle: { // default values below border: 'none', 'box-shadow': '0px 1px 3px rgba(41, 50, 61, 0.16)' } } return ( <div> <Map mapName=\"map\" mapStyle={MapStyle.url( mapStyle )} symbols={symbols} layers={layers} popups={popups} drawControl={this.drawControl} onClick={this.onMapClick} zoomRotateControl={zoomRotateControlConfig} onRollover={this.onRollover} onDrawCreate={this.geoshapes.onCreateShape} onDrawCombine={this.geoshapes.onCombineShapes} onDrawUpdate={this.geoshapes.onShapeUpdate} onDrawModeChange={this.geoshapes.onModeChange} /> { /* Add Other widgets if desired */ } <Button className={S.styleButton} onClick={this.onMapStyle}><i className=\"sm-globe\" /></Button> </div> ) } } export default connect(( state ) => { const myItems = state.myModel.myItems const selected = state.select.myItemsSelected || [] const targeted = state.select.myItemsTargeted || [] return { myItems, targeted, selected, rollover : state.select.rollover, geoshapes : state.myModel.geoshapes, geoshapesActive : state.ui.geoshapesActive, geoshapesVisible : state.fleet.userOptions.geoshapesVisible, geoshapeSelected : state.select.geoshapeSelected, mapMode : state.ui.map.mode, mapStyle : state.ui.map.style } })( MyMap )","title":"Example"},{"location":"ui/ui-redux/","text":"UI-redux contains some commonly used front end functionality and integrates it with redux to make it easily used across and app. Installation \u00b6 npm install @leverege/ui-redux --save Reducer Setup \u00b6 When creating the reducers, include the InterfaceReducer import { combineReducers } from 'redux' import { SelectReducer, UIReducer } from 'ui-redux' ... const reducers = combineReducers( { ... select : Select, ui : UI } ) export default reducers Actions and Access \u00b6 select \u00b6 select is a reducer responsible for maintaining arrays (groups) of items that have been selected. import { select } from '@leverege/ui-redux' dispatch( select.add( UUID, myGroup ) ) the methods on select are add( ids, group ) - adds ids to the specified group set( ids, group ) - changes the group to be the ids provided remove( ids, group ) - removes the ids from the group toggle( ids, group ) - if in group, removes id, if not in group adds id; for each id clear( group ) - resets the group to no selection clearAll() - resets all groups to no selection isSelected( groupData, id ) - checks if an item is in a group count( groupData ) - count of the number of items in a group isEmpty( groupData ) - checks if group is empty UI \u00b6 UI is a simple reducer that maintains keys. import { UI } from '@leverege/ui-redux' dispatch( UI.set( myKey, myValue ) ) the UI methods are set( key, value ) - sets the value of the key multiSet( keyValue ) - object that has all of its keys and values set Removed \u00b6 ReduxForm was removed so ui-redux does not have to depend on ui-elements. It its code is here, if need. import { Form, FormValidator } from '@leverege/ui-elements' import FormActions from './Form' class ConnectedComponent { constructor( { setData, getData } ) { this.setData = setData this.getData = getData } setState( diff ) { return this.setData( diff ) } get state() { return this.getData() } } class ReduxForm extends Form { setFormData = ( diff ) => { const { validator, id } = this.props return validator.dispatch( FormActions.multiSet( id, diff ) ) } getFormData = () => { const { data } = this.props return data } getComponent() { const { validator } = this.props if ( !this.component ) { this.component = new ConnectedComponent( { setData : this.setFormData, getData : this.getFormData, ...validator.connectedComponent } ) } return this.component } getValidator( ) { const { validator } = this.props if ( this.validator == null || this.validator.options !== validator ) { this.validator = FormValidator.create( this.getComponent(), validator, { onRevert : this.onRevert } ) } return this.validator } } module.exports = ReduxForm","title":"UI Redux"},{"location":"ui/ui-redux/#installation","text":"npm install @leverege/ui-redux --save","title":"Installation"},{"location":"ui/ui-redux/#reducer-setup","text":"When creating the reducers, include the InterfaceReducer import { combineReducers } from 'redux' import { SelectReducer, UIReducer } from 'ui-redux' ... const reducers = combineReducers( { ... select : Select, ui : UI } ) export default reducers","title":"Reducer Setup"},{"location":"ui/ui-redux/#actions-and-access","text":"","title":"Actions and Access"},{"location":"ui/ui-redux/#select","text":"select is a reducer responsible for maintaining arrays (groups) of items that have been selected. import { select } from '@leverege/ui-redux' dispatch( select.add( UUID, myGroup ) ) the methods on select are add( ids, group ) - adds ids to the specified group set( ids, group ) - changes the group to be the ids provided remove( ids, group ) - removes the ids from the group toggle( ids, group ) - if in group, removes id, if not in group adds id; for each id clear( group ) - resets the group to no selection clearAll() - resets all groups to no selection isSelected( groupData, id ) - checks if an item is in a group count( groupData ) - count of the number of items in a group isEmpty( groupData ) - checks if group is empty","title":"select"},{"location":"ui/ui-redux/#ui","text":"UI is a simple reducer that maintains keys. import { UI } from '@leverege/ui-redux' dispatch( UI.set( myKey, myValue ) ) the UI methods are set( key, value ) - sets the value of the key multiSet( keyValue ) - object that has all of its keys and values set","title":"UI"},{"location":"ui/ui-redux/#removed","text":"ReduxForm was removed so ui-redux does not have to depend on ui-elements. It its code is here, if need. import { Form, FormValidator } from '@leverege/ui-elements' import FormActions from './Form' class ConnectedComponent { constructor( { setData, getData } ) { this.setData = setData this.getData = getData } setState( diff ) { return this.setData( diff ) } get state() { return this.getData() } } class ReduxForm extends Form { setFormData = ( diff ) => { const { validator, id } = this.props return validator.dispatch( FormActions.multiSet( id, diff ) ) } getFormData = () => { const { data } = this.props return data } getComponent() { const { validator } = this.props if ( !this.component ) { this.component = new ConnectedComponent( { setData : this.setFormData, getData : this.getFormData, ...validator.connectedComponent } ) } return this.component } getValidator( ) { const { validator } = this.props if ( this.validator == null || this.validator.options !== validator ) { this.validator = FormValidator.create( this.getComponent(), validator, { onRevert : this.onRevert } ) } return this.validator } } module.exports = ReduxForm","title":"Removed"},{"location":"utilities/array-util/","text":"This library contains methods that expand on the functionality of common array methods Install \u00b6 npm install --save @leverege/array-util const ArrayUtil = require( '@leverege/array-util' ); index( array, key, value, from = 0 ) \u00b6 This will look for the first item whose key == value and return its index. If an item is not found, -1 is returned. const arr = [ { id : 1, name : 'hamster' }, { id : 2, name : 'cat' }, { id : 3, name : 'dog' } ] ArrayUtil.index( arr, 'name', 'cat' ) // 1 ArrayUtil.index( arr, 'name', 'cat', 1 ) // 1 ArrayUtil.index( arr, 'name', 'cat', 2 ) // -1 find( array, key, value, from = 0 ) \u00b6 This will look for the first item whose key == value and return it. If one is not found, null is returned. const arr = [ { id : 1, name : 'hamster' }, { id : 2, name : 'cat' }, { id : 3, name : 'dog' } ] ArrayUtil.find( arr, 'name', 'cat' ) // { id: 2, name: 'cat' } ArrayUtil.find( arr, 'name', 'cat', 1 ) // { id: 2, name: 'cat' } ArrayUtil.find( arr, 'name', 'cat', 2 ) // null replace( array, key, value, elem, from = 0 ) \u00b6 This will look for the first item whose key == value and replace it with elem . The index of the replacement is returned. If one is not found, -1 is returned. const arr = [ { id : 1, name : 'hamster' }, { id : 2, name : 'cat' }, { id : 3, name : 'dog' } ] ArrayUtil.replace( arr, 'name', 'cat', { name:'cat', active:false } ) // 1 // arr = [ { id: 1, name: 'hamster' }, { name: 'cat', active: false }, { id: 3, name: 'dog' } ] ArrayUtil.replace( arr, 'name', 'cat', { name:'cat', active:true }, 1 ) // 1 // arr = [ { id: 1, name: 'hamster' }, { name: 'cat', active: true }, { id: 3, name: 'dog' } ] ArrayUtil.replace( arr, 'name', 'cat', { name:'cat' }, 2 ) // -1 // arr = [ { id: 1, name: 'hamster' }, { name: 'cat', active: true }, { id: 3, name: 'dog' } ] remove( array, key, value, from = 0 ) \u00b6 Removes the first occurrence where arr[n][key] == value and returns the index. If no match is found, -1 is returned. findRemove( array, key, value, from = 0 ) \u00b6 Similar to remove() , but the actual item the is being removed is returned. removeAll( array, key, value, from = 0 ) \u00b6 Removes the all occurrences where arr[n][key] == value and returns the index. An array of the removed items is returned. diff( sArr, nArr ) \u00b6 Returns an object containing { added : [], removed : [], same : [] } where each of the added, removed and same array contains differences between sArr and nArr","title":"Array Util"},{"location":"utilities/array-util/#install","text":"npm install --save @leverege/array-util const ArrayUtil = require( '@leverege/array-util' );","title":"Install"},{"location":"utilities/array-util/#index-array-key-value-from-0","text":"This will look for the first item whose key == value and return its index. If an item is not found, -1 is returned. const arr = [ { id : 1, name : 'hamster' }, { id : 2, name : 'cat' }, { id : 3, name : 'dog' } ] ArrayUtil.index( arr, 'name', 'cat' ) // 1 ArrayUtil.index( arr, 'name', 'cat', 1 ) // 1 ArrayUtil.index( arr, 'name', 'cat', 2 ) // -1","title":"index( array, key, value, from = 0 )"},{"location":"utilities/array-util/#find-array-key-value-from-0","text":"This will look for the first item whose key == value and return it. If one is not found, null is returned. const arr = [ { id : 1, name : 'hamster' }, { id : 2, name : 'cat' }, { id : 3, name : 'dog' } ] ArrayUtil.find( arr, 'name', 'cat' ) // { id: 2, name: 'cat' } ArrayUtil.find( arr, 'name', 'cat', 1 ) // { id: 2, name: 'cat' } ArrayUtil.find( arr, 'name', 'cat', 2 ) // null","title":"find( array, key, value, from = 0 )"},{"location":"utilities/array-util/#replace-array-key-value-elem-from-0","text":"This will look for the first item whose key == value and replace it with elem . The index of the replacement is returned. If one is not found, -1 is returned. const arr = [ { id : 1, name : 'hamster' }, { id : 2, name : 'cat' }, { id : 3, name : 'dog' } ] ArrayUtil.replace( arr, 'name', 'cat', { name:'cat', active:false } ) // 1 // arr = [ { id: 1, name: 'hamster' }, { name: 'cat', active: false }, { id: 3, name: 'dog' } ] ArrayUtil.replace( arr, 'name', 'cat', { name:'cat', active:true }, 1 ) // 1 // arr = [ { id: 1, name: 'hamster' }, { name: 'cat', active: true }, { id: 3, name: 'dog' } ] ArrayUtil.replace( arr, 'name', 'cat', { name:'cat' }, 2 ) // -1 // arr = [ { id: 1, name: 'hamster' }, { name: 'cat', active: true }, { id: 3, name: 'dog' } ]","title":"replace( array, key, value, elem, from = 0 )"},{"location":"utilities/array-util/#remove-array-key-value-from-0","text":"Removes the first occurrence where arr[n][key] == value and returns the index. If no match is found, -1 is returned.","title":"remove( array, key, value, from = 0 )"},{"location":"utilities/array-util/#findremove-array-key-value-from-0","text":"Similar to remove() , but the actual item the is being removed is returned.","title":"findRemove( array, key, value, from = 0 )"},{"location":"utilities/array-util/#removeall-array-key-value-from-0","text":"Removes the all occurrences where arr[n][key] == value and returns the index. An array of the removed items is returned.","title":"removeAll( array, key, value, from = 0 )"},{"location":"utilities/array-util/#diff-sarr-narr","text":"Returns an object containing { added : [], removed : [], same : [] } where each of the added, removed and same array contains differences between sArr and nArr","title":"diff( sArr, nArr )"},{"location":"utilities/base62-util/","text":"This library has several functions, it generates various types of UUIDs. In addition this library has a simple interface for encoding and decoding base 62. Install \u00b6 npm install @leverege/base62-util --save Usage \u00b6 let B62 = require('@leverege/base62-util' ) console.log( 'v4', B62.v4() ) console.log( 'v1', B62.v1() ) let s = 'my-string' let p = 'b62:' let e = B62.encode( s, p ) let d = B62.decode( e, p ) console.log( 'encode', e ) console.log( 'decode', d ) Result v4 7ofXDw8naj2j3X844FYHMM v1 507UkVScUVn99HX6k2s0f1 encode b62:CO5mtg126i3l decode my-string","title":"Base62 Util"},{"location":"utilities/base62-util/#install","text":"npm install @leverege/base62-util --save","title":"Install"},{"location":"utilities/base62-util/#usage","text":"let B62 = require('@leverege/base62-util' ) console.log( 'v4', B62.v4() ) console.log( 'v1', B62.v1() ) let s = 'my-string' let p = 'b62:' let e = B62.encode( s, p ) let d = B62.decode( e, p ) console.log( 'encode', e ) console.log( 'decode', d ) Result v4 7ofXDw8naj2j3X844FYHMM v1 507UkVScUVn99HX6k2s0f1 encode b62:CO5mtg126i3l decode my-string","title":"Usage"},{"location":"utilities/comms/","text":"Make sure .babelrc is updated to use only server or browser targets or both as needed Comms \u00b6 This lib provides a chaining interface top of a request mechanism (Axios) Installation \u00b6 npm install --save @leverege/comms Usage \u00b6 Include the library in your module: const Comms = require( '@leverege/comms' ) Simple Usage \u00b6 const { Comms } = require( '@leverege/comms' ) const api = new Comms( { host } ) // GET <host>/assets?q=fun const list = await api.get( 'assets', { q : 'fun' } ) // GET <host>/assets/<id>/connections const list = await api.child( 'assets' ).child( id ).get( 'connections' ) Setting Auth \u00b6 The auth option can be supplied to set the Autentication header. It can be a string or an object with a getToken function. With a simple string: const auth = \"Some Auth Token\" const api = new Comms( { host, auth } ) With an object: function authToken( token, type = 'Bearer' ) { return { getToken( ) { return type == null ? token : `${type} ${token}` } } } const createAuth = ( opts ) => { return { getToken : ( method, url, extraParams, comms ) => { return `Bearer ${opts.create( methord, url, extraParams )}` } } } const api = new Comms( { host, auth : createAuth( opts ) } ) The getToken method will be called with every request. By default, the setToken( tkn ) call will assume a Bearer JWT token.","title":"Comms"},{"location":"utilities/comms/#comms","text":"This lib provides a chaining interface top of a request mechanism (Axios)","title":"Comms"},{"location":"utilities/comms/#installation","text":"npm install --save @leverege/comms","title":"Installation"},{"location":"utilities/comms/#usage","text":"Include the library in your module: const Comms = require( '@leverege/comms' )","title":"Usage"},{"location":"utilities/comms/#simple-usage","text":"const { Comms } = require( '@leverege/comms' ) const api = new Comms( { host } ) // GET <host>/assets?q=fun const list = await api.get( 'assets', { q : 'fun' } ) // GET <host>/assets/<id>/connections const list = await api.child( 'assets' ).child( id ).get( 'connections' )","title":"Simple Usage"},{"location":"utilities/comms/#setting-auth","text":"The auth option can be supplied to set the Autentication header. It can be a string or an object with a getToken function. With a simple string: const auth = \"Some Auth Token\" const api = new Comms( { host, auth } ) With an object: function authToken( token, type = 'Bearer' ) { return { getToken( ) { return type == null ? token : `${type} ${token}` } } } const createAuth = ( opts ) => { return { getToken : ( method, url, extraParams, comms ) => { return `Bearer ${opts.create( methord, url, extraParams )}` } } } const api = new Comms( { host, auth : createAuth( opts ) } ) The getToken method will be called with every request. By default, the setToken( tkn ) call will assume a Bearer JWT token.","title":"Setting Auth"},{"location":"utilities/data-store/","text":"Data Store is a wrapper for Front-end and Back-end database connections. Data Store maintains the structure of the database and provides an API for interacting with the database. It is important to use Data Store for every component interacting with the database, so that it is always in the expected format. Usage \u00b6 npm install --save @leverege/data-store Server Usage \u00b6 To use with a serviceAccount, use const DataStore = require( '@leverege/data-store' ) const store = DataStore.createRootStore( { type, serviceAccount, databaseUrl, root } ) const data = store.access( 'path/to/data' ) data.update( path, value, onComplete ) data.once( path, onComplete, onError ) Client Usage \u00b6 to use as a client with signIn capability: const DataStore = require( '@leverege/data-store/lib/web' ) const store = DataStore.createRootStore( { type, jwt, databaseUrl, root } ) store.access( 'path/to/data' ) ```","title":"Data Store"},{"location":"utilities/data-store/#usage","text":"npm install --save @leverege/data-store","title":"Usage"},{"location":"utilities/data-store/#server-usage","text":"To use with a serviceAccount, use const DataStore = require( '@leverege/data-store' ) const store = DataStore.createRootStore( { type, serviceAccount, databaseUrl, root } ) const data = store.access( 'path/to/data' ) data.update( path, value, onComplete ) data.once( path, onComplete, onError )","title":"Server Usage"},{"location":"utilities/data-store/#client-usage","text":"to use as a client with signIn capability: const DataStore = require( '@leverege/data-store/lib/web' ) const store = DataStore.createRootStore( { type, jwt, databaseUrl, root } ) store.access( 'path/to/data' ) ```","title":"Client Usage"},{"location":"utilities/exit/","text":"The purpose of this library is to enhance the native Promise feature with limit and spread operators. Installation \u00b6 npm install --save @leverege/promise Usage \u00b6 Include the library in your module: const P = require( '@leverege/promise' ) Promise Function \u00b6 A promise function is a function that returns a promise when invoked. This allows us to delay the creation ( and hence execution ) of the Promise until we have an available worker to process it. Promise functions in the map() call will take arguments : the data and index of the data to be processed. limit( Array PromiseFunctions[]|Object, Number | Options ) \u00b6 The limit() call takes either an Array of promiseFunctions or object specifying promiseFunctions. By default, the options is set to a limit of 10 and to exit on error Options Value Description limit number The number of concurrently executing promise functions handleReject boolean If true, Promise rejects will be inserted into the resulting array or object. If false, the limit call will stop and its returned promise will be rejected with the error. const funcs = [ ( ) => api.queryX( ), ( ) => api.queryY( ), ( ) => api.queryZ( ) ] const r = await P.limit( funcs ) // return [ x, y, z ] or throw exception const funcs = { valueX : ( ) => api.queryX( ), valueY : ( ) => api.queryY( ), valueZ : ( ) => api.queryZ( ) } // Run only two at a time const r = await P.limit( funcs, 2 ) // return { valueX : x, valueY : y, valueZ : z } or throw exception When not exiting on errors: const funcs = [ ( ) => api.queryX( ), ( ) => api.queryY( ), ( ) => api.queryZ( ) ] const r = await P.limit( funcs, { limit : 2, handleReject : true } ) // return [ Error, y, Error ] const funcs = { valueX : ( ) => api.queryX( ), valueY : ( ) => api.queryY( ), valueZ : ( ) => api.queryZ( ) } const r = await P.limit( funcs, { limit : 2, handleReject : true } ) // return { valueX : Error, valueY : y, valueZ : Error } The limit call can also work in deep structures: const f = { values : { A : () => api.queryA( ), B : () => api.queryB( ) }, deep : { array : [ api.queryC( ), api.queryD( ) ] } } const r = await P.limit( f, { limit : 2, handleReject : true } ) // return { values : { A : 'A', B : 'B' }, deep : { array : [ 'C', 'D' ] } } all( Array PromiseFunctions[]|Object, Options ) \u00b6 The all() call takes either an Array of promiseFunctions or object specifying promiseFunctions. There will be no limiting of the promises. If you are not interested in the handleReject or object handling, use Promise.all( [ promise1, promise2, ... ] ) . Options Value Description handleReject boolean If true, Promise rejects will be inserted into the resulting array or object. If false, the limit call will stop and its returned promise will be rejected with the error. const funcs = [ ( ) => api.queryX( ), ( ) => api.queryY( ), ( ) => api.queryZ( ) ] const r = await P.all( funcs, { handleReject : true } ) // return [ Error, y, Error ] const funcs = { valueX : ( ) => api.queryX( ), valueY : ( ) => api.queryY( ), valueZ : ( ) => api.queryZ( ) } const r = await P.limit( funcs, { limit : 2, handleReject : true } ) // return { valueX : Error, valueY : y, valueZ : Error } mapLimit( Array data[], PromiseFunction( data, index ), Number | Options ) \u00b6 The mapLimit() call takes an Array of data. Each data item is given to the PromiseFunction. The number of current running PromiseFunctions is determined by the limit. By default, the options is set to a limit of 10 and to exit on error Options Value Description limit number The number of concurrently executing promise functions handleReject boolean If true, Promise rejects will be inserted into the resulting array or object. If false, the limit call will stop and its returned promise will be rejected with the error. const data = [ 1, 2, 3 ] const r = await P.mapLimit( data, ( d ) => api.get( d ), 2 ) // return [ {result of get(1)}, {result of get(2)}, {result of get(3)} ] or throw exception const data = [ 1, 2, 3 ] const r = await P.mapLimit( data, ( d ) => api.get( d ), { limit : 2, handleReject : true } ) // return [ Error, {result of get(2)}, Error ] or throw exception map( Array data[], PromiseFunction( data, index ), Options ) \u00b6 This is like mapLimit, but without the limit option. deferred() \u00b6 Returns a promise with a resolve and reject function. This can be used to forward results from promises that have not been created yet.","title":"Promise"},{"location":"utilities/exit/#installation","text":"npm install --save @leverege/promise","title":"Installation"},{"location":"utilities/exit/#usage","text":"Include the library in your module: const P = require( '@leverege/promise' )","title":"Usage"},{"location":"utilities/exit/#promise-function","text":"A promise function is a function that returns a promise when invoked. This allows us to delay the creation ( and hence execution ) of the Promise until we have an available worker to process it. Promise functions in the map() call will take arguments : the data and index of the data to be processed.","title":"Promise Function"},{"location":"utilities/exit/#limit-array-promisefunctionsobject-number-options","text":"The limit() call takes either an Array of promiseFunctions or object specifying promiseFunctions. By default, the options is set to a limit of 10 and to exit on error Options Value Description limit number The number of concurrently executing promise functions handleReject boolean If true, Promise rejects will be inserted into the resulting array or object. If false, the limit call will stop and its returned promise will be rejected with the error. const funcs = [ ( ) => api.queryX( ), ( ) => api.queryY( ), ( ) => api.queryZ( ) ] const r = await P.limit( funcs ) // return [ x, y, z ] or throw exception const funcs = { valueX : ( ) => api.queryX( ), valueY : ( ) => api.queryY( ), valueZ : ( ) => api.queryZ( ) } // Run only two at a time const r = await P.limit( funcs, 2 ) // return { valueX : x, valueY : y, valueZ : z } or throw exception When not exiting on errors: const funcs = [ ( ) => api.queryX( ), ( ) => api.queryY( ), ( ) => api.queryZ( ) ] const r = await P.limit( funcs, { limit : 2, handleReject : true } ) // return [ Error, y, Error ] const funcs = { valueX : ( ) => api.queryX( ), valueY : ( ) => api.queryY( ), valueZ : ( ) => api.queryZ( ) } const r = await P.limit( funcs, { limit : 2, handleReject : true } ) // return { valueX : Error, valueY : y, valueZ : Error } The limit call can also work in deep structures: const f = { values : { A : () => api.queryA( ), B : () => api.queryB( ) }, deep : { array : [ api.queryC( ), api.queryD( ) ] } } const r = await P.limit( f, { limit : 2, handleReject : true } ) // return { values : { A : 'A', B : 'B' }, deep : { array : [ 'C', 'D' ] } }","title":"limit( Array PromiseFunctions[]|Object, Number | Options )"},{"location":"utilities/exit/#all-array-promisefunctionsobject-options","text":"The all() call takes either an Array of promiseFunctions or object specifying promiseFunctions. There will be no limiting of the promises. If you are not interested in the handleReject or object handling, use Promise.all( [ promise1, promise2, ... ] ) . Options Value Description handleReject boolean If true, Promise rejects will be inserted into the resulting array or object. If false, the limit call will stop and its returned promise will be rejected with the error. const funcs = [ ( ) => api.queryX( ), ( ) => api.queryY( ), ( ) => api.queryZ( ) ] const r = await P.all( funcs, { handleReject : true } ) // return [ Error, y, Error ] const funcs = { valueX : ( ) => api.queryX( ), valueY : ( ) => api.queryY( ), valueZ : ( ) => api.queryZ( ) } const r = await P.limit( funcs, { limit : 2, handleReject : true } ) // return { valueX : Error, valueY : y, valueZ : Error }","title":"all( Array PromiseFunctions[]|Object, Options )"},{"location":"utilities/exit/#maplimit-array-data-promisefunction-data-index-number-options","text":"The mapLimit() call takes an Array of data. Each data item is given to the PromiseFunction. The number of current running PromiseFunctions is determined by the limit. By default, the options is set to a limit of 10 and to exit on error Options Value Description limit number The number of concurrently executing promise functions handleReject boolean If true, Promise rejects will be inserted into the resulting array or object. If false, the limit call will stop and its returned promise will be rejected with the error. const data = [ 1, 2, 3 ] const r = await P.mapLimit( data, ( d ) => api.get( d ), 2 ) // return [ {result of get(1)}, {result of get(2)}, {result of get(3)} ] or throw exception const data = [ 1, 2, 3 ] const r = await P.mapLimit( data, ( d ) => api.get( d ), { limit : 2, handleReject : true } ) // return [ Error, {result of get(2)}, Error ] or throw exception","title":"mapLimit( Array data[], PromiseFunction( data, index ), Number | Options )"},{"location":"utilities/exit/#map-array-data-promisefunction-data-index-options","text":"This is like mapLimit, but without the limit option.","title":"map( Array data[], PromiseFunction( data, index ), Options )"},{"location":"utilities/exit/#deferred","text":"Returns a promise with a resolve and reject function. This can be used to forward results from promises that have not been created yet.","title":"deferred()"},{"location":"utilities/factory/","text":"This library creates a flexible Factory object. Installation \u00b6 npm install --save @leverege/factory Usage \u00b6 Include the library in your module: const Factory = require( '@leverege/factory' ) const f = new Factory() // Register your types f.add( 'myType', MyTypeMsg ) f.add( 'myOtherType', MyOtherTypeMsg ) const json = { type : 'myOtherType', value : 5 } const msg = f.create( json ) // At this point msg was created by calling new MyOtherMsg( json ) if MyOtherMsg // is a Class, otherwise it is the result of MyOtherMsg( json ) The Factory.create() method can take multiple arguments, depending on your particular factory needs. If can be invoked with an object as shown above, or with a the type string directly with extra arguments: const f = new Factory( Factory.ARGS ) f.add( 'someMsg', SomeMsg ) const msg = f.create( 'someMsg', param1, param2 ) Options \u00b6 You can create a factory by calling new Factory( strategy ) or new Factory( options ) where the options are defined as Option Default Description name 'Factory' Name used in errors typeKey 'type' The field name used when extracting the lookup type from an object registry null Map of type to object. This is like calling add() with key/value defaultObject null Default object to used if none is registered for the type strategy Factory.All The creation strategy Strategies \u00b6 There are several different strategies for creating the resulting object, and you can write your own, if needed. Factory.All \u00b6 This is the default strategy. It will pass all arguments given to create() into the Class/function. So, assuming json.type maps to a Clazz . f.create( json ) invokes new Clazz( json ) f.create( json, 'hello' ) invokes new Clazz( json, 'hello' ) f.create( json, 'hello', props ) invokes new Clazz( json, 'hello', props ) Factory.ARGS \u00b6 This strategy would normally be used when the type and data to process aren't contained in the same object. It will pass all arguments given to create() after the type into the Class/function. f.create( type ) invokes new Clazz( ) f.create( type, 'hello' ) invokes new Clazz( 'hello' ) f.create( type, 'hello', props ) invokes new Clazz( 'hello', props ) Factory.NONE \u00b6 In this strategy, no args are passed to the Class/Function f.create( type ) invokes new Clazz( ) f.create( type, 'hello' ) invokes new Clazz( ) f.create( type, 'hello', props ) invokes new Clazz( ) Factory.LOOKUP \u00b6 When using this strategy, the Factory acts as a lookup, and Factory.create() is the same as Factory.get() . The registered object is just returned f.create( type ) invokes Clazz f.create( type, 'hello' ) invokes Clazz f.create( type, 'hello', props ) invokes Clazz Factory.MERGE \u00b6 This strategy is a variant of the ARGS strategy. The type/object is merged into the first parameter. f.create( json ) invokes new Clazz( { model : json } ) f.create( json, { value : 1 } ) invokes new Clazz( { model : json, value : 1 } ) f.create( json, { value : 1 }, props ) invokes new Clazz( { model : json, value : 1 }, props ) Factory.MERGE uses 'model' as the merge key. If you wish to make it something else, use the Factory.mergeStrategy( 'otherName' ) to create a custom merge strategy. Custom Strategy \u00b6 You can define your strategy by passing your own function in via the options. The function be invoked with: // factory is the invoking factory // clzz is the item registered against the type // typeOrObj is the first argument into create // ...args the remaining objects strategy( factory, clzz, typeOrObj, ...args ) Factory.reactStrategy \u00b6 This is a strategy that will treat the first and second arguments passed into create() as props. The first argument will be merged into the second (the props) at a specified field name ( defaults to 'model' ). For this to work, you must supply the React object to the strategy as well: import React from 'react' const f = new Factory( Factory.reactStrategy( React, 'model' ) ) f.add( 'myComponent', MyComponent ) const myDataItem = { type : 'myComponent', data : 'values' } ... <div> {f.create( myDataItem, { prop1 : 1, prop2 : 2 } )} </div> // will result in <div> <MyComponent model={myDataItem} prop1={1} prop2={2}/> </div>","title":"Factory"},{"location":"utilities/factory/#installation","text":"npm install --save @leverege/factory","title":"Installation"},{"location":"utilities/factory/#usage","text":"Include the library in your module: const Factory = require( '@leverege/factory' ) const f = new Factory() // Register your types f.add( 'myType', MyTypeMsg ) f.add( 'myOtherType', MyOtherTypeMsg ) const json = { type : 'myOtherType', value : 5 } const msg = f.create( json ) // At this point msg was created by calling new MyOtherMsg( json ) if MyOtherMsg // is a Class, otherwise it is the result of MyOtherMsg( json ) The Factory.create() method can take multiple arguments, depending on your particular factory needs. If can be invoked with an object as shown above, or with a the type string directly with extra arguments: const f = new Factory( Factory.ARGS ) f.add( 'someMsg', SomeMsg ) const msg = f.create( 'someMsg', param1, param2 )","title":"Usage"},{"location":"utilities/factory/#options","text":"You can create a factory by calling new Factory( strategy ) or new Factory( options ) where the options are defined as Option Default Description name 'Factory' Name used in errors typeKey 'type' The field name used when extracting the lookup type from an object registry null Map of type to object. This is like calling add() with key/value defaultObject null Default object to used if none is registered for the type strategy Factory.All The creation strategy","title":"Options"},{"location":"utilities/factory/#strategies","text":"There are several different strategies for creating the resulting object, and you can write your own, if needed.","title":"Strategies"},{"location":"utilities/factory/#factoryall","text":"This is the default strategy. It will pass all arguments given to create() into the Class/function. So, assuming json.type maps to a Clazz . f.create( json ) invokes new Clazz( json ) f.create( json, 'hello' ) invokes new Clazz( json, 'hello' ) f.create( json, 'hello', props ) invokes new Clazz( json, 'hello', props )","title":"Factory.All"},{"location":"utilities/factory/#factoryargs","text":"This strategy would normally be used when the type and data to process aren't contained in the same object. It will pass all arguments given to create() after the type into the Class/function. f.create( type ) invokes new Clazz( ) f.create( type, 'hello' ) invokes new Clazz( 'hello' ) f.create( type, 'hello', props ) invokes new Clazz( 'hello', props )","title":"Factory.ARGS"},{"location":"utilities/factory/#factorynone","text":"In this strategy, no args are passed to the Class/Function f.create( type ) invokes new Clazz( ) f.create( type, 'hello' ) invokes new Clazz( ) f.create( type, 'hello', props ) invokes new Clazz( )","title":"Factory.NONE"},{"location":"utilities/factory/#factorylookup","text":"When using this strategy, the Factory acts as a lookup, and Factory.create() is the same as Factory.get() . The registered object is just returned f.create( type ) invokes Clazz f.create( type, 'hello' ) invokes Clazz f.create( type, 'hello', props ) invokes Clazz","title":"Factory.LOOKUP"},{"location":"utilities/factory/#factorymerge","text":"This strategy is a variant of the ARGS strategy. The type/object is merged into the first parameter. f.create( json ) invokes new Clazz( { model : json } ) f.create( json, { value : 1 } ) invokes new Clazz( { model : json, value : 1 } ) f.create( json, { value : 1 }, props ) invokes new Clazz( { model : json, value : 1 }, props ) Factory.MERGE uses 'model' as the merge key. If you wish to make it something else, use the Factory.mergeStrategy( 'otherName' ) to create a custom merge strategy.","title":"Factory.MERGE"},{"location":"utilities/factory/#custom-strategy","text":"You can define your strategy by passing your own function in via the options. The function be invoked with: // factory is the invoking factory // clzz is the item registered against the type // typeOrObj is the first argument into create // ...args the remaining objects strategy( factory, clzz, typeOrObj, ...args )","title":"Custom Strategy"},{"location":"utilities/factory/#factoryreactstrategy","text":"This is a strategy that will treat the first and second arguments passed into create() as props. The first argument will be merged into the second (the props) at a specified field name ( defaults to 'model' ). For this to work, you must supply the React object to the strategy as well: import React from 'react' const f = new Factory( Factory.reactStrategy( React, 'model' ) ) f.add( 'myComponent', MyComponent ) const myDataItem = { type : 'myComponent', data : 'values' } ... <div> {f.create( myDataItem, { prop1 : 1, prop2 : 2 } )} </div> // will result in <div> <MyComponent model={myDataItem} prop1={1} prop2={2}/> </div>","title":"Factory.reactStrategy"},{"location":"utilities/lol/","text":"Leverege Operational Lexicon, or LOL is a cli tool for interacting with the Leverege API. It makes and curls every request possible to the Leverege API. This can be used for many different things from testing to sending data. Install \u00b6 npm install -g @leverege/lol Usage \u00b6 lol help Usage: lol [options] [commands] To pass complex objects, use JSON objects wrapped in single quotes. Commands: help : this message set host <host> : sets the host to connect to set project <project id> : sets the project id to use by default projects : lists the available projects projects create <obj> : creates a project projects help : lists the projects options users : list the platform users users help : list users options project <id> : gets the project project <id> help : lists the available options on the project project <id> update : lists the available options on the project project <id> <resource> : lists the projects resources, where: <resources> include members, accounts, apiAccess, networks, scripts, systems, messageRoutes, scenarios, timers, blueprints If the lol set project <projectId> is used, this lol project xxx blueprints can be reduced to lol blueprints project <id> : returns the project's information project <id> help : list the projects options blueprint <id> : returns the blueprint's information blueprint <id> help : list the blueprints options attribute <id> : returns the attribute's information attribute <id> help: list the attribute options system <id> : returns the system's information system <id> help : list the system options device <id> : returns the device's information device <id> help : list the devices options messageRoute <id> : returns the messageRoute's information messageRoute <id> help : list the messageRoute options script <id> : returns the script's information script <id> help : list the script options apiAccess <id> : returns the apiAccess's information apiAccess <id> help : list the apiAccess options timer <id> : returns the timer's information timer <id> help : list the timer options user <id> : returns the user's information user <id> help : list the user options interface <system> <type> : calls the interface getDeviceByNetworkAlias <networkId> <propertyKey> <value> : returns the device with the given network id forgotPassword <username> <projectId> : send password reset message resetPassword options: -extract <path> : This will print the value from the result specified by path. Examples \u00b6 To list your projects lol projects To list blueprints in a project lol project <id> blueprints To change the name of a blueprint lol blueprint <id> update '{\"name\" : \"My New Name\" }'","title":"Lol"},{"location":"utilities/lol/#install","text":"npm install -g @leverege/lol","title":"Install"},{"location":"utilities/lol/#usage","text":"lol help Usage: lol [options] [commands] To pass complex objects, use JSON objects wrapped in single quotes. Commands: help : this message set host <host> : sets the host to connect to set project <project id> : sets the project id to use by default projects : lists the available projects projects create <obj> : creates a project projects help : lists the projects options users : list the platform users users help : list users options project <id> : gets the project project <id> help : lists the available options on the project project <id> update : lists the available options on the project project <id> <resource> : lists the projects resources, where: <resources> include members, accounts, apiAccess, networks, scripts, systems, messageRoutes, scenarios, timers, blueprints If the lol set project <projectId> is used, this lol project xxx blueprints can be reduced to lol blueprints project <id> : returns the project's information project <id> help : list the projects options blueprint <id> : returns the blueprint's information blueprint <id> help : list the blueprints options attribute <id> : returns the attribute's information attribute <id> help: list the attribute options system <id> : returns the system's information system <id> help : list the system options device <id> : returns the device's information device <id> help : list the devices options messageRoute <id> : returns the messageRoute's information messageRoute <id> help : list the messageRoute options script <id> : returns the script's information script <id> help : list the script options apiAccess <id> : returns the apiAccess's information apiAccess <id> help : list the apiAccess options timer <id> : returns the timer's information timer <id> help : list the timer options user <id> : returns the user's information user <id> help : list the user options interface <system> <type> : calls the interface getDeviceByNetworkAlias <networkId> <propertyKey> <value> : returns the device with the given network id forgotPassword <username> <projectId> : send password reset message resetPassword options: -extract <path> : This will print the value from the result specified by path.","title":"Usage"},{"location":"utilities/lol/#examples","text":"To list your projects lol projects To list blueprints in a project lol project <id> blueprints To change the name of a blueprint lol blueprint <id> update '{\"name\" : \"My New Name\" }'","title":"Examples"},{"location":"utilities/object-util/","text":"The object-util Library provides methods to easily transform object to and from path/value arrays. Path/value arrays are used when updating devices and information for most of the objects on the Leverege platform. Install \u00b6 npm install --save @leverege/object-util const ObjectUtil = require( '@leverege/object-util' ); copy( toCopy ) \u00b6 returns a copy if it is passed an array or object. assignFields( dst, src, fields, copyFields ) \u00b6 dst - object to have fields added to it src - object to have fields copied from fields - array of strings of keys to have copied copyFields - defaults to false, if true will deep copy the fields flatten( obj, options ) \u00b6 Turns a deep object into a shallow object with keys to represent placement of deep values obj - object to be flattened options - value to separate keys unflatten( data, opts ) \u00b6 Takes a shallow object and transforms it into a deep array data - the object to be unflattened opts - divider of the keys in data toPathValue( obj ) \u00b6 Returns an array of path and value objects from the obj fromPathValue( arr ) \u00b6 returns a deep object of the path value array pathValue( arr, path, dValue ) \u00b6 Scans arr for the path and returns the corresponding value.","title":"Object Util"},{"location":"utilities/object-util/#install","text":"npm install --save @leverege/object-util const ObjectUtil = require( '@leverege/object-util' );","title":"Install"},{"location":"utilities/object-util/#copy-tocopy","text":"returns a copy if it is passed an array or object.","title":"copy( toCopy )"},{"location":"utilities/object-util/#assignfields-dst-src-fields-copyfields","text":"dst - object to have fields added to it src - object to have fields copied from fields - array of strings of keys to have copied copyFields - defaults to false, if true will deep copy the fields","title":"assignFields( dst, src, fields, copyFields )"},{"location":"utilities/object-util/#flatten-obj-options","text":"Turns a deep object into a shallow object with keys to represent placement of deep values obj - object to be flattened options - value to separate keys","title":"flatten( obj, options )"},{"location":"utilities/object-util/#unflatten-data-opts","text":"Takes a shallow object and transforms it into a deep array data - the object to be unflattened opts - divider of the keys in data","title":"unflatten( data, opts )"},{"location":"utilities/object-util/#topathvalue-obj","text":"Returns an array of path and value objects from the obj","title":"toPathValue( obj )"},{"location":"utilities/object-util/#frompathvalue-arr","text":"returns a deep object of the path value array","title":"fromPathValue( arr )"},{"location":"utilities/object-util/#pathvalue-arr-path-dvalue","text":"Scans arr for the path and returns the corresponding value.","title":"pathValue( arr, path, dValue )"},{"location":"utilities/path/","text":"Many aspects of the Leverege system interact with data in a path/value format. This library offers some common functionality working with the path/value format. Install \u00b6 npm install --save @leverege/path Create a Path \u00b6 A path will turn a '/' separated string into a traversal pattern for an object. To create one, call const Path = require( '@leverege/path' ); const p = Path( 'my/path' ) const p2 = Path( ['my', 'path' ] ) const p3 = new Path( 'a/b' ) Array Paths \u00b6 A path created with an array of strings will preserve '/' in its array entries. For example: const obj = { 'a/b' : { c : 'hello' } } // null Path( 'a/b/c' ).get( obj ) // 'hello' Path( [ 'a/b', c ] ).get( obj ) Path( path ).get( object, defaultValue ) \u00b6 Path.get( path, object, defaultValue ) \u00b6 This will attempt to traverse the object using the current path. If it cannot traverse the path, defaultValue will be returned // given const obj = { id : 1, info : { name : \"Mr Wiggles\" } }; Path( 'id' ).get( obj ); // 1 Path( 'name' ).get( obj, 'dValue' ); // dValue Path( 'info/name' ).get( obj, 'dValue' ); // Mr Wiggles You can also invoke get as a function directly on path: Path.get( 'id', obj ); // 1 Path.get( 'name', obj, 'unknown' ); // unknown Path.get( 'info/name', obj, 'unknown' ); // Mr Wiggles Path( path ).set( object, value ) \u00b6 Path.set( path, object, value ) \u00b6 This will attempt to set the value on the object at the current path. If it cannot traverse the path, false will be returned. Otherwise, this will return true. // given Path( 'id' ).set( obj, 2 ); // true Path( 'info/name' ).set( obj, 'Mrs Wiggles' ); // true Path( 'info/name/last' ).set( obj, 'Wiggles' ); // false Path( path ).copy( object, [value] ) \u00b6 Path.copy( path, object, [value] ) \u00b6 This will make a shallow copy of object and all object along the path and return it. If value is supplied, the returned copy's value at path will be value. If the path crosses a primitive, the primitive will be left in place. Path( path ).delete( object ) \u00b6 Path.delete( path, object ) \u00b6 This will call delete on the value at the path. If the delete occurred, true will be returned. Otherwise, false is returned.","title":"Path"},{"location":"utilities/path/#install","text":"npm install --save @leverege/path","title":"Install"},{"location":"utilities/path/#create-a-path","text":"A path will turn a '/' separated string into a traversal pattern for an object. To create one, call const Path = require( '@leverege/path' ); const p = Path( 'my/path' ) const p2 = Path( ['my', 'path' ] ) const p3 = new Path( 'a/b' )","title":"Create a Path"},{"location":"utilities/path/#array-paths","text":"A path created with an array of strings will preserve '/' in its array entries. For example: const obj = { 'a/b' : { c : 'hello' } } // null Path( 'a/b/c' ).get( obj ) // 'hello' Path( [ 'a/b', c ] ).get( obj )","title":"Array Paths"},{"location":"utilities/path/#path-path-get-object-defaultvalue","text":"","title":"Path( path ).get( object, defaultValue )"},{"location":"utilities/path/#pathget-path-object-defaultvalue","text":"This will attempt to traverse the object using the current path. If it cannot traverse the path, defaultValue will be returned // given const obj = { id : 1, info : { name : \"Mr Wiggles\" } }; Path( 'id' ).get( obj ); // 1 Path( 'name' ).get( obj, 'dValue' ); // dValue Path( 'info/name' ).get( obj, 'dValue' ); // Mr Wiggles You can also invoke get as a function directly on path: Path.get( 'id', obj ); // 1 Path.get( 'name', obj, 'unknown' ); // unknown Path.get( 'info/name', obj, 'unknown' ); // Mr Wiggles","title":"Path.get( path, object, defaultValue )"},{"location":"utilities/path/#path-path-set-object-value","text":"","title":"Path( path ).set( object, value )"},{"location":"utilities/path/#pathset-path-object-value","text":"This will attempt to set the value on the object at the current path. If it cannot traverse the path, false will be returned. Otherwise, this will return true. // given Path( 'id' ).set( obj, 2 ); // true Path( 'info/name' ).set( obj, 'Mrs Wiggles' ); // true Path( 'info/name/last' ).set( obj, 'Wiggles' ); // false","title":"Path.set( path, object, value )"},{"location":"utilities/path/#path-path-copy-object-value","text":"","title":"Path( path ).copy( object, [value] )"},{"location":"utilities/path/#pathcopy-path-object-value","text":"This will make a shallow copy of object and all object along the path and return it. If value is supplied, the returned copy's value at path will be value. If the path crosses a primitive, the primitive will be left in place.","title":"Path.copy( path, object, [value] )"},{"location":"utilities/path/#path-path-delete-object","text":"","title":"Path( path ).delete( object )"},{"location":"utilities/path/#pathdelete-path-object","text":"This will call delete on the value at the path. If the delete occurred, true will be returned. Otherwise, false is returned.","title":"Path.delete( path, object )"},{"location":"utilities/reasoner/","text":"The reasoner library abstracts FaaS capabilities, wrapping scripts that can be run and triggered from anywhere in the Leverege Platform. Currently only Cloud Functions are supported by reasoner. Reasoner prepares a context with device, system, blueprint and project information in addition to several methods, and then invokes the script with that context. Deploying \u00b6 A script is deployed through the ui, under the script section, scripts can be created, edited, deployed and have their logs viewed. Context \u00b6 The context enables a lot of functionality that can be utilized within Reason scripts. The context comes with several data properties and several methods. Properties \u00b6 key type description scriptId string id of script deviceData object current data of the device (before the incoming data reasoner receives) cache object cache for the device for this script system object current data of the system device object current device information blueprint object blueprint information of the device scriptCache object cache for the script params object user defined parameters to run the script with Methods \u00b6 setCache( data ) The setCache method is used to set the cache for the device. This cache then gets added to the context for subsequent times the reason script is triggered by a device change. param type description data object the data to be set for the device's cache setScriptCache( data ) The setScriptCache method is used to set the cache for the script. This cache then gets added to the context for subsequent times the reason script is triggered via the scriptCache property. param type description data object the data to be set for the script's cache sendEmail( msg, targets ) The sendEmail method is used to send emails. This method formats an EmailMsg which is then sent into the Leverege Platform, where it is received and processed by Emailer . param type description msg object follows the format of an EmailMsg , does not need to specify a to field targets array the recipients of the email sendSms( msg, targets ) The sendSms method is used to send SMS messages. This method formats a SmsMsg which is then sent into the Leverege Platform, where it is received and processed by Messenger . param type description msg object follows the format of an SmsMsg , does not need to specify a to field targets array the recipients of the SMS setDeviceData( pathValueArray ) This method is used to set device data. param type description pathValueArray array array of objects pathValueArray[x].path string / separated string that specifies where to save the value to pathValueArray[x].value any the value to be saved to the device log( args ) This method currently just uses console.log , but is recommended in case any additional logging functionality is added or existing functionality gets changed. error( args ) This method currently just uses console.error , but is recommended in case any additional logging functionality is added or existing functionality gets changed.","title":"Reasoner"},{"location":"utilities/reasoner/#deploying","text":"A script is deployed through the ui, under the script section, scripts can be created, edited, deployed and have their logs viewed.","title":"Deploying"},{"location":"utilities/reasoner/#context","text":"The context enables a lot of functionality that can be utilized within Reason scripts. The context comes with several data properties and several methods.","title":"Context"},{"location":"utilities/reasoner/#properties","text":"key type description scriptId string id of script deviceData object current data of the device (before the incoming data reasoner receives) cache object cache for the device for this script system object current data of the system device object current device information blueprint object blueprint information of the device scriptCache object cache for the script params object user defined parameters to run the script with","title":"Properties"},{"location":"utilities/reasoner/#methods","text":"setCache( data ) The setCache method is used to set the cache for the device. This cache then gets added to the context for subsequent times the reason script is triggered by a device change. param type description data object the data to be set for the device's cache setScriptCache( data ) The setScriptCache method is used to set the cache for the script. This cache then gets added to the context for subsequent times the reason script is triggered via the scriptCache property. param type description data object the data to be set for the script's cache sendEmail( msg, targets ) The sendEmail method is used to send emails. This method formats an EmailMsg which is then sent into the Leverege Platform, where it is received and processed by Emailer . param type description msg object follows the format of an EmailMsg , does not need to specify a to field targets array the recipients of the email sendSms( msg, targets ) The sendSms method is used to send SMS messages. This method formats a SmsMsg which is then sent into the Leverege Platform, where it is received and processed by Messenger . param type description msg object follows the format of an SmsMsg , does not need to specify a to field targets array the recipients of the SMS setDeviceData( pathValueArray ) This method is used to set device data. param type description pathValueArray array array of objects pathValueArray[x].path string / separated string that specifies where to save the value to pathValueArray[x].value any the value to be saved to the device log( args ) This method currently just uses console.log , but is recommended in case any additional logging functionality is added or existing functionality gets changed. error( args ) This method currently just uses console.error , but is recommended in case any additional logging functionality is added or existing functionality gets changed.","title":"Methods"},{"location":"utilities/string-util/","text":"This library has methods for manipulating and accessing strings. Install \u00b6 npm install --save @leverege/string-util const StringUtil = require( '@leverege/string-util' ); camelCaseToTitleCase( string, capitalizeAll = false ) \u00b6 returns a space separated title case version of the string capitalize( str ) \u00b6 returns the str with the first letter capitalized capitalizeAll( str ) \u00b6 returns the str with every letter capitalized escapeRegExp( str ) \u00b6 escapes the string replaceAll( str, find, replace ) \u00b6 Replaces all instances of the find value in str with the replace value isEmpty( str ) \u00b6 returns true if the string is empty or only contains spaces nullOrTrim( str, returnNullIfNotString = true ) \u00b6 trims the string if it is a string","title":"String Util"},{"location":"utilities/string-util/#install","text":"npm install --save @leverege/string-util const StringUtil = require( '@leverege/string-util' );","title":"Install"},{"location":"utilities/string-util/#camelcasetotitlecase-string-capitalizeall-false","text":"returns a space separated title case version of the string","title":"camelCaseToTitleCase( string, capitalizeAll = false )"},{"location":"utilities/string-util/#capitalize-str","text":"returns the str with the first letter capitalized","title":"capitalize( str )"},{"location":"utilities/string-util/#capitalizeall-str","text":"returns the str with every letter capitalized","title":"capitalizeAll( str )"},{"location":"utilities/string-util/#escaperegexp-str","text":"escapes the string","title":"escapeRegExp( str )"},{"location":"utilities/string-util/#replaceall-str-find-replace","text":"Replaces all instances of the find value in str with the replace value","title":"replaceAll( str, find, replace )"},{"location":"utilities/string-util/#isempty-str","text":"returns true if the string is empty or only contains spaces","title":"isEmpty( str )"},{"location":"utilities/string-util/#nullortrim-str-returnnullifnotstring-true","text":"trims the string if it is a string","title":"nullOrTrim( str, returnNullIfNotString = true )"},{"location":"utilities/validator/","text":"A set of useful validators and key validator, to validate user input data. Install \u00b6 npm install --save @leverege/validator Usage \u00b6 const { Validator } = require( '@leverege/validator' ); Validator.isEmail( str ); // true or false Validator.isPhone( str ); // true or false keyValidator \u00b6 There is only the isAlphaNumeric method on keyValidator which takes in a key board event and verifies that the character is alphaNumeric Validator \u00b6 Validator contains 3 methods. isEmail verifies that the value provided to it is an email. isPhone verifies that the value provided to it is a phone number. optional transforms a validator to allow empty or null values.","title":"Validator"},{"location":"utilities/validator/#install","text":"npm install --save @leverege/validator","title":"Install"},{"location":"utilities/validator/#usage","text":"const { Validator } = require( '@leverege/validator' ); Validator.isEmail( str ); // true or false Validator.isPhone( str ); // true or false","title":"Usage"},{"location":"utilities/validator/#keyvalidator","text":"There is only the isAlphaNumeric method on keyValidator which takes in a key board event and verifies that the character is alphaNumeric","title":"keyValidator"},{"location":"utilities/validator/#validator","text":"Validator contains 3 methods. isEmail verifies that the value provided to it is an email. isPhone verifies that the value provided to it is a phone number. optional transforms a validator to allow empty or null values.","title":"Validator"},{"location":"utilities/value-cache/","text":"The value cache is designed to simplify the storing of calculated values based on other values. This can be useful in React applications where you want to use the same properties object if the data didn't change. Install \u00b6 npm install --save @leverege/value-cache Usage \u00b6 There are two potential usages. A version where the cache can contain only one computed value, and a version that uses a key to handle many computed values. One Value - ValueCache \u00b6 const { ValueCache } = require( '@leverege/value-cache' ) class MyClass { constructor() { this.cache = new ValueCache() } doStuff( a, b, c ) { if ( this.cache.isSame( a, b, c ) ) { return this.cache.value } let v = // compute v based on a, b, c return this.cache.set( v, a, b, c ) } } Another way to use the cache is by creating a function to compute the value and calling calc( fn, args ) . function calculate( a, b, c ) { let v = ... // compute v based on a, b, c return v } return this.cache.calc( calculate, a, b, c ) Many Values - ValuesCache \u00b6 The ValuesCache stores ValueCache objects at keys so you don't have to make many ValueCache Object. It has all the same methods as the ValueCache, but the first argument is a key that represents the value you're caching. On difference to note - to get the value you all value( key ) instead of accessing .value off the ValueCache. const { ValueCache } = require( '@leverege/value-cache' ) class MyClass { constructor() { this.cache = new ValuesCache() } doStuff( a, b, c ) { if ( this.cache.isSame( 'myThing', a, b, c ) ) { return this.cache.value( 'myThing' ) } let v = // compute v based on a, b, c return this.cache.set( 'myThing', v, a, b, c ) } } The calc() methods works as well, but requires the key function calculate( a, b, c ) { let v = ... // compute v based on a, b, c return v } return this.cache.calc( 'myThing', calculate, a, b, c ) Custom equals \u00b6 You can pass into ValueCache, ValuesCache or memoize an options object that contains a equals function. const equals = ( a, b, index ) => { return myEquals( a, b ) } new ValueCache( { equals } ) } ) The Equals takes a third argument that is the index of the argument, which would allow different equality checks per argument","title":"Value Cache"},{"location":"utilities/value-cache/#install","text":"npm install --save @leverege/value-cache","title":"Install"},{"location":"utilities/value-cache/#usage","text":"There are two potential usages. A version where the cache can contain only one computed value, and a version that uses a key to handle many computed values.","title":"Usage"},{"location":"utilities/value-cache/#one-value-valuecache","text":"const { ValueCache } = require( '@leverege/value-cache' ) class MyClass { constructor() { this.cache = new ValueCache() } doStuff( a, b, c ) { if ( this.cache.isSame( a, b, c ) ) { return this.cache.value } let v = // compute v based on a, b, c return this.cache.set( v, a, b, c ) } } Another way to use the cache is by creating a function to compute the value and calling calc( fn, args ) . function calculate( a, b, c ) { let v = ... // compute v based on a, b, c return v } return this.cache.calc( calculate, a, b, c )","title":"One Value - ValueCache"},{"location":"utilities/value-cache/#many-values-valuescache","text":"The ValuesCache stores ValueCache objects at keys so you don't have to make many ValueCache Object. It has all the same methods as the ValueCache, but the first argument is a key that represents the value you're caching. On difference to note - to get the value you all value( key ) instead of accessing .value off the ValueCache. const { ValueCache } = require( '@leverege/value-cache' ) class MyClass { constructor() { this.cache = new ValuesCache() } doStuff( a, b, c ) { if ( this.cache.isSame( 'myThing', a, b, c ) ) { return this.cache.value( 'myThing' ) } let v = // compute v based on a, b, c return this.cache.set( 'myThing', v, a, b, c ) } } The calc() methods works as well, but requires the key function calculate( a, b, c ) { let v = ... // compute v based on a, b, c return v } return this.cache.calc( 'myThing', calculate, a, b, c )","title":"Many Values - ValuesCache"},{"location":"utilities/value-cache/#custom-equals","text":"You can pass into ValueCache, ValuesCache or memoize an options object that contains a equals function. const equals = ( a, b, index ) => { return myEquals( a, b ) } new ValueCache( { equals } ) } ) The Equals takes a third argument that is the index of the argument, which would allow different equality checks per argument","title":"Custom equals"}]}